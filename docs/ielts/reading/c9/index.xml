<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Notebook – C9</title><link>https://note.codiy.net/docs/ielts/reading/c9/</link><description>Recent content in C9 on Notebook</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://note.codiy.net/docs/ielts/reading/c9/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: R9.1.1 William Henry Perkin</title><link>https://note.codiy.net/ielts/r9.1.1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.codiy.net/ielts/r9.1.1/</guid><description>
&lt;blockquote>
&lt;p>&lt;strong>The man who invented synthetic dyes&lt;/strong>&lt;/p>
&lt;/blockquote>
&lt;p>William Henry Perkin was born on March 12, 1838, in London, England. As a boy, Perkin&amp;rsquo;s curiosity prompted early interests in the arts, sciences, photography, and engineering. But it was a chance stumbling upon a run-down, yet functional, laboratory in his late grandfather&amp;rsquo;s home that solidified the young man’s enthusiasm for chemistry.&lt;/p>
&lt;p>As a student at the city of London School, Perkin became immersed in the study of chemistry. His talent and devotion to the subject were perceived by his teacher, Thomas Hall, who encouraged him to attend a series of lectures given by the eminent scientist Michael Faraday at the Royal Institution. Those speeches fired the young chemistry&amp;rsquo;s enthusiasm further, and he later went on to attend the Royal College of Chemistry, which he succeeded in entering in 1853, at the age of 15.&lt;/p>
&lt;p>At the time of Perkin&amp;rsquo;s enrolment, the Royal College of Chemistry was headed by the noted German chemist August Wilhelm Hofmann. Perkin&amp;rsquo;s scientific gifts soon caught Hofmann&amp;rsquo;s attention and, within two years, he became Hofmann&amp;rsquo;s youngest assistant. Not long after that, Perkin made the scientific breakthrough that would bring him both fame and fortune.&lt;/p>
&lt;p>At the time, quinine was the only viable medical treatment for malaria. The drug is derived from the bark of the cinchona tree, native to South America, and by 1856, demand for the drug was surpassing the available supply. Thus, when Hofmann made some passing comments about the desirability of a synthetic substitute for quinine, it was unsurprising that his star pupil was moved to take up the challenge.&lt;/p>
&lt;p>During his vacation in 1856, Perkin spent his time in the laboratory on the top floor of his family&amp;rsquo;s house. He was attempting to manufacture quinine from aniline, an inexpensive and readily available coal tar waste product. Despite his best efforts, however, he did not end up with quinine. Instead, he produced a mysterious dark sludge. Luckily, Perkin&amp;rsquo;s scientific training and nature prompted him to investigate the substance further. Incorporating potassium dichromate and alcohol into the aniline at various stages of the experimental process, he finally produced a deep purple solution. And, proving the truth of the famous scientist Louis Pasteur&amp;rsquo;s words &amp;lsquo;chance favours only the prepared mind&amp;rsquo;, Perkin saw the potential of his unexpected find.&lt;/p>
&lt;p>Historically, textile dyes were made from such natural sources as plants and animal excretions. Some of these, such as the glandular mucus of snails, were difficult to obtain and outrageously expensive. Indeed, the purple colour extracted from a snail was once so costly that in society at the time only the rich could afford it. Further, natural dyes tended to be muddy in hue and fade quickly. It was against this backdrop that Perkin&amp;rsquo;s discovery was made.&lt;/p>
&lt;p>Perkin quickly grasped that his purple solution could be used to colour fabric, thus making it the world&amp;rsquo;s first synthetic dye. Realising the importance of this breakthrough, he lost no time in patenting it. But perhaps the most fascinating of all Perkin&amp;rsquo;s reactions to his find was his nearly instant recognition that the new dye had commercial possibilities.&lt;/p>
&lt;p>Perkin originally named his dye Tyrian Purple, but it later became commonly known as mauve (from the French for the plant used to make the colour violet). He asked advice of Scottish dye works owner Robert Pullar, who assured him that manufacturing the dye would be well worth it if the colour remained fast (i.e.would not fade) and the cost was relatively low. So, over the fierce objections of his mentor Hofmann, he left college to give birth to the modern chemical industry.&lt;/p>
&lt;p>With the help of his father and brother, Perkin set up a factory not far from London. Utilising the cheap and plentiful coal tar that was an almost unlimited byproduct of London&amp;rsquo;s gas street lighting, the dye works began producing the world&amp;rsquo;s first synthetically dyed material in 1857. The company received a commercial boost from the Empress Eugénie of France, when she decided the new colour flattered her. Very soon, mauve was the necessary shade for all the fashionable ladies in that country. Not to be outdone, England&amp;rsquo;s Queen Victoria also appeared in public wearing a mauve gown, thus making it all the rage in England as well. The dye was bold and fast, and the public clamoured for more. Perkin went back to the drawing board.&lt;/p>
&lt;p>Although Perkin&amp;rsquo;s fame was achieved and fortune assured by his first discovery, the chemist continued his research. Among other dyes he developed and introduced were aniline red (1859) and aniline black (1863) and, in the late 1860s, Perkin&amp;rsquo;s green. It is important to note that Perkin&amp;rsquo;s synthetic dye discoveries had outcomes far beyond the merely decorative. The dyes also became vital to medical research in many ways. For instance, they were used to stain previously invisible microbes and bacteria, allowing researchers to identify such bacilli as tuberculosis, cholera, and anthrax. Artificial dyes continue to play a crucial role today. And, in what would have been particularly pleasing to Perkin, their current use is in the search for a vaccine against malaria.&lt;/p></description></item><item><title>Docs: R9.1.2 IS THERE ANYBODY OUT THERE?</title><link>https://note.codiy.net/ielts/r9.1.2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.codiy.net/ielts/r9.1.2/</guid><description>
&lt;blockquote>
&lt;p>&lt;strong>The Search for Extra-terrestrial Intelligence&lt;/strong>&lt;/p>
&lt;/blockquote>
&lt;p>The question of whether we are alone in the Universe has haunted humanity for centuries, but we may now stand poised on the brink of the answer to that question, as we search for radio signals from other intelligent civilisations. This search, often known by the acronym SETI (search for extra-terrestrial intelligence), is a difficult one. Although groups around the world have been searching intermittently for three decades, it is only now that we have reached the level of technology where we can make a determined attempt to search all nearby stars for any sign of life.&lt;/p>
&lt;h4 id="a">A&lt;/h4>
&lt;p>The primary reason for the search is basic curiosity—the same curiosity about the natural world that drives all pure science. We want to know whether we are alone in the Universe. We want to know whether life evolves naturally if given the right conditions, or whether there is something very special about the Earth to have fostered the variety of life forms that we see around us on the planet. The simple detection of a radio signal will be sufficient to answer this most basic of all the questions. In this sense, SETI is another cog in the machinery of pure science which is continually pushing out the horizon of our knowledge. However, there are other reasons for being interested in whether life exists elsewhere. For example, we have had civilization on Earth for perhaps only a few thousand years, and the threats of the nuclear war and pollution over the last few decades have told us that our survival may be tenuous. Will we last another two thousand years or will we wipe ourselves out? Since the lifetime of a planet like ours is several billion years, we can expect that, if other civilizations do survive in our galaxy, their ages will range from zero to several billion years. Thus any mere existence of such a civilization will tell us that long-term survival is possible, and gives us some cause for optimism. It is even possible that the older civilization may pass on the benefits of their experience in dealing with threats to survival such as nuclear war and global pollution, and other threats that we haven&amp;rsquo;t yet discovered.&lt;/p>
&lt;h4 id="b">B&lt;/h4>
&lt;p>In discussing whether we are alone, most SETI scientists adopt two ground rules. First, UFOs (Unidentified Flying Objects) are generally ignored since most scientists don&amp;rsquo;t consider the evidence for them to be strong enough to bear serious consideration although it is also important to keep an open mind in case any really convincing evidence emerges in the future. Second, we make a very conservative assumption that we are looking for a life form that is pretty well like us, since if it differs radically from us we may well not recognize it as a life form, quite apart from whether we are able to communicate with it. In other words, the life form we are looking for may well have two green heads and seven fingers, but it will nevertheless resemble us in that it should communicate with its fellows, be interested in the Universe, live on a planet orbiting a star like our Sun, and perhaps most restrictively, have a chemistry, like us, based on carbon and water.&lt;/p>
&lt;h4 id="c">C&lt;/h4>
&lt;p>Even when we make these assumptions, our understanding of other life forms is still severely limited. We do not even know, for example, how many stars have planets, and we certainly do not know how likely it is that life will arise naturally, given the right conditions. However, when we look at the 100 billion stars in our galaxy (the Milky Way), and 100 billion galaxies in the observable Universe, it seems inconceivable that at least one of these planets does not have a life form on it: in fact, the best educated guess we can make, using the little we do know about the conditions for carbon-based life, leads us to estimate that perhaps one in 100,000 stars might have a life-bearing planet orbiting it. That means that our nearest neighbours are perhaps 100 light years away, which is almost next door in astronomical terms.&lt;/p>
&lt;h4 id="d">D&lt;/h4>
&lt;p>An alien civilization could choose many different ways of sending information across the galaxy, but many of these either require too much energy, or else are severely attenuated while traversing the vast distances across the galaxy. It turns out that, for a given amount of transmitted power, radio waves in the frequency range 1000 to 3000 MHz travel the greatest distance, and so all searches to date have concentrated on looking for radio waves in this frequency range. So far there have been a number of searches by various groups around the world, including Australian searches using the radio telescope at Parkes, New South Wales. Until now there have not been any detections from the few hundred stars which have been searched. The scale of the searches has been increased dramatically since 1992, when the US Congress voted NASA $10 million per year for ten years to conduct a thorough search for extra-terrestrial life.Much of the money in this project is being spent on the developing the special hardware needed to search many frequencies at once. The project has two parts. One part is a targeted search using the world&amp;rsquo;s largest radio telescopes, the American-operated telescope in Arecibo, Puerto Rico and the French telescope in Nancy in France. This part of the project is searching the nearest 1000 likely stars with high sensitivity for signals in the frequency range 1000 to 3000 MHz. The other part of the project is an undirected search which is monitoring all of space with a lower sensitivity, using the smaller antennas of NASA&amp;rsquo;s Deep Space Network.&lt;/p>
&lt;h4 id="e">E&lt;/h4>
&lt;p>There is considerable debate over how we should react if we detect a signal from an alien civilization. Everybody agrees that we should not reply immediately. Quite apart from the impracticality of sending a reply over such large distances at short notice, it raises a host of ethical questions that would have to be addressed by the global community before any reply could be sent. Would the human race face the culture shock if faced with a superior and much older civilization? Luckily, there is no urgency about this. The stars being searched are hundreds of light years away, so it takes hundreds of years for their signal to reach us, and a further few hundreds for our reply to reach them. It&amp;rsquo;s not important, then, if there&amp;rsquo;s a delay of a few years, or decades, while the human race debates the question of whether to reply, and perhaps carefully drafts a reply.&lt;/p></description></item><item><title>Docs: R9.1.3 The history of the tortoise</title><link>https://note.codiy.net/ielts/r9.1.3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.codiy.net/ielts/r9.1.3/</guid><description>
&lt;p>If you go back far enough, everything lived in the sea. At various points in evolutionary history, enterprising individuals within many different animal groups moved out onto the land, sometimes even to the most parched deserts, taking their own private seawater with them in blood and cellular fluids. In addition to the reptiles, birds, mammals and insects which we see all around us, other groups that have succeeded out of water include scorpions, snails, crustaceans such as woodlice and land crabs, millipedes and centipedes, spiders and various worms. And we mustn&amp;rsquo;t forget the plants, without whose prior invasion of the land none of the other migrations could have happened.&lt;/p>
&lt;p>Moving from water to land involved a major redesign of every aspect of life, including breathing and reproduction. Nevertheless, a good number of thoroughgoing land animals later turned around, abandoned their hard-earned terrestrial re-tooling, and returned to the water again. Seals have only gone part way back. They show us what the intermediates might have been like, on the way to extreme cases such as whales and dugongs. Whales (including the small whales we call dolphins) and dugongs, with their close cousins the manatees, ceased to be land creatures altogether and reverted to the full marine habits of their remote ancestors. They don&amp;rsquo;t even come ashore to breed. They do, however, still breathe air, having never developed anything equivalent to the gills of their earlier marine incarnation. Turtles went back to the sea a very long time ago and, like all vertebrate returnees to the water, they breathe air. However, they are, in one respect, less fully given back to the water than whales or dugongs, for turtles still lay their eggs on beaches.&lt;/p>
&lt;p>There is evidence that all modern turtles are descended from a terrestrial ancestor which lived before most of the dinosaurs. There are two key fossils called Proganochleys quenstedti and Palaeochersis talampayensis dating from early dinousaur times, which appear to be close to the ancestry of all modern turtles and tortoises. You might wonder how we can tell whether fossil animals lived on land or in water, especially if only fragments are found. Sometimes it&amp;rsquo;s obvious. Ichthyosaurs were reptilian contemporaries of the dinosaurs, with fins and streamlined bodies. The fossils look like dolphins, and they surely lived like dolphins, in the water. With turtles it is a little less obvious. One way to tell is by measuring the bones of their forelimbs.&lt;/p>
&lt;p>Walter Joyce and Jacques Gauthier, at Yale University, obtained three measurements in these particular bones of 71 species of living turtles and tortoises. They used a kind of triangular graph paper to plot the three measurements against one another. All the land tortoise species formed a tight cluster of points in the upper part of the triangle; all the water turtles cluster in the lower part of the triangular graph. There was no overlap, except when they added some species that spend time both in water and on land. Sure enough, these amphibious species show up on the triangular graph approximately half way between the &amp;lsquo;wet cluster&amp;rsquo; of sea tortoises and the &amp;lsquo;dry cluster&amp;rsquo; of land tortoises. The next step was to determine where the fossils fell. The bones of P. quenstedti and P.talampayensis leave us in no doubt. Their points on the graph are right in the thick of the dry cluster. Both these fossils were dry-land tortoises. They come from the era before our turtles returned to the water.&lt;/p>
&lt;p>You might think, therefore, that modern land tortoises have probably stayed on land ever since those early terrestrial times, as most mammals did after a few of them went back to the sea. But apparently not. If you draw out the family tree of all modern turtles and tortoises, nearly all the branches are aquatic. Today&amp;rsquo;s land tortoises constitute a single branch, deeply nested among branches consisting of aquatic turtles. This suggests that modern land tortoises have not stayed on land continuously since the time of P. quenstedti and P. talampayensis. Rather, their ancestors were among those who went back to the water, and they then re-emerged back onto the land in (relatively) more recent times.&lt;/p>
&lt;p>Tortoises therefore represent a remarkable double return. In common with all mammals, reptiles and birds, their remote ancestors were marine fish and before that various more or less worm-like creatures stretching back, still in the sea, to the primeval bacteria. Later ancestors lived on land and stayed there for a very large number of generations. Later ancestors still evolved back into the water and became sea turtles. And finally they returned yet again to the land as tortoises, some of which now live in the driest of deserts.&lt;/p></description></item><item><title>Docs: R9.2.1</title><link>https://note.codiy.net/ielts/r9.2.1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.codiy.net/ielts/r9.2.1/</guid><description>
&lt;h4 id="a">A&lt;/h4>
&lt;p>Hearing impairment or other auditory function deficit in young children can have a major impact on their development of speech and communication, resulting in a detrimental effect on their ability to learn at school. This is likely to have major consequences for the individual and the population as a whole. The New Zealand Ministry of Health has found from research carried out over two decades that 6-10% of children in that country are affected by hearing loss.&lt;/p>
&lt;h4 id="b">B&lt;/h4>
&lt;p>A preliminary study in New Zealand has shown that classroom noise presents a major concern for teachers and pupils. Modern teaching practices, the organisation of desks in the classroom, poor classroom acoustics, and mechanical means of ventilation such as air-conditioning units all contribute to the number of children unable to comprehend the teacher&amp;rsquo;s voice. Education researchers Nelson and Soli have also suggested that recent trends in learning often involve collaborative interaction of multiple minds and tools as much as individual possession of information. This all amounts to heightened activity and noise levels, which have the potential to be particularly serious for children experiencing auditory function deficit. Noise in classrooms can only exacerbate their difficulty in comprehending and processing verbal communication with other children and instructions from the teacher.&lt;/p>
&lt;h4 id="c">C&lt;/h4>
&lt;p>Children with auditory function deficit are potentially failing to learn to their maximum potential because of noise levels generated in classrooms. The effects of noise on the ability of children to learn effectively in typical classroom environments are now the subject of increasing concern. The International Institute of Noise Control Engineering (WNCE), on the advice of the World Health Organization, has established an international working party, which includes New Zealand, to evaluate noise and reverberation control for school rooms.&lt;/p>
&lt;h4 id="d">D&lt;/h4>
&lt;p>While the detrimental effects of noise in classroom situations are not limited to children experiencing disability, those with a disability that affects their processing of speech and verbal communication could be extremely vulnerable. The auditory function deficits in question include hearing impairment, autistic spectrum disorders (ASD) and attention deficit disorders (ADD/ADHD).&lt;/p>
&lt;h4 id="e">E&lt;/h4>
&lt;p>Autism is considered a neurological and genetic life-long disorder that causes discrepancies in the way information is processed. This disorder is characterised by interlinking problems with social imagination, social communication and social interaction. According to Janzen, this affects the ability to understand and relate in typical ways to people, understand events and objects in the environment, and understand or respond to sensory stimuli. Autism does not allow learning or thinking in the same ways as in children who are developing normally. Autistic spectrum disorders often result in major difficulties in comprehending verbal information and speech processing. Those experiencing these disorders often find sounds such as crowd noise and the noise generated by machinery painful and distressing. This is difficult to scientifically quantify as such extra-sensory stimuli vary greatly from one autistic individual to another. But a child who finds any type of noise in their classroom or learning space intrusive is likely to be adversely affected in their ability to process information.&lt;/p>
&lt;h4 id="f">F&lt;/h4>
&lt;p>The attention deficit disorders are indicative of neurological and genetic disorders and are characterised by difficulties with sustaining attention, effort and persistence, organisation skills and disinhibition. Children experiencing these disorders find it difficult to screen out unimportant information, and focus on everything in the environment rather than attending to a single activity. Background noise in the classroom becomes a major distraction, which can affect their ability to concentrate.&lt;/p>
&lt;h4 id="g">G&lt;/h4>
&lt;p>Children experiencing an auditory function deficit can often find speech and communication very difficult to isolate and process when set against high levels of background noise. These levels come from outside activities that penetrate the classroom structure, from teaching activities, and other noise generated inside,.which can be exacerbated by room reverberation. Strategies are needed to obtain the optimum classroom construction and perhaps a change in classroom culture and methods of teaching. In particular, the effects of noisy classrooms and activities on those experiencing disabilities in the form of auditory function deficit need thorough investigation. It is probable that many undiagnosed children exist in the education system with &amp;lsquo;invisible’ disabilities. Their needs are less likely to be met than those of children with known disabilities.&lt;/p>
&lt;h4 id="h">H&lt;/h4>
&lt;p>The New Zealand Government has developed a New Zealand Disability Strategy and has embarked on a wide-ranging consultation process. The strategy recognises that people experiencing disability face significant barriers in achieving a full quality of life in areas such as attitude, education, employment and access to services. Objective 3 of the New Zealand Disability Strategy is to ^Provide the Best Education for Disabled People* by improving education so that all children, youth learners and adult learners will have equal opportunities to learn and develop within their already existing local school. For a successful education, the learning environment is vitally significant, so any effort to improve this is likely to be of great benefit to all children, but especially to those with auditory function disabilities.&lt;/p>
&lt;h4 id="i">I&lt;/h4>
&lt;p>A number of countries are already in the process of formulating their own standards for the control and reduction of classroom noise. New Zealand will probably follow their example. The literature to date on noise in school rooms appears to focus on the effects on schoolchildren in general, their teachers and the hearing impaired. Only limited attention appears to have been given to those students experiencing the other disabilities involving auditory function deficit. It is imperative that the needs of these children are taken into account in the setting of appropriate international standards to be promulgated in future.&lt;/p></description></item><item><title>Docs: R9.2.2</title><link>https://note.codiy.net/ielts/r9.2.2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.codiy.net/ielts/r9.2.2/</guid><description>
&lt;blockquote>
&lt;p>June 2004 saw the first passage, known as a &amp;rsquo;transit&amp;rsquo; , of the planet Venus across the face of the Sun in 122 year. Transits have helped shape our view of the whole Universe, as Healther Cooper and Nigel Henbest explain.&lt;/p>
&lt;/blockquote>
&lt;h4 id="a">A&lt;/h4>
&lt;p>On 8 June 2004, more than half the population of the world were treated to a rare astronomical event. For over six hours, the planet Venus steadily inched its way over the surface of the Sun. This &amp;rsquo;transit&amp;rsquo; of Venus was the first since 6 December 1882. On that occasion, the American astronomer Professor Simon Newcomb led a party to South Africa to observe the event. They were based at a girl&amp;rsquo;s school, where—it is alleged—the combined forces of three schoolmistresses outperformed the professionals with the accuracy of their observations.&lt;/p>
&lt;h4 id="b">B&lt;/h4>
&lt;p>For centuries, transits of Venus have drawn explores and astronomers alike to the four corners of the globe. And you can put it all down to the extraordinary polymath Edmond Halley. In November 1677, Halley observed a transit of the innermost planet, Mercury, from the desolate island of St Helena in the south Pacific. He realized that, from different latitudes, the passage of the planet across the Sun&amp;rsquo;s disc would appear to differ. By timing the transit from two widely-separated locations, teams of astronomers could calculate the parallax angle—the apparent difference in position of an astronomical body due to a difference in the observer&amp;rsquo;s position. Calculating this angle would allow astronomers to measure what was then the ultimate goal: the distance of the Earth from the sun. This distance is known as the &amp;lsquo;astronomical&amp;rsquo; or AU.&lt;/p>
&lt;h4 id="c">C&lt;/h4>
&lt;p>Halley was aware that the AU was one of the most fundamental of all astronomical measurements. Johannes Kepler, in the early 17th century, had shown that the distances of the planets from the Sun governed their orbital speeds, which were easily measurable.
But no-one had found a way to calculate accurate distances to the planets from the Earth. The goal was to measure the AU; then, knowing the orbital speeds of all the other planets round the Sun, the scale of the Solar System would fall into place. However, Halley realised that Mercury was so far away that its parallax angel would be very difficult to determine. As Venus was closer to the Earth, its parallax angle would be larger, and Halley worked out that by using Venus it would be possible to measure the Sun&amp;rsquo;s distance to 1 part in 500. But there was a problem: transits of Venus, unlike those of Mercury, are rare, occurring in pairs roughly eight years apart every hundred or so years.
Nevertheless, he accurately predicted that Venus would cross the face of the Sun in both 1761 and 1769—though he didn&amp;rsquo;t survive to see either.&lt;/p>
&lt;h4 id="d">D&lt;/h4>
&lt;p>Inspired by Halley&amp;rsquo;s suggestion of a way to pin down the scale of the Solar System, teams of British and French astronomers set out on expeditions to places as diverse as India and Siberia. But things weren&amp;rsquo;t helped by Britain and France being at war. The person who deserves most sympathy is the French astronomer Guillaume Le Gentil. He was thwarted by the fact that the British were besieging his observation site at Pondicherry in India. Fleeing on a French warship crossing the Indian Ocean, Le Gentil saw a wonderful transit—but the ship&amp;rsquo;s pitching and rolling ruled out any attempt at making accurate observations. Undaunted, he remained south of the equator, keeping himself busy by studying the islands of Mauritius and Madagascar before setting off to observe the next transit in the Philippines. Ironically after travelling nearly 50,000 kilometers, his view was clouded out at the last moment, a very dispiriting experience.&lt;/p>
&lt;h4 id="e">E&lt;/h4>
&lt;p>While the early transit timings were as precise as instruments would allow, the measurements were dogged by the &amp;lsquo;black drop&amp;rsquo; effect. When Venus begins to cross the Sun&amp;rsquo;s disc, it looks smeared not circular—which makes it difficult to establish timings. This is due to diffraction of light. The second problem is that Venus exhibits a halo of light when it is seen just outside the Sun&amp;rsquo;s disc. While this showed astronomers that Venus was surrounded by thick layer of gases refracting sunlight around it, both effects made it impossible to obtain accurate timings.&lt;/p>
&lt;h4 id="f">F&lt;/h4>
&lt;p>But astronomers laboured hard to analyse the results of these expeditions to observe Venus transits. Johann Franz Encke, Director of the Berlin Observatory, finally determined a value for the AU based on all these parallax measurements: 153,340,000km. Reasonably accurate for the time, that is quite close to today&amp;rsquo;s value of methods in accuracy. The AU is a cosmic measuring rod, and the basis of how we scale the Universe today. The parallax principle can be extended to measure the distances to the stars. If we look at a star in January—when Earth is at one point in its orbit—it will seem to be in a different position from where it appears six months later. Knowing the width of Earth&amp;rsquo;s orbit, the parallax shift lets astronomers calculate the distance.&lt;/p>
&lt;h4 id="g">G&lt;/h4>
&lt;p>June 2004&amp;rsquo;s transit of Venus was thus more of an astronomical spectacle than a scientifically important event. But such transits have paved the way for what might prove to be one of the most vital breakthroughs in the cosmos—detecting Earth-sized planets orbiting other stars.&lt;/p></description></item><item><title>Docs: R9.2.3 A neuroscientist reveals how to think differently</title><link>https://note.codiy.net/ielts/r9.2.3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.codiy.net/ielts/r9.2.3/</guid><description>
&lt;p>In the last decade a revolution has occurred in the way that scientists think about the brain. We now know that the decisions humans make can be traced to the firing patterns of neurons in specific parts of the brain. These discoveries have led to the field known as neuroeconomics, which studies the brain&amp;rsquo;s secrets to success in an economic environment that demands innovation and being able to do things differently from competitors. A brain that can do this is an iconoclastic one. Briefly, an iconoclast is a person who does something that others say can&amp;rsquo;t be done.&lt;/p>
&lt;p>This definition implies that iconoclasts are different from other people, but more precisely, it is the brains that are different in three distinct ways: perception, fear response, and social intelligence. Each of these three functions utilizes a different circuit in the brain. Naysayers might suggest that the brain is irrelevant, that thinking in an original, even revolutionary, way is more a matter of personality than brain function. But the field of neuroeconomics was born out of the realization that the physical workings of the brain place limitations on the way we make decisions. By understanding these constraints, we begin to understand why some people march to a different drumbeat.&lt;/p>
&lt;p>The first thing to realize is that the brain suffers from limited resources. It has a fixed energy budget, about the same as a 40 watt light bulb, so it has evolved to work as efficiently as possible. This is where most people are impeded from being an iconoclast. For example, when confronted with information streaming from the eyes, the brain will interpret this information in the quickest way possible. Thus it will draw on both past experience and any other source of information, such as what other people say, to make sense of what it is seeing. This happens all the time. The brain takes shortcuts that work so well we are hardly ever aware of them. We think our perceptions of the world are real, but they are only biological and electrical rumblings. Perception is not simply a product of what your eyes or ears transmit to your brain. More than the physical reality of photons or sound waves, perception is a product of the brain.&lt;/p>
&lt;p>Perception is central to iconoclasm. Iconoclasts see things differently to other people. Their brains do not fail into efficiency pitfalls as much as the average person&amp;rsquo;s brain. Iconoclasts, either because they were born that way or through learning, have found ways to work around the perceptual shortcuts that plague most people. Perception is not something that is hardwired into the brain. It is a learned process, which is both a curse and an opportunity for change. The brain faces the fundamental problem of interpreting physical stimuli from the sense. Everything the brain sees, hears, or touches has multiple interpretations. The one that is ultimately chosen is simply the brain&amp;rsquo;s best theory. In technical terms, these conjectures have their basis in the statistical likelihood of one interpretation over another and are heavily influenced by past experience and, importantly for potential iconoclasts, what other people say.&lt;/p>
&lt;p>The best way to see things differently to other people is to bombard the brain with things it has never encouraged before. Novelty releases the perceptual process from the chains of past experience and forces the brain to make new judgments. Successful iconoclasts have an extraordinary willingness to be exposed to what is fresh and different. Observation of iconoclasts shows that they embrace novelty while most people avoid things that are different.&lt;/p>
&lt;p>The problem with novelty, however, is that it tends to trigger the brain&amp;rsquo;s fear system. Fear is a major impediment to thinking like an iconoclast and stops the average person in his tracks. There are many types of fear, but the two that inhibit iconoclastic thinking and people generally find difficult to deal with are fear of uncertainty and fear of public ridicule. These may seem like trivial phobias. But fear of public speaking, which everyone must do from time to time, afflicts one-third of the population. This makes it too common to be considered a mental disorder. It is simply a common variant of human nature, one which iconoclasts do not let inhibit their reactions.&lt;/p>
&lt;p>Finally, to be successful iconoclasts, individuals must sell their ideas to other people.&lt;/p>
&lt;p>This is where social intelligence comes in. Social intelligence is the ability to understand and manage people in a business setting. In the last decade there has been an explosion of knowledge about the social brain and how the brain works when groups coordinate decision making. Neuroscience has revealed which brain circuits are responsible for functions like understanding what other people think, empathy, fairness, and social identity. These brain regions play key roles in whether people convince others of their ideas. Perception is important in social cognition too. The perception of someone&amp;rsquo;s enthusiasm, or reputation, can make or beak a deal. Understanding how perception becomes intertwined with social decision making shows why successful iconoclasts are so rare.&lt;/p>
&lt;p>Iconoclasts create new opportunities in every area from artistic expression to technology to business. They supply creativity and innovation not easily accomplished by committees. Rules aren&amp;rsquo;t important to them. Iconoclasts face alienation and failure, but can also be a major asset to any organization. It is crucial for success in any field to understand how the iconoclastic mind works.&lt;/p></description></item><item><title>Docs: R9.3.1 Attitudes to language</title><link>https://note.codiy.net/ielts/r9.3.1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.codiy.net/ielts/r9.3.1/</guid><description>
&lt;p>It is not easy to be systematic and objective about language study. Popular linguistic debate regularly deteriorates into invective and polemic. Language belongs to everyone, so most people feel they have a right to hold an opinion about it. And when opinions differ, emotions can run high. Arguments can start as easily over minor points of usage as over major policies of linguistic education.&lt;/p>
&lt;p>Language, moreover, is a very public behavior, so it is easy for different usages to be noted and criticised. No part of society or social behavior is exempt. Linguistic factors influence how we judge personality, intelligence, social status, educational standards, job aptitude, and many other areas of identity and social survival. As a result, it is easy to hurt, and to be hurt, when language use is unfeelingly attacked.&lt;/p>
&lt;p>In its most general sense, prescriptivism is the view that one variety of language has an inherently higher value than others, and that this ought to be imposed on the whole of the speech community. The view is propounded especially in relation to grammar and vocabulary, and frequently with reference to pronunciation. The variety which is favoured, in this account, is usually a version of the &amp;lsquo;standard&amp;rsquo; written language, especially as encountered in literature, or in the formal spoken language which most closely reflects this style. Adherents to this variety are said to speak or write &amp;lsquo;correctly&amp;rsquo;; deviations from it are said to be &amp;lsquo;incorrect&amp;rsquo;.&lt;/p>
&lt;p>All the main languages have been studied prescriptively, especially in the 18th century approach to the writing of grammars and dictionaries. The aims of these early grammarians were threefold: (a) they wanted to codify the principles of their languages, to show that there was a system beneath the apparent chaos of usage; (b) they wanted a means of settling disputes over usage, and (c) they wanted to point out what they felt to be common errors, in order to &amp;lsquo;improve&amp;rsquo; the language. The authoritarian nature of the approach is best characterised by its reliance on &amp;lsquo;rules&amp;rsquo; of grammar. Some usages are &amp;lsquo;prescribed&amp;rsquo;, to be learnt and followed accurately; others are &amp;lsquo;proscribed&amp;rsquo;, to be avoided.&lt;/p>
&lt;p>In this early period, there were no half-measures: usage was either right or wrong, and it was the task of the grammarian not simply to record alternatives, but to pronounce judgment upon them.&lt;/p>
&lt;p>These attitudes are still with us, and they motivate a widespread concern that linguistic standards should be maintained. Nevertheless, there is an alternative point of view that is concerned less with standards than with the facts of linguistic usage. This approach is summarised in the statement that it is the task of the grammarian to describe, not prescribe—to record the facts of linguistic diversity, and not to attempt the impossible tasks of evaluating language variation or halting language change. In the second half of the 18th century, we already find advocates of this view, such as Joseph Priestley, whose Rudiments of English Grammar (1761) insists that &amp;rsquo;the custom of speaking is the original and only just standard of any language&amp;rsquo;. Linguistic issues, it is argued, cannot be solved by logic and legislation. And this view has become the tenet of the modern linguistic approach to grammatical analysis.&lt;/p>
&lt;p>In our own time, the opposition between &amp;lsquo;descriptivists&amp;rsquo; and &amp;lsquo;prescriptivists&amp;rsquo; has often become extreme, with both sides painting unreal pictures of the other. Descriptive grammarians have been presented as people who do not care about standards, because of the way they see all forms of usage as equally valid. Prescriptive grammarians have been presented as blind adherents to a historical tradition. The opposition has even been presented in quasi-political terms—of radical liberalism vs elitist conservatism.&lt;/p></description></item><item><title>Docs: R9.3.2 Tidal Power</title><link>https://note.codiy.net/ielts/r9.3.2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.codiy.net/ielts/r9.3.2/</guid><description>
&lt;blockquote>
&lt;p>Undersea turbines which produce electricity from the tides are set to become an important source of renewable energy for Britain. It is still too early to predict the extent of the impact they may have, but all the signs are that they will play a significant role in the future.&lt;/p>
&lt;/blockquote>
&lt;h4 id="a">A&lt;/h4>
&lt;p>Operating on the same principle as wind turbines, the power in sea turbines comes from tidal currents which turn blades similar to ships propellers, but, unlike wind, the tides are predictable and the power input is constant. The technology raises the prospect of Britain becoming self-sufficient in renewable energy and drastically reducing its carbon dioxide emissions. If tide, wind and wave power are all developed, Britain would be able to close gas, coal and nuclear plants and export renewable power to other parts of Europe. Unlike wind power, which Britain originally developed and then abandoned for 20 years allowing the Dutch to make it a major industry, undersea turbines could become a big export earner to island nations such as Japan and New Zealand.&lt;/p>
&lt;h4 id="b">B&lt;/h4>
&lt;p>Tidal sites have already been identified that will produce one sixth or more of the UK&amp;rsquo;s power—and at prices competitive with modern gas turbines and undercutting those of the already ailing nuclear industry. One site alone, the Pentland Firth, between Orkney and mainland Scotland, could produce 10% of the country&amp;rsquo;s electricity with banks of turbines under the sea, and another at Alderney in the Channel Islands three times the 1,2000 megawatts of Britain&amp;rsquo;s largest and newest nuclear plant, Sizewell B, in Suffolk. Other sites identified include the Bristol Channel and the west coast of Scotland, particularly the channel between Campbelttown and Northern Irealand.&lt;/p>
&lt;h4 id="c">C&lt;/h4>
&lt;p>Work on designs for the new turbine blades and sites are well advanced at the university of Southampton&amp;rsquo;s sustainable energy research group. The first station is expected to be installed off Lynmouth in Devon shortly to test the technology in a venture jointly funded by the department of Trade and Industry and the European Union. AbuBakr Bahaj, in charge of the Southampton research, said: &amp;lsquo;The prospects for energy from tidal currents are far better than from wind because the flows of water are predictable and constant. The technology for dealing with the hostile saline environment under the sea has been developed in the North Sea oil industry and much is already known about turbine blade design, because of wind power and ship propellers. There are a few technical difficulties, but I believe in the next five to ten years we will be installing commercial marine turbine farms. &amp;rsquo; Southampton has been awarded £215,000 over three years to develop the turbines and is working with Marine Current Turbines, a subsidiary of IT power, on the Lynmouth project. EU research has now identified 106 potential sites for tidal power, 80% round the coasts of Britain. The best sites are between islands or around heavily indented coasts where there are strong tidal currents.&lt;/p>
&lt;h4 id="d">D&lt;/h4>
&lt;p>A marine turbine blade needs to be only one third of the size of a wind generator to produce three times as much power. The blades will be about 20 meters in diameter, so around 30 meters of water is required. Unlike wind power, there are unlikely to be environmental objections. Fish and other creatures are thought unlikely to be at risk from the relatively slow-turning blades. Each turbine will be mounted on a tower which will connect to the national power supply grid via underwater cables. The towers will stick out of the water and be lit, to warn shipping, and also be designed to be lifted out of the water for maintenance and to clean seaweed from the blades.&lt;/p>
&lt;h4 id="e">E&lt;/h4>
&lt;p>Dr Bahaj has done most work on the Alderney site, where there are powerful currents. The single undersea turbine farm would produce far more power than needed for the Channel Islands and most would be fed into the French Grid and be re-imported into Britain via the cable under the Channel.&lt;/p>
&lt;h4 id="f">F&lt;/h4>
&lt;p>One technical difficulty is cavitation, where low pressure behind a turning blade causes air bubbles. These can cause vibration and damage the blades of the turbines. Dr Bahaj said: &amp;lsquo;We have to test a number of blade types to avoid this happening or at least make sure it does not damage the turbines or reduce performance. Another slight concern is submerged debris floating into the blades. So far we do not know how much of a problem it might be. We will have to make the turbines robust because the sea is a hostile environment, but all the signs that we can do it are good.&lt;/p></description></item><item><title>Docs: R9.3.3 Information theory - the big idea</title><link>https://note.codiy.net/ielts/r9.3.3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.codiy.net/ielts/r9.3.3/</guid><description>
&lt;blockquote>
&lt;p>Information theory lies at the heart of everything—from DVD players and the genetic code of DNA to the physics of the universe at its most fundamental. It has been central to the development of the science of communication, which enables data to be sent electronically and has therefore had a major impact on our lives.&lt;/p>
&lt;/blockquote>
&lt;h4 id="a">A&lt;/h4>
&lt;p>In April 2002 an event took place which demonstrated one of the many applications of information theory. The space probe Voyager I, launched in 1977, had sent back spectacular images of Jupiter and Saturn and then soared out of the Solar System on a one-way mission to the stars. After 25 years of exposure to the freezing temperatures of deep space, the probe was beginning to show its age. Sensors and circuits were on the brink of failing and NASA experts realized that they had to do something or lose contact with their probe forever. The solution was to get a message to Voyager I to instruct it to use spares to change the failing parts. With the probe 12 billion kilometers from Earth, this was not an easy task. By means of a radio dish belonging to NASA&amp;rsquo;s Deep Space Network, the message was sent out into the depths of space. Even travelling at the speed of light, it took over 11 hours to reach its target, far beyond the orbit of Pluto. Yet, incredibly, the little probe managed to hear the faint call from its home planet and successfully made the switchover.&lt;/p>
&lt;h4 id="b">B&lt;/h4>
&lt;p>It was the longest-distance repair job history, and a triumph for the NASA engineers. But it also highlighted the astonishing power of the techniques developed by American communications engineer Claude Shannon, who had died just a year earlier. Born in 1916 in Petoskey, Michigan, Shannon showed an early talent for maths and for building gadgets, and made breakthroughs in the foundations of computer technology when still a student. While at Bell Laboratories, Shannon developed information theory, but shunned the resulting acclaim. In the 1940s, he singe-handedly created an entire science of communication which has since inveigled its way into a host of applications, from DVDs to satellite communications to bar codes - any area, in short, where data has to be conveyed rapidly yet accurately.&lt;/p>
&lt;h4 id="c">C&lt;/h4>
&lt;p>This all seems light years away from the down-to earth uses Shannon originally had for his work, which began when he was a 22-year-old graduate engineering student at the prestigious Massachusetts Institute of Technology in 1939. He set out with an apparently simple aim: to pin down the precise meaning of the concept of &amp;lsquo;information&amp;rsquo;. The most basic form of information, Shannon argued, is whether something is true or false—which can be captured in the binary unit, or &amp;lsquo;bit&amp;rsquo;, of the form 1 or 0. Having identified this fundamental unit, Shannon set about defining otherwise vague ideas about information and how to transmit it from place to place. In the process he discovered something surprising: it is always possible to guarantee information will get through random interference—&amp;rsquo;noise&amp;rsquo;—intact.&lt;/p>
&lt;h4 id="d">D&lt;/h4>
&lt;p>Noise usually means unwanted sounds which interfere with genuine information. Information theory generalises this idea via theorems that capture the effects of noise with mathematical precision. In particular, Shannon showed that noise sets a limit on the rate at which information can pass along communication channels while remaining error-free. This rate depends on the relative strengths of the signal and noise travelling down the communication channel, and on its capacity (its &amp;lsquo;bandwidth&amp;rsquo;). The resulting limit, given in units of bits per second, is the absolute maximum rate of error-free communication given signal strength and noise level. The trick, Shannon showed, is to find ways of packaging up—&amp;lsquo;coding&amp;rsquo;—information to cope with the ravages of noise, while staying within the information-carrying capacity—&amp;lsquo;bandwidth&amp;rsquo;—of the communication system being used.&lt;/p>
&lt;h4 id="e">E&lt;/h4>
&lt;p>Over the years scientists have devised many such coding methods, and they have proved crucial in many technological feats. The Voyager spacecraft transmitted data using codes which added one extra bit for every single bit of information; the result was an error rate of just one bit in 10,000—and stunningly clear pictures of the planets. Other codes have become part of everyday life—such as the Universal Product Code, or bar code, which uses a simple error-detecting system that ensures supermarket check-out lasers can read the price even on, say, a crumpled bag of crisps. As recently as 1993, engineers made a major breakthrough by discovering so-called turbo codes - which come very close to Shannon&amp;rsquo;s ultimate limit for the maximum rate that data can be transmitted reliably, and now play a key role in the mobile videophone revolution.&lt;/p>
&lt;h4 id="f">F&lt;/h4>
&lt;p>Shannon also laid the foundations of more efficient ways of storing information, by stripping out superfluous (&amp;lsquo;redundant&amp;rsquo;) bits from data which contributed little real information. As mobile phone text messages like &amp;lsquo;I CN C U&amp;rsquo; show, it is often possible to leave out a lot of data without losing much meaning. As with error correction, however, there&amp;rsquo;s a limit beyond which messages become too ambiguous. Shannon showed how to calculate this limit, opening the way to the design of compression methods that cram maximum information into the minimum space.&lt;/p></description></item><item><title>Docs: R9.4.1 The life and work of Marie Curie</title><link>https://note.codiy.net/ielts/r9.4.1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.codiy.net/ielts/r9.4.1/</guid><description>
&lt;p>Marie Curie is probably the most famous woman scientist who has ever lived. Born Maria Sklodowska in Poland in 1867, she is famous for her work on radioactivity, and was twice a winner of the Nobel Prize. With her husband, Pierre Curie, and Henri Becquerel, she was awarded the 1903 Nobel Prize for Physics, and was then sole winner of the 1911 Nobel Prize for Chemistry. She was the first woman to win a Noble Prize.&lt;/p>
&lt;p>From childhood, Marie was remarkable for her prodigious memory, and at the age of 16 won a gold medal on completion of her secondary education. Because her father lost his savings through bad investment, she then had to take work as a teacher. From her earnings she was able to finance her sister Bronia’s medical studies in Paris, on the understanding that Bronia would, in turn, later help her to get an education.&lt;/p>
&lt;p>In 1891 this promise was fulfilled and Marie went to Paris and began to study at the Sorbonne (the University of Paris). She often worked far into the night and lived on little more than bread and butter and tea. She came first in the examination in the physical sciences in 1893, and in 1894 was placed second in the examination in mathematical sciences. It was not until the spring of that year that she was introduced to Pierre Curie.&lt;/p>
&lt;p>Their marriage in 1895 marked the start of a partnership that was soon to achieve results of world significance. Following Henry Becquerel&amp;rsquo;s discovery in 1896 of a new phenomenon, which Marie later called &amp;lsquo;radioactivity&amp;rsquo;. Marie Curie decided to find out if the radioactivity discovered in uranium was to be found in other elements. She discovered that this was true for thorium.&lt;/p>
&lt;p>Turning her attention to minerals, she found her interest drawn to pitchblende, a mineral whose radioactivity, superior to that of pure uranium, could be explained only by the presence in the ore of small quantities of an unknown substance of very high activity. Pierre Curie joined her in the work that she had undertaken to resolve this problem, and that led to the discovery of the new elements, polonium and radium. While Pierre Curie devoted himself chiefly to the physical study of the new radiations, Marie Curie struggled to obtain pure radium in the metallic state. This was achieved with the help of the chemist Andrè -Louis Debierne, one of Pierre Curie&amp;rsquo;s pupils. Based on the results of this research, Marie Curie received her Doctorate of Science, and in 1903 Marie and Pierre shared with Becquerel the Nobel Prize for Physics for the discovery of radioactivity.&lt;/p>
&lt;p>The births of Marie&amp;rsquo;s two daughters, Irène and Eve, in 1897 and 1904 failed to interrupt her scientific work. She was appointed lecturer in physics at the École Normale Supérieure for girls in Sèrves, France (1900), and introduced a method of teaching based on experimental demonstrations. In December 1904 she was appointed chief assistant in the laboratory directed by Pierre Curie.&lt;/p>
&lt;p>The sudden death of her husband in 1906 was a bitter blow to Marie Curie, but was also a turning point in her career: henceforth she was to devote all her energy to completing alone the scientific work that they had undertaken. On May 13,1906, she was appointed to the professorship that had been left vacant on her husband&amp;rsquo;s death, becoming the first woman to teach at the Sorbonne. In 1911 she was awarded the Nobel Prize for Chemistry for the isolation of a pure form of radium.&lt;/p>
&lt;p>During World War I, Marie Curie, with the help of her daughter Irène devoted herself to the development of the use of X-radiography, including the mobile units which came to be known as &amp;rsquo;little curies&amp;rsquo;, used for the treatment of wounded soldiers. In 1918 the Radium Institute, whose staff lrène had joined, began to operate in earnest, and became a center for nuclear physics and chemistry. Marie Curie, now at the highest point of her fame and, from 1922, a member of the Academy of Medicine, researched the chemistry of radioactive substances and their medical applications.&lt;/p>
&lt;p>In 1921, accompanied by her two daughters, Marie Curie made a triumphant journey to the United States to raise funds for research on radium. Women there presented her with a gram of radium for her campaign. Marie also gave lectures in Belgium, Brazil, Spain and Czechoslovakia and, in addition, had the satisfaction of seeing the development of the Curie Foundation in Paris, and the inauguration in 1932 in Warsaw of the Radium Institute, where her sister Bronia became director.&lt;/p>
&lt;p>One of Marie Curie&amp;rsquo;s outstanding achievements was to have understood the need to accumulate intense radioactive sources, not only to treat illness but also to maintain an abundant supply for research. The existence in Pairs at the Radium Institute of a stock of 1. 5 grams of radium made a decisive contribution to the success of the experiments undertaken in the years around 1930. This work prepared the way for the discovery of the neutron by Sir James Chadwick and, above all, for the discovery in 1934 by Irène and Frédéric Joliot-Curie of artificial radioactivity. A few months after this discovery, Marie Curie died as a result of leukaemia caused by exposure to radiation. She had often carried test tubes containing radioactive isotopes in her pocket, remarking on the pretty blue-green light they gave off.&lt;/p>
&lt;p>Her contribution to physics had been immense, not only in her own work, the importance of which had been demonstrated by her two Nobel Prizes, but because of her influence on subsequent generations of nuclear physicists and chemists.&lt;/p></description></item><item><title>Docs: R9.4.2 Young children's sense of identity</title><link>https://note.codiy.net/ielts/r9.4.2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.codiy.net/ielts/r9.4.2/</guid><description>
&lt;h4 id="a">A&lt;/h4>
&lt;p>A sense of self develops in young children by degrees. The process can usefully be thought of in terms of the gradual emergence of two somewhat separate features: the self as a subject, and the self as an object. William James introduced the distinction in 1892, and contemporaries of his, such as Charles Cooley, added to the developing debate. Ever since then psychologists have continued building on the theory.&lt;/p>
&lt;h4 id="b">B&lt;/h4>
&lt;p>According to James, a child&amp;rsquo;s first step on the road to self-understanding can be seen as the recognition that he or she exits. This is an aspect of the self that he labelled &amp;lsquo;self-as-subject&amp;rsquo;, and he gave it various elements. These included an awareness of one&amp;rsquo;s own agency (i.e. one&amp;rsquo;s power to act), and an awareness of one&amp;rsquo;s distinctiveness from other people. These features gradually emerge as infants explore their world and interact with caregivers. Cooley (1902) suggested that a sense of the self-as-subject was primarily concerned with being able to exercise power. He proposed that the earliest examples of this are an infant&amp;rsquo;s attempts to control physical objects, such as toys or his or her own limbs. This is followed by attempts to affect the behavior of other people. For example, infants learn that when they cry or smile someone responds to them.&lt;/p>
&lt;h4 id="c">C&lt;/h4>
&lt;p>Another powerful source of information for infants about the effects they can have on the world around them is provided when other mimic them. Many parents spend a lot of time, particularly in the early months, copying their infant&amp;rsquo;s vocalizations and expressions. In addition, young children enjoy looking in mirrors, where the movements they can see are dependent upon their own movements. This is not to say that infants recognize the reflection as their own image (a later development). However, Lewis and Brooks-Gunn (1979) suggest that infants&amp;rsquo; developing understanding that the movements they see in the mirror are contingent on their own, leads to a growing awareness that they are distinct from other people. This is because they, and only they, can change the reflection in the mirror.&lt;/p>
&lt;h4 id="d">D&lt;/h4>
&lt;p>This understanding that children gain of themselves as active agents continues to develop in their attempts to co-operate with others in play. Dunn (1988) points out that it is in such day-to-day relationships and interactions that the child&amp;rsquo;s understanding of his- or herself emerges. Empirical investigations of the self-as-subject in young children are, however, rather scarce because of difficulties of communication: even if young infants can reflect on their experience, they certainly cannot express this aspect of the self directly.&lt;/p>
&lt;h4 id="e">E&lt;/h4>
&lt;p>Once children have acquired a certain level of self-awareness, they begin to place themselves in a whole series of categories, which together play such an important part in defining them uniquely as &amp;rsquo;themselves&amp;rsquo;. This second step in the development of a full sense of self is what James called the &amp;lsquo;self-as-object&amp;rsquo;. This has been seen by many to be the aspect of the self which is most influenced by social elements, since it is made up of social roles (such as student, brother, colleague) and characteristics which derive their meaning from comparison or interaction with other people (such as trustworthiness, shyness, sporting ability).&lt;/p>
&lt;h4 id="f">F&lt;/h4>
&lt;p>Cooley and other researchers suggested a close connection between a person&amp;rsquo;s own understanding of their identity and other people&amp;rsquo;s understanding of it. Cooley believed that people build up their sense of identity from the reactions of others to them, and from the view they believe others have of them. He called the self-as-object the &amp;rsquo;looking-glass self&amp;rsquo;, since people come to see themselves as they are reflected in others. Mead (1934) went even further, and saw the self and the social world as inextricably bound together: &amp;lsquo;The self is essentially a social structure, and it arises in social experience … it is impossible to conceive of a self arising outside of social experience.&lt;/p>
&lt;h4 id="g">G&lt;/h4>
&lt;p>Lewis and Brooks-Gunn argued that an important development milestone is reached when children become able to recognize themselves visually without the support of seeing contingent movement. This recognition occurs around their second birthday. In one experiment, Lewis and Brooks-Gunn (1979) dabbed some red powder on the noses of children who were playing in front of a mirror, and then observed how often they touched their noses. The psychologists reasoned that if the children knew what they usually looked like, they would be surprised by the unusual red mark and would start touching it. On the other hand, they found that children of 15 to 18 months are generally not able to recognize themselves unless other cues such as movement are present.&lt;/p>
&lt;h4 id="h">H&lt;/h4>
&lt;p>Finally, perhaps the most graphic expressions of self-awareness in general can be seen in the displays of rage which are most common from 18 months to 3 years of age. In a longitudinal study of groups of three or four children, Bronson (1975) found that the intensity of the frustration and anger in their disagreements increased sharply between the ages of 1 and 2 years. Often, the children&amp;rsquo;s disagreements involved a struggle over a toy that none of them had played with before or after the tug-of-war: the children seemed to be disputing ownership rather than wanting to play with it. Although it may be less marked in other societies, the link between the sense of &amp;lsquo;self&amp;rsquo; and of &amp;lsquo;ownership&amp;rsquo; is a notable feature of childhood in Western societies.&lt;/p></description></item><item><title>Docs: R9.4.3 The Development of Museums</title><link>https://note.codiy.net/ielts/r9.4.3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.codiy.net/ielts/r9.4.3/</guid><description>
&lt;h4 id="a">A&lt;/h4>
&lt;p>The conviction that historical relics provide infallible testimony about the past is rooted in the nineteenth and early twentieth centuries, when science was regarded as objective and value free. As one writer observes: ‘Although it is now evident that artefacts are as easily altered as chronicles, public faith in their veracity endures: a tangible relic seems ipso facto real. Such conviction was, until recently, reflected in museum displays. Museums used to look – and some still do – much like storage rooms of objects packed together in showcases: good for scholars who wanted to study the subtle differences in design, but not for the ordinary visitor, to whom it all looked alike. Similarly, the information accompanying the objects often made little sense to the lay visitor. The content and format of explanations dated back to a time when the museum was the exclusive domain of the scientific researcher.&lt;/p>
&lt;h4 id="b">B&lt;/h4>
&lt;p>Recently, however, attitudes towards history and the way it should be presented have altered. The key word in heritage display is now ‘experience’, the more exciting the better and if possible, involving all the senses. Good examples of this approach in the UK are the Jorvik Center in York; the National Museum of Photography, Film and Television in Bradford; and the Imperial War Museum in London. In the US the trend emerged much earlier: Williamsburg has been a prototype for many heritage developments in other parts of the world. No one can predict where the process will end. On so-called heritage sites the re-enactment of historical events is increasingly popular, and computers will soon provide virtual reality experiences, which will present visitors with a vivid image of the period of their choice, in which they themselves can act as if part of the historical environment. Such developments have been criticized as an intolerable vulgarization, but the success of many historical theme parks and similar locations suggests that the majority of the public does not share this opinion.&lt;/p>
&lt;h4 id="c">C&lt;/h4>
&lt;p>In a related development, the sharp distinction between museum and heritage sites on the one hand, and theme parks on the other, is gradually evaporating. They already borrow ideas and concepts from one another. For example, museums have adopted story lines for exhibitions, sites have accepted ‘theming’ as a relevant tool, and theme parks are moving towards more authenticity and research-based presentations. In zoos, animals are no longer kept in cages, but in great spaces, either in the open air or in enormous greenhouses, such as the jungle and desert environments in Burgers’ Zoo in Holland. This particular trend is regarded as one of the major developments in the presentation of natural history in the twentieth century.&lt;/p>
&lt;h4 id="d">D&lt;/h4>
&lt;p>Theme parks are undergoing other changes, too, as they try to present more serious social and cultural issues, and move away from fantasy. This development is a response to market forces and , although museums and heritage sites have a special, rather distinct, role to fulfil, they are also operating in a very competitive environments, where visitors make choices on how and where to spend their free time. Heritage and museum experts do not have to invent stories and recreate historical environments to attract their visitors: their assets are already in place. However, exhibits must be both based on artefacts and facts as we know them, and attractively presented. Those who are professionally engaged in the art of interpreting history are thus in a difficult position, as they must steer a narrow course between the demands of ‘evidence’ and ‘attractiveness’, especially given the increasing need in the heritage industry for income-generating activities.&lt;/p>
&lt;h4 id="e">E&lt;/h4>
&lt;p>It could be claimed that in order to make everything in heritage more ‘real’, historical accuracy must be increasingly altered. For example, Pithecanthropus erectus is depicted in an Indonesian museum with Malay facial features, because this corresponds to public perceptions. Similarly, in the Museum of Natural History in Washington, Neanderthal man is shown making a dominant gesture to his wife. Such presentations tell us more about contemporary perceptions of the world than about our ancestors. There is one compensation, however, for the professionals who make these interpretations: if they did not provide the interpretation, visitors would do it for themselves, based on their own ideas, misconceptions and prejudices. And no matter how exciting the result, it would contain a lot more bias than the presentations provided by experts.&lt;/p>
&lt;h4 id="f">F&lt;/h4>
&lt;p>Human bias is inevitable, but another source of bias in the representation of history has to do with the transitory nature of the materials themselves. The simple fact is that not everything from history survives the historical process. Castles, palaces and cathedrals have a longer lifespan than the dwellings of ordinary people. The same applies to the furnishings and other contents of the premises. In a town like Leyden in Holland, which in the seventeenth century was occupied by approximately the same number of inhabitants as today, people lived within the walled town, an area more than five times smaller than modern Leyden. In most of the houses several families lived together in circumstances beyond our imagination. Yet in museums, fine period rooms give only an image of the lifestyle of the upper class of that era. No wonder that people who stroll around exhibitions are filled with nostalgia; the evidence in museums indicates that life was so much better in the past. This notion is induced by the bias in its representation in museums and heritage centers.&lt;/p></description></item></channel></rss>