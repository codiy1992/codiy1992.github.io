<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Notebook – Reading</title><link>https://note.codiy.net/docs/ielts/reading/</link><description>Recent content in Reading on Notebook</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://note.codiy.net/docs/ielts/reading/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: R5.4.1 The Impact of Wilderness Tourism</title><link>https://note.codiy.net/ielts/r5.4.1</link><pubDate>Mon, 15 Aug 2022 13:13:12 +0800</pubDate><guid>https://note.codiy.net/ielts/r5.4.1</guid><description>
&lt;h4 id="a">A&lt;/h4>
&lt;p>The market for tourism in remote areas is booming as never before. Countries all across the world are actively promoting their ‘wilderness’ regions - such as mountains, Arctic lands, deserts, small islands and wetlands - to high-spending tourists. The attraction of these areas is obvious: by definition, wilderness tourism requires little or no initial investment. But that does not mean that there is no cost. As the 1992 United Nations Conference on Environment and Development recognized, these regions are fragile (i.e. highly vulnerable to abnormal pressures) not just in terms of their ecology, but also in terms of the culture of their inhabitants. The three most significant types of fragile environment in these respects, and also in terms of the proportion of the Earth’s surface they cover, are deserts, mountains and Arctic areas. An important characteristic is their marked seasonality, with harsh conditions prevailing for many months each year. Consequently, most human activities, including tourism, are limited to quite clearly defined parts of the year.&lt;/p>
&lt;p>Tourists are drawn to these regions by their natural landscape beauty and the unique cultures of their indigenous people. And poor governments in these isolated areas have welcomed the new breed of ‘adventure tourist’, grateful for the hard currency they bring. For several years now, tourism has been the prime source of foreign exchange in Nepal and Bhutan. Tourism is also a key element in the economies of Arctic zones such as Lapland and Alaska and in desert areas such as Ayers Rock in Australia and Arizona’s Monument Valley.&lt;/p>
&lt;h4 id="b">B&lt;/h4>
&lt;p>Once a location is established as a main tourist destination, the effects on the local community are profound. When hill-farmers, for example, can make more money in a few weeks working as porters for foreign trekkers than they can in a year working in their fields, it is not surprising that many of them give up their farm-work, which is thus left to other members of the family. In some hill-regions, this has led to a serious decline in farm output and a change in the local diet, because there is insufficient labour to maintain terraces and irrigation systems and tend to crops. The result has been that many people in these regions have turned to outside supplies of rice and other foods.&lt;/p>
&lt;p>In Arctic and desert societies, year-round survival has traditionally depended on hunting animals and fish and collecting fruit over a relatively short season. However, as some inhabitants become involved in tourism, they no longer have time to collect wild food; this has led to increasing dependence on bought food and stores. Tourism is not always the culprit behind such changes. All kinds of wage labour, or government handouts, tend to undermine traditional survival systems. Whatever the cause, the dilemma is always the same: what happens if these new, external sources of income dry up?&lt;/p>
&lt;p>The physical impact of visitors is another serious problem associated with the growth in adventure tourism. Much attention has focused on erosion along major trails, but perhaps more important are the deforestation and impacts on water supplies arising from the need to provide tourists with cooked food and hot showers. In both mountains and deserts, slow-growing trees are often the main sources of fuel and water supplies may be limited or vulnerable to degradation through heavy use.&lt;/p>
&lt;h4 id="c">C&lt;/h4>
&lt;p>Stories about the problems of tourism have become legion in the last few years. Yet it does not have to be a problem. Although tourism inevitably affects the region in which it takes place, the costs to these fragile environments and their local cultures can be minimized. Indeed, it can even be a vehicle for reinvigorating local cultures, as has happened with the Sherpas of Nepal’s Khumbu Valley and in some Alpine villages. And a growing number of adventure tourism operators are trying to ensure that their activities benefit the local population and environment over the long term.&lt;/p>
&lt;p>In the Swiss Alps, communities have decided that their future depends on integrating tourism more effectively with the local economy. Local concern about the rising number of second home developments in the Swiss Pays d’Enhaut resulted in limits being imposed on their growth. There has also been a renaissance in communal cheese production in the area, providing the locals with a reliable source of income that does not depend on outside visitors.&lt;/p>
&lt;p>Many of the Arctic tourist destinations have been exploited by outside companies, who employ transient workers and repatriate most of the profits to their home base. But some Arctic communities are now operating tour businesses themselves, thereby ensuring that the benefits accrue locally. For instance, a native corporation in Alaska, employing local people, is running an air tour from Anchorage to Kotzebue, where tourists eat Arctic food, walk on the tundra and watch local musicians and dancers.&lt;/p>
&lt;p>Native people in the desert regions of the American Southwest have followed similar strategies, encouraging tourists to visit their pueblos and reservations to purchase high-quality handicrafts and artwork. The Acoma and San Ildefonso pueblos have established highly profitable pottery businesses, while the Navajo and Hopi groups have been similarly successful with jewellery.&lt;/p>
&lt;p>Too many people living in fragile environments have lost control over their economies, their culture and their environment when tourism has penetrated their homelands. Merely restricting tourism cannot be the solution to the imbalance, because people’s desire to see new places will not just disappear. Instead, communities in fragile environments must achieve greater control over tourism ventures in their regions, in order to balance their needs and aspirations with the demands of tourism. A growing number of communities are demonstrating that, with firm communal decision-making, this is possible. The critical question now is whether this can become the norm, rather than the exception.&lt;/p></description></item><item><title>Docs: R5.4.2 Flawed Beauty： the problem with toughened glass</title><link>https://note.codiy.net/ielts/r5.4.2</link><pubDate>Mon, 15 Aug 2022 13:13:12 +0800</pubDate><guid>https://note.codiy.net/ielts/r5.4.2</guid><description>
&lt;p>On 2nd August 1999, a particularly hot day in the town of Cirencester in the UK, a large pane of toughened glass in the roof of a shopping centre at Bishops Walk shattered without warning and fell from its frame. When fragments were analysed by experts at the giant glass manufacturer Pilkington, which had made the pane, they found that minute crystals of nickel sulphide trapped inside the glass had almost certainly caused the failure.&lt;/p>
&lt;p>&amp;lsquo;The glass industry is aware of the issue,&amp;rsquo; says Brian Waldron, chairman of the standards committee at the Glass and Glazing Federation, a British trade association, and standards development officer at Pilkington. But he insists that cases are few and far between. &amp;lsquo;It&amp;rsquo;s a very rare phenomenon,&amp;rsquo; he says.&lt;/p>
&lt;p>Others disagree. &amp;lsquo;On average I see about one or two buildings a month suffering from nickel sulphide related failures,&amp;rsquo; says Barrie Josie, a consultant engineer involved in the Bishops Walk investigation. Other experts tell of similar experiences. Tony Wilmott of London-based consulting engineers Sandberg, and Simon Armstrong at CladTech Associates in Hampshire both say they know of hundreds of cases. &amp;lsquo;What you hear is only the tip of the iceberg,&amp;rsquo; says Trevor Ford, a glass expert at Resolve Engineering in Brisbane, Queensland. He believes the reason is simple: &amp;lsquo;No-one wants bad press.&amp;rsquo;&lt;/p>
&lt;p>Toughened glass is found everywhere, from cars and bus shelters to the windows, walls and roofs of thousands of buildings around the world. It&amp;rsquo;s easy to see why. This glass has five times the strength of standard glass, and when it does break it shatters into tiny cubes rather than large, razor-sharp shards. Architects love it because large panels can be bolted together to make transparent walls, and turning it into ceilings and floors is almost as easy.&lt;/p>
&lt;p>It is made by heating a sheet of ordinary glass to about 620℃ to soften it slightly, allowing its structure to expand, and then cooling it rapidly with jets of cold air. This causes the outer layer of the pane to contract and solidify before the interior. When the interior finally solidifies and shrinks, it exerts a pull on the outer layer that leaves it in permanent compression and produces a tensile force inside the glass. As cracks propagate best in materials under tension, the compressive force on the surface must be overcome before the pane will break, making it more resistant to cracking.&lt;/p>
&lt;p>The problem starts when glass contains nickel sulphide impurities. Trace amounts of nickel and sulphur are usually present in the raw materials used to make glass, and nickel can also be introduced by fragments of nickel alloys falling into the molten glass. As the glass is heated, these atoms react to form tiny crystals of nickel sulphide. Just a tenth of a gram of nickel in the furnace can create up to 50,000 crystals.&lt;/p>
&lt;p>These crystals can exist in two forms: a dense form called the alpha phase, which is stable at high temperatures, and a less dense form called the beta phase, which is stable at room temperatures. The high temperatures used in the toughening process convert all the crystals to the dense, compact alpha form. But the subsequent cooling is so rapid that the crystals don’t have time to change back to the beta phase. This leaves unstable alpha crystals in the glass, primed like a coiled spring, ready to revert to the beta phase without warning.&lt;/p>
&lt;p>When this happens, the crystals expand by up to 4%. And if they are within the central, tensile region of the pane, the stresses this unleashes can shatter the whole sheet. The time that elapses before failure occurs is unpredictable. It could happen just months after manufacture, or decades later, although if the glass is heated - by sunlight, for example - the process is speeded up. Ironically, says Graham Dodd, of consulting engineers Arup in London, the oldest pane of toughened glass known to have failed due to nickel sulphide inclusions was in Pilkington&amp;rsquo;s glass research building in Lathom, Lancashire. The pane was 27 years old.&lt;/p>
&lt;p>Data showing the scale of the nickel sulphide problem is almost impossible to find. The picture is made more complicated by the fact that these crystals occur in batches. So even if, on average, there is only one inclusion in 7 tonnes of glass, if you experience one nickel sulphide failure in your building, that probably means you&amp;rsquo;ve got a problem in more than one pane. Josie says that in the last decade he has worked on over 15 buildings with the number of failures into double figures.&lt;/p>
&lt;p>One of the worst examples of this is Waterfront Place, which was completed in 1990. Over the following decade the 40-storey Brisbane block suffered a rash of failures. Eighty panes of its toughened glass shattered due to inclusions before experts were finally called in. John Barry, an expert in nickel sulphide contamination at the University of Queensland, analysed every glass pane in the building. Using a studio camera, a photographer went up in a cradle to take photos of every pane. These were scanned under a modified microfiche reader for signs of nickel sulphide crystals. &amp;lsquo;We discovered at least another 120 panes with potentially dangerous inclusions which were then replaced,&amp;rsquo; says Barry. &amp;lsquo;It was a very expensive and time-consuming process that took around six months to complete.&amp;rsquo; Though the project cost A$1.6 million (nearly ￡700,000), the alternative - re-cladding the entire building - would have cost ten times as much.&lt;/p></description></item><item><title>Docs: R5.4.3 The effects of light on plant and animal species</title><link>https://note.codiy.net/ielts/r5.4.3</link><pubDate>Mon, 15 Aug 2022 13:13:12 +0800</pubDate><guid>https://note.codiy.net/ielts/r5.4.3</guid><description>
&lt;p>Light is important to organisms for two different reasons. Firstly it is used as a cue for the timing of daily and seasonal rhythms in both plants and animals, and secondly it is used to assist growth in plants.&lt;/p>
&lt;p>Breeding in most organisms occurs during a part of the year only, and so a reliable cue is needed to trigger breeding behaviour. Day length is an excellent cue, because it provides a perfectly predictable pattern of change within the year. In the temperate zone in spring, temperatures fluctuate greatly from day to day, but day length increases steadily by a predictable amount. The seasonal impact of day length on physiological responses is called photoperiodism, and the amount of experimental evidence for this phenomenon is considerable. For example, some species of birds&amp;rsquo; breeding can be induced even in midwinter simply by increasing day length artificially (Wolfson 1964). Other examples of photoperiodism occur in plants. A short-day plant flowers when the day is less than a certain critical length. A long-day plant flowers after a certain critical day length is exceeded. In both cases the critical day length differs from species to species. Plants which flower after a period of vegetative growth, regardless of photoperiod, are known as day-neutral plants.&lt;/p>
&lt;p>Breeding seasons in animals such as birds have evolved to occupy the part of the year in which offspring have the greatest chances of survival. Before the breeding season begins, food reserves must be built up to support the energy cost of reproduction, and to provide for young birds both when they are in the nest and after fiedging. Thus many temperate-zone birds use the increasing day lengths in spring as a cue to begin the nesting cycle, because this is a point when adequate food resources will be assured.&lt;/p>
&lt;p>The adaptive significance of photoperiodism in plants is also clear. Short-day plants that flower in spring in the temperate zone are adapted to maximising seedling growth during the growing season. Long-day plants are adapted for situations that require fertilization by insects, or a long period of seed ripening. Short-day plants that flower in the autumn in the temperate zone are able to build up food reserves over the growing season and over winter as seeds. Day-neutral plants have an evolutionary advantage when the connection between the favourable period for reproduction and day length is much less certain. For example, desert annuals germinate, flower and seed whenever suitable rainfall occurs, regardless of the day length.&lt;/p>
&lt;p>The breeding season of some plants can be delayed to extraordinary lengths. Bamboos are perennial grasses that remain in a vegetative state for many years and then suddenly flower, fruit and die (Evans 1976). Every bamboo of the species Chusquea abietifolia on the island of Jamaica flowered, set seed and died during 1884. The next generation of bamboo flowered and died between 1916 and 1918, which suggests a vegetative cycle of about 31 years. The climatic trigger for this flowering cycle is not yet known, but the adaptive significance is clear. The simultaneous production of masses of bamboo seeds (in some cases lying 12 to 15 centimetres deep on the ground) is more than all the seed-eating animals can cope with at the time, so that some seeds escape being eaten and grow up to form the next generation (Evans 1976).&lt;/p>
&lt;p>The second reason light is important to organisms is that it is essential for photosynthesis. This is the process by which plants use energy from the sun to convert carbon from soil or water into organic material for growth. The rate of photosynthesis in a plant can be measured by calculating the rate of its uptake of carbon. There is a wide range of photosynthetic responses of plants to variations in light intensity. Some plants reach maximal photosynthesis at one-quarter full sunlight, and others, like sugarcane, never reach a maximum, but continue to increase photosynthesis rate as light intensity rises.&lt;/p>
&lt;p>Plants in general can be divided into two groups: shade-tolerant species and shade-intolerant species. This classification is commonly used in forestry and horticulture. Shade-tolerant plants have lower photosynthetic rates and hence have lower growth rates than those of shade-intolerant species. Plant species become adapted to living in a certain kind of habitat, and in the process evolve a series of characteristics that prevent them from occupying other habitats. Grime (1966) suggests that light may be one of the major components directing these adaptations. For example, eastern hemlock seedlings are shade-tolerant. They can survive in the forest understorey under very low light levels because they have a low photosynthetic rate.&lt;/p></description></item><item><title>Docs: R5.3.1 Early Childhood Education</title><link>https://note.codiy.net/ielts/r5.3.1</link><pubDate>Fri, 12 Aug 2022 13:17:12 +0800</pubDate><guid>https://note.codiy.net/ielts/r5.3.1</guid><description>
&lt;blockquote>
&lt;p>New Zealand’s National Party spokesman on education, Dr Lockwood Smith, recently visited the US and Britain. Here he reports on the findings of his trip and what they could mean for New Zealand&amp;rsquo;s education policy&lt;/p>
&lt;/blockquote>
&lt;h4 id="a">A&lt;/h4>
&lt;p>‘Education To Be More’ was published last August. It was the report of the New Zealand Government&amp;rsquo;s Early Childhood Care and Education Working Group. The report argued for enhanced equity of access and better funding for childcare and early childhood education institutions. Unquestionably, that&amp;rsquo;s a real need; but since parents don&amp;rsquo;t normally send children to pre-schools until the age of three, are we missing out on the most important years of all?&lt;/p>
&lt;h4 id="b">B&lt;/h4>
&lt;p>A 13-year study of early childhood development at Harvard University has shown that, by the age of three, most children have the potential to understand about 1000 words - most of the language they will use in ordinary conversation for the rest of their lives.&lt;/p>
&lt;p>Furthermore, research has shown that while every child is born with a natural curiosity, it can be suppressed dramatically during the second and third years of life. Researchers claim that the human personality is formed during the first two years of life, and during the first three years children learn the basic skills they will use in all their later learning both at home and at school. Once over the age of three, children continue to expand on existing knowledge of the world.&lt;/p>
&lt;h4 id="c">C&lt;/h4>
&lt;p>It is generally acknowledged that young people from poorer socio-economic backgrounds tend to do less well in our education system. That&amp;rsquo;s observed not just in New Zealand, but also in Australia, Britain and America. In an attempt to overcome that educational under-achievement, a nationwide programme called &amp;lsquo;Headstart&amp;rsquo; was launched in the United States in 1965. A lot of money was poured into it. It took children into pre-school institutions at the age of three and was supposed to help the children of poorer families succeed in school.&lt;/p>
&lt;p>Despite substantial funding, results have been disappointing. It is thought that there are two explanations for this. First, the programme began too late. Many children who entered it at the age of three were already behind their peers in language and measurable intelligence. Second, the parents were not involved. At the end of each day, &amp;lsquo;Headstart&amp;rsquo; children returned to the same disadvantaged home environment.&lt;/p>
&lt;h4 id="d">D&lt;/h4>
&lt;p>As a result of the growing research evidence of the importance of the first three years of a child&amp;rsquo;s life and the disappointing results from &amp;lsquo;Headstart&amp;rsquo;, a pilot programme was launched in Missouri in the US that focused on parents as the child&amp;rsquo;s first teachers. The &amp;lsquo;Missouri&amp;rsquo; programme was predicated on research showing that working with the family, rather than bypassing the parents, is the most effective way of helping children get off to the best possible start in life. The four-year pilot study included 380 families who were about to have their first child and who represented a cross-section of socio-economic status, age and family configurations. They included single-parent and two-parent families, families in which both parents worked, and families with either the mother or father at home.&lt;/p>
&lt;p>The programme involved trained parent-educators visiting the parents&amp;rsquo; home and working with the parent, or parents, and the child. Information on child development, and guidance on things to look for and expect as the child grows were provided, plus guidance in fostering the child&amp;rsquo;s intellectual, language, social and motor-skill development. Periodic check-ups of the child&amp;rsquo;s educational and sensory development (hearing and vision) were made to detect possible handicaps that interfere with growth and development. Medical problems were referred to professionals.&lt;/p>
&lt;p>Parent-educators made personal visits to homes and monthly group meetings were held with other new parents to share experience and discuss topics of interest. Parent resource centres, located in school buildings, offered learning materials for families and facilitators for child care.&lt;/p>
&lt;h4 id="e">E&lt;/h4>
&lt;p>At the age of three, the children who had been involved in the &amp;lsquo;Missouri&amp;rsquo; programme were evaluated alongside a cross-section of children selected from the same range of socio-economic backgrounds and family situations, and also a random sample of children that age. The results were phenomenal. By the age of three, the children in the programme were significantly more advanced in language development than their peers, had made greater strides in problem solving and other intellectual skills, and were further along in social development. In fact, the average child on the programme was performing at the level of the top 15 to 20 per cent of their peers in such things as auditory comprehension, verbal ability and language ability.&lt;/p>
&lt;p>Most important of all, the traditional measures of &amp;lsquo;risk&amp;rsquo;, such as parents&amp;rsquo; age and education, or whether they were a single parent, bore little or no relationship to the measures of achievement and language development. Children in the programme performed equally well regardless of socio-economic disadvantages. Child abuse was virtually eliminated. The one factor that was found to affect the child s development was family stress leading to a poor quality of parent-child interaction. That interaction was not necessarily bad in poorer families.&lt;/p>
&lt;h4 id="f">F&lt;/h4>
&lt;p>These research findings are exciting. There is growing evidence in New Zealand that children from poorer socio-economic backgrounds are arriving at school less well developed and that our school system tends to perpetuate that disadvantage. The initiative outlined above could break that cycle of disadvantage. The concept of working with parents in their homes, or at their place of work, contrasts quite markedly with the report of the Early Childhood Care and Education Working Group. Their focus is on getting children and mothers access to childcare and institutionalised early childhood education. Education from the age of three to five is undoubtedly vital, but without a similar focus on parent education and on the vital importance of the first three years, some evidence indicates that it will not be enough to overcome educational inequity.&lt;/p></description></item><item><title>Docs: R5.3.2 Disappearing Delta</title><link>https://note.codiy.net/ielts/r5.3.2</link><pubDate>Fri, 12 Aug 2022 13:17:12 +0800</pubDate><guid>https://note.codiy.net/ielts/r5.3.2</guid><description>
&lt;h4 id="a">A&lt;/h4>
&lt;p>The fertile land of the Nile delta is being eroded along Egypt’s Mediterranean coast at an astounding rate, in some parts estimated at 100 metres per year. In the past, land scoured away from the coastline by the currents of the Mediterranean Sea used to be replaced by sediment brought down to the delta by the River Nile, but this is no longer happening.&lt;/p>
&lt;h4 id="b">B&lt;/h4>
&lt;p>Up to now, people have blamed this loss of delta land on the two large dams at Aswan in the south of Egypt, which hold back virtually all of the sediment that used to flow down the river. Before the dams were built, the Nile flowed freely, carrying huge quantities of sediment north from Africa’s interior to be deposited on the Nile delta. This continued for 7,000 years, eventually covering a region of over 22,000 square kilometres with layers of fertile silt . Annual flooding brought in new, nutrient-rich soil to the delta region, replacing what had been washed away by the sea, and dispensing with the need for fertilizers in Egypt’s richest food-growing area. But when the Aswan dams were constructed in the 20th century to provide electricity and irrigation, and to protect the huge population centre of Cairo and its surrounding areas from annual flooding and drought, most of the sediment with its natural fertilizer accumulated up above the dam in the southern, upstream half of Lake Nasser, instead of passing down to the delta.&lt;/p>
&lt;h4 id="c">C&lt;/h4>
&lt;p>Now, however, there turns out to be more to the story. It appears that the sediment-free water emerging from the Aswan dams picks up silt and sand as it erodes the river bed and banks on the 800-kilometre trip to Cairo. Daniel Jean Stanley of the Smithsonian Institute noticed that water samples taken in Cairo, just before the river enters the delta, indicated that the river sometimes carries more than 850 grams of sediment per cubic metre of water - almost half of what it carried before the dams were built. ‘I&amp;rsquo;m ashamed to say that the significance of this didn’t strike me until after I had read 50 or 60 studies,’ says Stanley in Marine Geology. ‘There is still a lot of sediment coming into the delta, but virtually no sediment comes out into the Mediterranean to replenish the coastline. So this sediment must be trapped on the delta itself.’&lt;/p>
&lt;h4 id="d">D&lt;/h4>
&lt;p>Once north of Cairo, most of the Nile water is diverted into more than 10,000 kilometres of irrigation canals and only a small proportion reaches the sea directly through the rivers in the delta. The water in the irrigation canals is still or very slow-moving and thus cannot carry sediment, Stanley explains. The sediment sinks to the bottom of the canals and then is added to fields by farmers or pumped with the water into the four large freshwater lagoons that are located near the outer edges of the delta. So very little of it actually reaches the coastline to replace what is being washed away by the Mediterranean currents.&lt;/p>
&lt;h4 id="e">E&lt;/h4>
&lt;p>The farms on the delta plains and fishing and aquaculture in the lagoons account for much of Egypt’s food supply. But by the time the sediment has come to rest in the fields and lagoons it is laden with municipal, industrial and agricultural waste from the Cairo region, which is home to more than 40 million people. ‘Pollutants are building up faster and faster,’ says Stanley.&lt;/p>
&lt;p>Based on his investigations of sediment from the delta lagoons, Frederic Siegel of George Washington University concurs. ‘In Manzalah Lagoon, for example, the increase in mercury, lead, copper and zinc coincided with the building of the High Dam at Aswan, the availability of cheap electricity, and the development of major power-based industries,’ he says. Since that time the concentration of mercury has increased significantly. Lead from engines that use leaded fuels and from other industrial sources has also increased dramatically. These poisons can easily enter the food chain, affecting the productivity of fishing and farming. Another problem is that agricultural wastes include fertilizers which stimulate increases in plant growth in the lagoons and upset the ecology of the area, with serious effects on the fishing industry.&lt;/p>
&lt;h4 id="f">F&lt;/h4>
&lt;p>According to Siegel, international environmental organisations are beginning to pay closer attention to the region, partly because of the problems of erosion and pollution of the Nile delta, but principally because they fear the impact this situation could have on the whole Mediterranean coastal ecosystem. But there are no easy solutions. In the immediate future, Stanley believes that one solution would be to make artificial floods to flush out the delta waterways, in the same way that natural floods did before the construction of the dams. He says, however, that in the long term an alternative process such as desalination may have to be used to increase the amount of water available. ‘In my view, Egypt must devise a way to have more water running through the river and the delta,’ says Stanley. Easier said than done in a desert region with a rapidly growing population.&lt;/p></description></item><item><title>Docs: R5.3.3 The Return of Artificial Intelligence</title><link>https://note.codiy.net/ielts/r5.3.3</link><pubDate>Fri, 12 Aug 2022 13:17:12 +0800</pubDate><guid>https://note.codiy.net/ielts/r5.3.3</guid><description>
&lt;blockquote>
&lt;p>It is becoming acceptable again to talk of computers performing human tasks such as problem-solving and pattern-recognition&lt;/p>
&lt;/blockquote>
&lt;h4 id="a">A&lt;/h4>
&lt;p>After years in the wilderness, the term &amp;lsquo;artificial intelligence&amp;rsquo; (AI) seems poised to make a comeback. AI was big in the 1980s but vanished in the 1990s. It re-entered public consciousness with the release of AI, a movie about a robot boy. This has ignited public debate about AI, but the term is also being used once more within the computer industry. Researchers, executives and marketing people are now using the expression without irony or inverted commas. And it is not always hype. The term is being applied, with some justification, to products that depend on technology that was originally developed by AI researchers. Admittedly, the rehabilitation of the term has a long way to go, and some firms still prefer to avoid using it. But the fact that others are starting to use it again suggests that AI has moved on from being seen as an over-ambitious and under-achieving field of research.&lt;/p>
&lt;h4 id="b">B&lt;/h4>
&lt;p>The field was launched, and the term ‘artificial intelligence’ coined, at a conference in 1956 by a group of researchers that included Marvin Minsky, John McCarthy, Herbert Simon and Alan Newell, all of whom went on to become leading figures in the field. The expression provided an attractive but informative name for a research programme that encompassed such previously disparate fields as operations research, cybernetics, logic and computer science. The goal they shared was an attempt to capture or mimic human abilities using machines. That said, different groups of researchers attacked different problems, from speech recognition to chess playing, in different ways; AI unified the field in name only. But it was a term that captured the public imagination.&lt;/p>
&lt;h4 id="c">C&lt;/h4>
&lt;p>Most researchers agree that AI peaked around 1985. A public reared on science-fiction movies and excited by the growing power of computers had high expectations. For years, AI researchers had implied that a breakthrough was just around the corner. Marvin Minsky said in 1967 that within a generation the problem of creating ‘artificial intelligence’ would be substantially solved. Prototypes of medical-diagnosis programs and speech recognition software appeared to be making progress. It proved to be a false dawn. Thinking computers and household robots failed to materialise, and a backlash ensued. ‘There was undue optimism in the early 1980s,’ says David Leake, a researcher at Indiana University. ‘Then when people realised these were hard problems, there was retrenchment. By the late 1980s, the term AI was being avoided by many researchers, who opted instead to align themselves with specific sub-disciplines such as neural networks, agent technology, case-based reasoning, and so on.’&lt;/p>
&lt;h4 id="d">D&lt;/h4>
&lt;p>Ironically, in some ways AI was a victim of its own success. Whenever an apparently mundane problem was solved, such as building a system that could land an aircraft unattended, the problem was deemed not to have been AI in the first place. ‘If it works, it can’t be AI,’ as Dr Leake characterises it. The effect of repeatedly moving the goal-posts in this way was that AI came to refer to ‘blue-sky’ research that was still years away from commercialisation. Researchers joked that AI stood for ‘almost implemented’. Meanwhile, the technologies that made it onto the market, such as speech recognition, language translation and decision-support software, were no longer regarded as AI. Yet all three once fell well within the umbrella of AI research.&lt;/p>
&lt;h4 id="e">E&lt;/h4>
&lt;p>But the tide may now be turning, according to Dr Leake. HNC Software of San Diego, backed by a government agency, reckon that their new approach to artificial intelligence is the most powerful and promising approach ever discovered. HNC claim that their system, based on a cluster of 30 processors, could be used to spot camouflaged vehicles on a battlefield or extract a voice signal from a noisy background - tasks humans can do well, but computers cannot. ‘Whether or not their technology lives up to the claims made for it, the fact that HNC are emphasising the use of AI is itself an interesting development,’ says Dr Leake.&lt;/p>
&lt;h4 id="f">F&lt;/h4>
&lt;p>Another factor that may boost the prospects for AI in the near future is that investors are now looking for firms using clever technology, rather than just a clever business model, to differentiate themselves. In particular, the problem of information overload, exacerbated by the growth of e-mail and the explosion in the number of web pages, means there are plenty of opportunities for new technologies to help filter and categorise information - classic AI problems. That may mean that more artificial intelligence companies will start to emerge to meet this challenge.&lt;/p>
&lt;h4 id="g">G&lt;/h4>
&lt;p>The 1969 film, 2001: A Space Odyssey, featured an intelligent computer called HAL 9000. As well as understanding and speaking English, HAL could play chess and even learned to lipread. HAL thus encapsulated the optimism of the 1960s that intelligent computers would be widespread by 2001. But 2001 has been and gone, and there is still no sign of a HAL-like computer. Individual systems can play chess or transcribe speech, but a general theory of machine intelligence still remains elusive. It may be, however, that the comparison with HAL no longer seems quite so important, and AI can now be judged by what it can do, rather than by how well it matches up to a 30-year-old science-fiction film. ‘People are beginning to realise that there are impressive things that these systems can do,’ says Dr Leake hopefully.&lt;/p></description></item><item><title>Docs: R5.2.1 BAKELITE</title><link>https://note.codiy.net/ielts/r5.2.1</link><pubDate>Thu, 11 Aug 2022 19:17:12 +0800</pubDate><guid>https://note.codiy.net/ielts/r5.2.1</guid><description>
&lt;h4 id="the-birth-of-modern-plastics">The birth of modern plastics&lt;/h4>
&lt;p>In 1907, Leo Hendrick Baekeland, a Belgian scientist working in New York, discovered and patented a revolutionary new synthetic material. His invention, which he named &amp;lsquo;Bakelite&amp;rsquo;, was of enormous technological importance, and effectively launched the modern plastics industry.&lt;/p>
&lt;p>The term &amp;lsquo;plastic&amp;rsquo; comes from the Greek plassein, meaning &amp;rsquo;to mould&amp;rsquo;. Some plastics are derived from natural sources, some are semi-synthetic (the result of chemical action on a natural substance), and some are entirely synthetic, that is, chemically engineered from the constituents of coal or oil. Some are &amp;rsquo;thermoplastic&amp;rsquo;, which means that, like candlewax, they melt when heated and can then be reshaped. Others are &amp;rsquo;thermosetting&amp;rsquo;: like eggs, they cannot revert to their original viscous state, and their shape is thus fixed for ever. Bakelite had the distinction of being the first totally synthetic thermosetting plastic.&lt;/p>
&lt;p>The history of today&amp;rsquo;s plastics begins with the discovery of a series of semi-synthetic thermoplastic materials in the mid-nineteenth century. The impetus behind the development of these early plastics was generated by a number of factors - immense technological progress in the domain of chemistry, coupled with wider cultural changes, and the pragmatic need to find acceptable substitutes for dwindling supplies of &amp;rsquo;luxury&amp;rsquo; materials such as tortoiseshell and ivory.&lt;/p>
&lt;p>Baekeland&amp;rsquo;s interest in plastics began in 1885 when, as a young chemistry student in Belgium, he embarked on research into phenolic resins, the group of sticky substances produced when phenol (carbolic acid) combines with an aldehyde (a volatile fluid similar to alcohol). He soon abandoned the subject, however, only returning to it some years later. By 1905 he was a wealthy New Yorker, having recently made his fortune with the invention of a new photographic paper. While Baekeland had been busily amassing dollars, some advances had been made in the development of plastics. The years 1899 and 1900 had seen the patenting of the first semi-synthetic thermosetting material that could be manufactured on an industrial scale. In purely scientific terms, Baekeland&amp;rsquo;s major contribution to the field is not so much the actual discovery of the material to which he gave his name, but rather the method by which a reaction between phenol and formaldehyde could be controlled, thus making possible its preparation on a commercial basis. On 13 July 1907, Baekeland took out his famous patent describing this preparation, the essential features of which are still in use today.&lt;/p>
&lt;p>The original patent outlined a three-stage process, in which phenol and formaldehyde (from wood or coal) were initially combined under vacuum inside a large egg-shaped kettle. The result was a resin known as Novalak, which became soluble and malleable when heated. The resin was allowed to cool in shallow trays until it hardened, and then broken up and ground into powder. Other substances were then introduced: including fillers, such as woodflour, asbestos or cotton, which increase strength and moisture resistance, catalysts (substances to speed up the reaction between two chemicals without joining to either) and hexa, a compound of ammonia and formaldehyde which supplied the additional formaldehyde necessary to form a thermosetting resin. This resin was then left to cool and harden, and ground up a second time. The resulting granular powder was raw Bakelite, ready to be made into a vast range of manufactured objects. In the last stage, the heated Bakelite was poured into a hollow mould of the required shape and subjected to extreme heat and pressure, thereby &amp;lsquo;setting&amp;rsquo; its form for life.&lt;/p>
&lt;p>The design of Bakelite objects, everything from earrings to television sets, was governed to a large extent by the technical requirements of the moulding process. The object could not be designed so that it was locked into the mould and therefore difficult to extract. A common general rule was that objects should taper towards the deepest part of the mould, and if necessary the product was moulded in separate pieces. Moulds had to be carefully designed so that the molten Bakelite would flow evenly and completely into the mould. Sharp corners proved impractical and were thus avoided, giving rise to the smooth, &amp;lsquo;streamlined&amp;rsquo; style popular in the 1930s. The thickness of the walls of the mould was also crucial: thick walls took longer to cool and harden, a factor which had to be considered by the designer in order to make the most efficient use of machines.&lt;/p>
&lt;p>Baekeland&amp;rsquo;s invention, although treated with disdain in its early years, went on to enjoy an unparalleled popularity which lasted throughout the first half of the twentieth century. It became the wonder product of the new world of industrial expansion - &amp;rsquo;the material of a thousand uses&amp;rsquo;. Being both non-porous and heat-resistant, Bakelite kitchen goods were promoted as being germ-free and sterilisable. Electrical manufacturers seized on its insulating properties, and consumers everywhere relished its dazzling array of shades, delighted that they were now, at last, no longer restricted to the wood tones and drab browns of the pre-plastic era. It then fell from favour again during the 1950s, and was despised and destroyed in vast quantities. Recently, however, it has been experiencing something of a renaissance, with renewed demand for original Bakelite objects in the collectors&amp;rsquo; marketplace, and museums, societies and dedicated individuals once again appreciating the style and originality of this innovative material.&lt;/p></description></item><item><title>Docs: R5.2.2 What’s so funny?</title><link>https://note.codiy.net/ielts/r5.2.2</link><pubDate>Thu, 11 Aug 2022 19:17:12 +0800</pubDate><guid>https://note.codiy.net/ielts/r5.2.2</guid><description>
&lt;blockquote>
&lt;p>John McCrone reviews recent research on humour&lt;/p>
&lt;/blockquote>
&lt;p>The joke comes over the headphones: &amp;lsquo;Which side of a dog has the most hair? The left.&amp;rsquo; No, not funny. Try again. &amp;lsquo;Which side of a dog has the most hair? The outside.&amp;rsquo; Hah! The punchline is silly yet fitting, tempting a smile, even a laugh. Laughter has always struck people as deeply mysterious, perhaps pointless. The writer Arthur Koestler dubbed it the luxury reflex: &amp;lsquo;unique in that it serves no apparent biological purpose&amp;rsquo;.&lt;/p>
&lt;p>Theories about humour have an ancient pedigree. Plato expressed the idea that humour is simply a delighted feeling of superiority over others. Kant and Freud felt that joke-telling relies on building up a psychic tension which is safely punctured by the ludicrousness of the punchline. But most modern humour theorists have settled on some version of Aristotle&amp;rsquo;s belief that jokes are based on a reaction to or resolution of incongruity, when the punchline is either a nonsense or, though appearing silly, has a clever second meaning.&lt;/p>
&lt;p>Graeme Ritchie, a computational linguist in Edinburgh, studies the linguistic structure of jokes in order to understand not only humour but language understanding and reasoning in machines. He says that while there is no single format for jokes, many revolve around a sudden and surprising conceptual shift. A comedian will present a situation followed by an unexpected interpretation that is also apt.&lt;/p>
&lt;p>So even if a punchline sounds silly, the listener can see there is a clever semantic fit and that sudden mental &amp;lsquo;Aha!&amp;rsquo; is the buzz that makes us laugh. Viewed from this angle, humour is just a form of creative insight, a sudden leap to a new perspective.&lt;/p>
&lt;p>However, there is another type of laughter, the laughter of social appeasement and it is important to understand this too. Play is a crucial part of development in most young mammals. Rats produce ultrasonic squeaks to prevent their scuffles turning nasty. Chimpanzees have a &amp;lsquo;play-face&amp;rsquo; - a gaping expression accompanied by a panting &amp;lsquo;ah, ah&amp;rsquo; noise. In humans, these signals have mutated into smiles and laughs. Researchers believe social situations, rather than cognitive events such as jokes, trigger these instinctual markers of play or appeasement. People laugh on fairground rides or when tickled to flag a play situation, whether they feel amused or not.&lt;/p>
&lt;p>Both social and cognitive types of laughter tap into the same expressive machinery in our brains, the emotion and motor circuits that produce smiles and excited vocalisations. However, if cognitive laughter is the product of more general thought processes, it should result from more expansive brain activity.&lt;/p>
&lt;p>Psychologist Vinod Goel investigated humour using the new technique of &amp;lsquo;single event&amp;rsquo; functional magnetic resonance imaging (fMRI). An MRI scanner uses magnetic fields and radio waves to track the changes in oxygenated blood that accompany mental activity. Until recently, MRI scanners needed several minutes of activity and so could not be used to track rapid thought processes such as comprehending a joke. New developments now allow half-second &amp;lsquo;snapshots&amp;rsquo; of all sorts of reasoning and problem-solving activities.&lt;/p>
&lt;p>Although Goel felt being inside a brain scanner was hardly the ideal place for appreciating a joke, he found evidence that understanding a joke involves a widespread mental shift. His scans showed that at the beginning of a joke the listener&amp;rsquo;s prefrontal cortex lit up, particularly the right prefrontal believed to be critical for problem solving. But there was also activity in the temporal lobes at the side of the head (consistent with attempts to rouse stored knowledge) and in many other brain areas. Then when the punchline arrived, a new area sprang to life - the orbital prefrontal cortex. This patch of brain tucked behind the orbits of the eyes is associated with evaluating information.&lt;/p>
&lt;p>Making a rapid emotional assessment of the events of the moment is an extremely demanding job for the brain, animal or human. Energy and arousal levels may need to be retuned in the blink of an eye. These abrupt changes will produce either positive or negative feelings. The orbital cortex, the region that becomes active in Goel&amp;rsquo;s experiment, seems the best candidate for the site that feeds such feelings into higher-level thought processes, with its close connections to the brain&amp;rsquo;s sub-cortical arousal apparatus and centres of metabolic control.&lt;/p>
&lt;p>All warm-blooded animals make constant tiny adjustments in arousal in response to external events, but humans, who have developed a much more complicated internal life as a result of language, respond emotionally not only to their surroundings, but to their own thoughts. Whenever a sought-for answer snaps into place, there is a shudder of pleased recognition. Creative discovery being pleasurable, humans have learned to find ways of milking this natural response. The fact that jokes tap into our general evaluative machinery explains why the line between funny and disgusting, or funny and frightening, can be so fine. Whether a joke gives pleasure or pain depends on a person&amp;rsquo;s outlook.&lt;/p>
&lt;p>Humour may be a luxury, but the mechanism behind it is no evolutionary accident. As Peter Derks, a psychologist at William and Mary College in Virginia, says: &amp;lsquo;I like to think of humour as the distorted mirror of the mind. It&amp;rsquo;s creative, perceptual, analytical and lingual. If we can figure out how the mind processes humour, then we&amp;rsquo;ll have a pretty good handle on how it works in general.&amp;rsquo;&lt;/p></description></item><item><title>Docs: R5.2.3 The Birth of Scientific English</title><link>https://note.codiy.net/ielts/r5.2.3</link><pubDate>Thu, 11 Aug 2022 19:17:12 +0800</pubDate><guid>https://note.codiy.net/ielts/r5.2.3</guid><description>
&lt;p>World science is dominated today by a small number of languages, including Japanese, German and French, but it is English which is probably the most popular global language of science. This is not just because of the importance of English-speaking countries such as the USA in scientific research; the scientists of many non-English-speaking countries find that they need to write their research papers in English to reach a wide international audience. Given the prominence of scientific English today, it may seem surprising that no one really knew how to write science in English before the 17th century. Before that, Latin was regarded as the lingua franca1 for European intellectuals.&lt;/p>
&lt;p>The European Renaissance (c. 14th-16th century) is sometimes called the &amp;lsquo;revival of learning&amp;rsquo;, a time of renewed interest in the &amp;rsquo;lost knowledge&amp;rsquo; of classical times. At the same time, however, scholars also began to test and extend this knowledge. The emergent nation states of Europe developed competitive interests in world exploration and the development of trade. Such expansion, which was to take the English language west to America and east to India, was supported by scientific developments such as the discovery of magnetism (and hence the invention of the compass), improvements in cartography and - perhaps the most important scientific revolution of them all - the new theories of astronomy and the movement of the Earth in relation to the planets and stars, developed by Copernicus (1473-1543).&lt;/p>
&lt;p>England was one of the first countries where scientists adopted and publicised Copernican ideas with enthusiasm. Some of these scholars, including two with interests in language -John Wallis and John Wilkins - helped found the Royal Society in 1660 in order to promote empirical scientific research.&lt;/p>
&lt;p>Across Europe similar academies and societies arose, creating new national traditions of science. In the initial stages of the scientific revolution, most publications in the national languages were popular works, encyclopaedias, educational textbooks and translations. Original science was not done in English until the second half of the 17th century. For example, Newton published his mathematical treatise, known as the Principia, in Latin, but published his later work on the properties of light - Opticks - in English.&lt;/p>
&lt;p>There were several reasons why original science continued to be written in Latin. The first was simply a matter of audience. Latin was suitable for an international audience of scholars, whereas English reached a socially wider, but more local, audience. Hence, popular science was written in English.&lt;/p>
&lt;p>A second reason for writing in Latin may, perversely, have been a concern for secrecy. Open publication had dangers in putting into the public domain preliminary ideas which had not yet been fully exploited by their &amp;lsquo;author&amp;rsquo;. This growing concern about intellectual property rights was a feature of the period - it reflected both the humanist notion of the individual, rational scientist who invents and discovers through private intellectual labour, and the growing connection between original science and commercial exploitation. There was something of a social distinction between &amp;lsquo;scholars and gentlemen&amp;rsquo; who understood Latin, and men of trade who lacked a classical education. And in the mid-17th century it was common practice for mathematicians to keep their discoveries and proofs secret, by writing them in cipher, in obscure languages, or in private messages deposited in a sealed box with the Royal Society. Some scientists might have felt more comfortable with Latin precisely because its audience, though international, was socially restricted. Doctors clung the most keenly to Latin as an &amp;lsquo;insider language&amp;rsquo;.&lt;/p>
&lt;p>A third reason why the writing of original science in English was delayed may have been to do with the linguistic inadequacy of English in the early modern period. English was not well equipped to deal with scientific argument. First, it lacked the necessary technical vocabulary. Second, it lacked the grammatical resources required to represent the world in an objective and impersonal way, and to discuss the relations, such as cause and effect, that might hold between complex and hypothetical entities.&lt;/p>
&lt;p>Fortunately, several members of the Royal Society possessed an interest in language and became engaged in various linguistic projects. Although a proposal in 1664 to establish a committee for improving the English language came to little, the society&amp;rsquo;s members did a great deal to foster the publication of science in English and to encourage the development of a suitable writing style. Many members of the Royal Society also published monographs in English. One of the first was by Robert Hooke, the society&amp;rsquo;s first curator of experiments, who described his experiments with microscopes in Micrographia (1665). This work is largely narrative in style, based on a transcript of oral demonstrations and lectures.&lt;/p>
&lt;p>In 1665 a new scientific journal, Philosophical Transactions, was inaugurated. Perhaps the first international English-language scientific journal, it encouraged a new genre of scientific writing, that of short, focused accounts of particular experiments.&lt;/p>
&lt;p>The 17th century was thus a formative period in the establishment of scientific English. In the following century much of this momentum was lost as German established itself as the leading European language of science. It is estimated that by the end of the 18th century 401 German scientific journals had been established as opposed to 96 in France and 50 in England. However, in the 19th century scientific English again enjoyed substantial lexical growth as the industrial revolution created the need for new technical vocabulary, and new, specialised, professional societies were instituted to promote and publish in the new disciplines.&lt;/p>
&lt;ol>
&lt;li>lingua franca: a language which is used for communication between groups of people who speak different languages&lt;/li>
&lt;/ol></description></item><item><title>Docs: R5.1.1 Johnson's Dictionary</title><link>https://note.codiy.net/ielts/r5.1.1</link><pubDate>Thu, 11 Aug 2022 13:17:12 +0800</pubDate><guid>https://note.codiy.net/ielts/r5.1.1</guid><description>
&lt;p>For the century before Johnson&amp;rsquo;s Dictionary was published in 1775, there had been concern about the state of the English language. There was no standard way of speaking or writing and no agreement as to the best way of bringing some order to the chaos of English spelling. Dr Johnson provided the solution.&lt;/p>
&lt;p>There had, of course, been dictionaries in the past, the first of these being a little book of some 120 pages, compiled by a certain Robert Cawdray, published in 1604 under the title A Table Alphabeticall &amp;lsquo;of hard usuall English wordes&amp;rsquo;. Like the various dictionaries that came after it during the seventeenth century, Cawdray&amp;rsquo;s tended to concentrate on &amp;lsquo;scholarly&amp;rsquo; words; one function of the dictionary was to enable its student to convey an impression of fine learning.&lt;/p>
&lt;p>Beyond the practical need to make order out of chaos, the rise of dictionaries is associated with the rise of the English middle class, who were anxious to define and circumscribe the various worlds to conquer- lexical as well as social and commercial. It is highly appropriate that Dr Samuel Johnson, the very model of an eighteenth-century literary man, as famous in his own time as in ours, should have published his Dictionary at the very beginning of the heyday of the middle class.&lt;/p>
&lt;p>Johnson was a poet and critic who raised common sense to the heights of genius. His approach to the problems that had worried writers throughout the late seventeenth and early eighteenth centuries was intensely practical. Up until his time, the task of producing a dictionary on such a large scale had seemed impossible without the establishment of an academy to make decisions about right and wrong usage. Johnson decided he did not need an academy to settle arguments about language; he would write a dictionary himself; and he would do it single-handed. Johnson signed the contract for the Dictionary with the bookseller Robert Dosley at a breakfast held at the Golden Anchor Inn near Holborn Bar on 18 June 1764. He was to be paid ￡1,575 in instalments, and from this he took money to rent 17 Gough Square, in which he set up his &amp;lsquo;dictionary workshop&amp;rsquo;.&lt;/p>
&lt;p>James Boswell, his biographer, described the garret where Johnson worked as &amp;lsquo;itted up like a counting house&amp;rsquo; with a long desk running down the middle at which the copying clerks would work standing up. Johnson himself was stationed on a rickety chair at an &amp;lsquo;old crazy deal table&amp;rsquo; surrounded by a chaos of borrowed books. He was also helped by six assistants, two of whom died whilst the Dictionary was still in preparation.&lt;/p>
&lt;p>The work was immense; filling about eighty large notebooks (and without a library to hand), Johnson wrote the definitions of over 40,000 words, and illustrated their many meanings with some 114,000 quotations drawn from English writing on every subject, from the Elizabethans to his own time. He did not expect to achieve complete originality. Working to a deadline, he had to draw on the best of all previous dictionaries, and to make his work one of heroic synthesis. In fact, it was very much more. Unlike his predecessors, Johnson treated English very practically, as a living language, with many different shades of meaning. He adopted his definitions on the principle of English common law-according to precedent. After its publication, his Dictionary was not seriously rivalled for over a century.&lt;/p>
&lt;p>After many vicissitudes the Dictionary was finally published on 15 April 1775. It was instantly recognised as a landmark throughout Europe. &amp;lsquo;This very noble work,&amp;rsquo; wrote the leading Italian lexicographer, &amp;lsquo;will be a perpetual monument of Fame to the Author, an Honour to his own Country in particular, and a general Benefit to the republic of Letters throughout Europe.&amp;rsquo; The fact that Johnson had taken on the Academies of Europe and matched them (everyone knew that forty French academics had taken forty years to produce the first French national dictionary) was cause for much English celebration.&lt;/p>
&lt;p>Johnson had worked for nine years, &amp;lsquo;with little assistance of the learned, and without any patronage of the great; not in the soft obscurities of retirement, or under the shelter of academic bowers, but amidst inconvenience and distraction, in sickness and in sorrow&amp;rsquo;. For all its faults and eccentricities his two-volume work is a masterpiece and a landmark in his own words, &amp;lsquo;setting the orthography, displaying the analogy, regulating the structures, and ascertaining the significations of English words&amp;rsquo;. It is the cornerstone of Standard English, an achievement which, in James Boswell&amp;rsquo;s words, &amp;lsquo;conferred stability on the language of his country&amp;rsquo;.&lt;/p>
&lt;p>The Dictionary, together with his other writing, made Johnson famous and so well esteemed that his friends were able to prevail upon King George Ⅲ to offer him a pension. From then on, he was to become the Johnson of folklore.&lt;/p></description></item><item><title>Docs: R5.1.2 Nature or Nurture?</title><link>https://note.codiy.net/ielts/r5.1.2</link><pubDate>Thu, 11 Aug 2022 13:17:12 +0800</pubDate><guid>https://note.codiy.net/ielts/r5.1.2</guid><description>
&lt;h4 id="a">A&lt;/h4>
&lt;p>A few years ago, in one of the most fascinating and disturbing experiments in behavioural psychology, Stanley Milgram of Yale University tested 40 subjects from all walks of life for their willingness to obey instructions given by a &amp;rsquo;leader&amp;rsquo; in a situation in which the subjects might feel a personal distaste for the actions they were called upon to perform. Specifically, Milgram told each volunteer &amp;rsquo;teacher-subject&amp;rsquo; that the experiment was in the noble cause of education, and was designed to test whether or not punishing pupils for their mistakes would have a positive effect on the pupils&amp;rsquo; ability to learn.&lt;/p>
&lt;h4 id="b">B&lt;/h4>
&lt;p>Milgram&amp;rsquo;s experimental set-up involved placing the teacher-subject before a panel of thirty switches with labels ranging from &amp;lsquo;15 volts of electricity (slight shock)&amp;rsquo; to &amp;lsquo;450 volts (danger - severe shock)&amp;rsquo; in steps of 15 volts each. The teacher-subject was told that whenever the pupil gave the wrong answer to a question, a shock was to be administered, beginning at the lowest level and increasing in severity with each successive wrong answer. The supposed &amp;lsquo;pupil&amp;rsquo; was in reality an actor hired by Milgram to simulate receiving the shocks by emitting a spectrum of groans, screams and writhings together with an assortment of statements and expletives denouncing both the experiment and the experimenter. Milgram told the teacher-subject to ignore the reactions of the pupil, and to administer whatever level of shock was called for, as per the rule governing the experimental situation of the moment.&lt;/p>
&lt;h4 id="c">C&lt;/h4>
&lt;p>As the experiment unfolded, the pupil would deliberately give the wrong answers to questions posed by the teacher, thereby bringing on various electrical punishments, even up to the danger level of 300 volts and beyond. Many of the teacher-subjects balked at administering the higher levels of punishment, and turned to Milgram with questioning looks and/or complaints about continuing the experiment. In these situations, Milgram calmly explained that the teacher-subject was to ignore the pupil&amp;rsquo;s cries for mercy and carry on with the experiment. If the subject was still reluctant to proceed, Milgram said that it was important for the sake of the experiment that the procedure be followed through to the end. His final argument was, &amp;lsquo;You have no other choice. You must go on.&amp;rsquo; What Milgram was trying to discover was the number of teacher-subjects who would be willing to administer the highest levels of shock, even in the face of strong personal and moral revulsion against the rules and conditions of the experiment.&lt;/p>
&lt;h4 id="d">D&lt;/h4>
&lt;p>Prior to carrying out the experiment, Milgram explained his idea to a group of 39 psychiatrists and asked them to predict the average percentage of people in an ordinary population who would be willing to administer the highest shock level of 450 volts. The overwhelming consensus was that virtually all the teacher-subjects would refuse to obey the experimenter. The psychiatrists felt that &amp;lsquo;most subjects would not go beyond 150 volts&amp;rsquo; and they further anticipated that only four per cent would go up to 300 volts. Furthermore, they thought that only a lunatic fringe of about one in 1,000 would give the highest shock of 450 volts.&lt;/p>
&lt;h4 id="e">E&lt;/h4>
&lt;p>What were the actual results? Well, over 60 per cent of the teacher-subjects continued to obey Milgram up to the 450-volt limit! In repetitions of the experiment in other countries, the percentage of obedient teacher-subjects was even higher, reaching 85 per cent in one country. How can we possibly account for this vast discrepancy between what calm, rational, knowledgeable people predict in the comfort of their study and what pressured, flustered, but cooperative &amp;rsquo;teachers&amp;rsquo; actually do in the laboratory of real life?&lt;/p>
&lt;h4 id="f">F&lt;/h4>
&lt;p>One&amp;rsquo;s first inclination might be to argue that there must be some sort of built-in animal aggression instinct that was activated by the experiment, and that Milgram&amp;rsquo;s teacher-subjects were just following a genetic need to discharge this pent-up primal urge onto the pupil by administering the electrical shock. A modern hard-core sociobiologist might even go so far as to claim that this aggressive instinct evolved as an advantageous trait, having been of survival value to our ancestors in their struggle against the hardships of life on the plains and in the caves, ultimately finding its way into our genetic make-up as a remnant of our ancient animal ways.&lt;/p>
&lt;h4 id="g">G&lt;/h4>
&lt;p>An alternative to this notion of genetic programming is to see the teacher-subjects&amp;rsquo; actions as a result of the social environment under which the experiment was carried out. As Milgram himself pointed out, &amp;lsquo;Most subjects in the experiment see their behaviour in a larger context that is benevolent and useful to society - the pursuit of scientific truth. The psychological laboratory has a strong claim to legitimacy and evokes trust and confidence in those who perform there. An action such as shocking a victim, which in isolation appears evil, acquires a completely different meaning when placed in this setting.&amp;rsquo;&lt;/p>
&lt;h4 id="h">H&lt;/h4>
&lt;p>Thus, in this explanation the subject merges his unique personality and personal and moral code with that of larger institutional structures, surrendering individual properties like loyalty, self-sacrifice and discipline to the service of malevolent systems of authority.&lt;/p>
&lt;h4 id="i">I&lt;/h4>
&lt;p>Here we have two radically different explanations for why so many teacher-subjects were willing to forgo their sense of personal responsibility for the sake of an institutional authority figure. The problem for biologists, psychologists and anthropologists is to sort out which of these two polar explanations is more plausible. This, in essence, is the problem of modern sociobiology - to discover the degree to which hard-wired genetic programming dictates, or at least strongly biases, the interaction of animals and humans with their environment, that is, their behaviour. Put another way, sociobiology is concerned with elucidating the biological basis of all behaviour.&lt;/p></description></item><item><title>Docs: R5.1.3 The Truth about the Environment</title><link>https://note.codiy.net/ielts/r5.1.3</link><pubDate>Thu, 11 Aug 2022 13:17:12 +0800</pubDate><guid>https://note.codiy.net/ielts/r5.1.3</guid><description>
&lt;p>For many environmentalists, the world seems to be getting worse. They have developed a hit-list of our main fears: that natural resources are running out; that the population is ever growing, leaving less and less to eat; that species are becoming extinct in vast numbers, and that the planet&amp;rsquo;s air and water are becoming ever more polluted.&lt;/p>
&lt;p>But a quick look at the facts shows a different picture. First, energy and other natural resources have become more abundant, not less so, since the book &amp;lsquo;The Limits to Growth&amp;rsquo; was published in 1972 by a group of scientists. Second, more food is now produced per head of the world&amp;rsquo;s population than at any time in history. Fewer people are starving. Third, although species are indeed becoming extinct, only about 0.7% of them are expected to disappear in the next 50 years, not 25-50%, as has so often been predicted. And finally, most forms of environmental pollution either appear to have been exaggerated, or are transient - associated with the early phases of industrialisation and therefore best cured not by restricting economic growth, but by accelerating it. One form of pollution - the release of greenhouse gases that causes global warming - does appear to be a phenomenon that is going to extend well into our future, but its total impact is unlikely to pose a devastating problem. A bigger problem may well turn out to be an inappropriate response to it.&lt;/p>
&lt;p>Yet opinion polls suggest that many people nurture the belief that environmental standards are declining and four factors seem to cause this disjunction between perception and reality.&lt;/p>
&lt;p>One is the lopsidedness built into scientific research. Scientific funding goes mainly to areas with many problems. That may be wise policy, but it will also create an impression that many more potential problems exist than is the case.&lt;/p>
&lt;p>Secondly, environmental groups need to be noticed by the mass media. They also need to keep the money rolling in. Understandably, perhaps, they sometimes overstate their arguments. In 1997, for example, the World Wide Fund for Nature issued a press release entitled: &amp;lsquo;Two thirds of the world&amp;rsquo;s forests lost forever&amp;rsquo;. The truth turns out to be nearer 20%.&lt;/p>
&lt;p>Though these groups are run overwhelmingly by selfless folk, they nevertheless share many of the characteristics of other lobby groups. That would matter less if people applied the same degree of scepticism to environmental lobbying as they do to lobby groups in other fields. A trade organisation arguing for, say, weaker pollution controls is instantly seen as self-interested. Yet a green organisation opposing such a weakening is seen as altruistic, even if an impartial view of the controls in question might suggest they are doing more harm than good.&lt;/p>
&lt;p>A third source of confusion is the attitude of the media. People are clearly more curious about bad news than good. Newspapers and broadcasters are there to provide what the public wants. That, however, can lead to significant distortions of perception. An example was America&amp;rsquo;s encounter with El Niño in 1997 and 1998. This climatic phenomenon was accused of wrecking tourism, causing allergies, melting the ski-slopes and causing 22 deaths. However, according to an article in the Bulletin of the American Meteorological Society, the damage it did was estimated at US $4 billion but the benefits amounted to some US $19 billion. These came from higher winter temperatures (which saved an estimated 850 lives, reduced heating costs and diminished spring floods caused by meltwaters).&lt;/p>
&lt;p>The fourth factor is poor individual perception. People worry that the endless rise in the amount of stuff everyone throws away will cause the world to run out of places to dispose of waste. Yet, even if America&amp;rsquo;s trash output continues to rise as it has done in the past, and even if the American population doubles by 2100, all the rubbish America produces through the entire 21st century will still take up only one-12,000th of the area of the entire United States.&lt;/p>
&lt;p>So what of global warming? As we know, carbon dioxide emissions are causing the planet to warm. The best estimates are that the temperatures will rise by 2-3℃ in this century, causing considerable problems, at a total cost of US $5,000 billion.&lt;/p>
&lt;p>Despite the intuition that something drastic needs to be done about such a costly problem, economic analyses clearly show it will be far more expensive to cut carbon dioxide emissions radically than to pay the costs of adaptation to the increased temperatures. A model by one of the main authors of the United Nations Climate Change Panel shows how an expected temperature increase of 2.1 degrees in 2100 would only be diminished to an increase of 1.9 degrees. Or to put it another way, the temperature increase that the planet would have experienced in 2094 would be postponed to 2100.&lt;/p>
&lt;p>So this does not prevent global warming, but merely buys the world six years. Yet the cost of reducing carbon dioxide emissions, for the United States alone, will be higher than the cost of solving the world&amp;rsquo;s single, most pressing health problem: providing universal access to clean drinking water and sanitation. Such measures would avoid 2 million deaths every year, and prevent half a billion people from becoming seriously ill.&lt;/p>
&lt;p>It is crucial that we look at the facts if we want to make the best possible decisions for the future. It may be costly to be overly optimistic - but more costly still to be too pessimistic.&lt;/p></description></item><item><title>Docs: R.OG1.1 The Dover Bronze-Age Boat</title><link>https://note.codiy.net/ielts/r.og1.1</link><pubDate>Tue, 09 Aug 2022 13:11:52 +0800</pubDate><guid>https://note.codiy.net/ielts/r.og1.1</guid><description>
&lt;blockquote>
&lt;p>A beautifully preserved boat, made around 3,000 years ago and discovered by chance in a muddy hole, has had a profound impact on archaeological research.&lt;/p>
&lt;/blockquote>
&lt;p>It was 1992. In England, workmen were building a new road through the heart of Dover, to connect the ancient port and the Channel Tunnel, which, when it opened just two years later, was to be the first land link between Britain and Europe for over 10,000 years. A small team from the Canterbury Archaeological Trust (CAT) worked alongside the workmen, recording new discoveries brought to light by the machines.&lt;/p>
&lt;p>At the base of a deep shaft six metres below the modern streets a wooden structure was revealed. Cleaning away the waterlogged site overlying the timbers, archaeologists realised its true nature. They had found a prehistoric boat, preserved by the type of sediment in which it was buried. It was then named the Dover Bronze-Age Boat.&lt;/p>
&lt;p>About nine metres of the boat’s length was recovered; one end lay beyond the excavation and had to be left. What survived consisted essentially of four intricately carved oak planks: two on the bottom, joined along a central seam by a complicated system of wedges and timbers, and two at the side, curved and stitched to the others. The seams had been made watertight by pads of moss, fixed by wedges and yew stitches.&lt;/p>
&lt;p>The timbers that closed the recovered end of the boat had been removed in antiquity when it was abandoned, but much about its original shape could be deduced. There was also evidence for missing upper side planks. The boat was not a wreck, but had been deliberately discarded, dismantled and broken. Perhaps it had been &amp;lsquo;ritually killed, at the end of its life, like other Bronze-Age objects.&lt;/p>
&lt;p>With hindsight, it was significant that the boat was found and studied by mainstream archaeologists who naturally focused on its cultural context. At the time, ancient boats were often considered only from a narrower technological perspective, but news about the Dover boat reached a broad audience. In 2002, on the tenth anniversary of the discovery, the Dover Bronze-Age Boat Trust hosted a conference, where this meeting of different traditions became apparent. Alongside technical papers about the boat, other speakers explored its social and economic contexts, and the religious perceptions of boats in Bronze-Age societies. Many speakers came from overseas, and debate about cultural connections was renewed.&lt;/p>
&lt;p>Within seven years of excavation, the Dover boat had been conserved and displayed, but it was apparent that there were issues that could not be resolved simply by studying the old wood. Experimental archaeology seemed to be the solution: a boat reconstruction, half-scale or full-sized, would permit assessment of the different hypotheses regarding its build and the missing end. The possibility of returning to Dover to search for the boat&amp;rsquo;s unexcavated northern end was explored, but practical and financial difficulties were insurmountable 一 and there was no guarantee that the timbers had survived the previous decade in the changed environment.&lt;/p>
&lt;p>Detailed proposals to reconstruct the boat were drawn up in 2004. Archaeological evidence was beginning to suggest a Bronze-Age community straddling the Channel, brought together by the sea, rather than separated by it. In a region today divided by languages and borders, archaeologists had a duty to inform the general public about their common cultural heritage.&lt;/p>
&lt;p>The boat project began in England but it was conceived from the start as a European collaboration. Reconstruction was only part of a scheme that would include a major exhibition and an extensive educational and outreach programme. Discussions began early in 2005 with archaeological bodies, universities and heritage organisations either side of the Channel. There was much enthusiasm and support, and an official launch of the project was held at an international seminar in France in 2007. Financial support was confirmed in 2008 and the project then named BOAT 1550BC got under way in June 2011.&lt;/p>
&lt;p>A small team began to make the boat at the start of 2012 on the Roman Lawn outside Dover museum. A full-scale reconstruction of a mid-section had been made in 1996, primarily to see how Bronze- Age replica tools performed. In 2012, however, the hull shape was at the centre of the work, so modem power tools were used to carve the oak planks, before turning to prehistoric tools for finishing. It was decided to make the replica half-scale for reasons of cost and time, and synthetic materials were used for the stitching, owing to doubts about the scaling and tight timetable.&lt;/p>
&lt;p>Meanwhile, the exhibition was being prepared ready for opening in July 2012 at the Castle Museum in Boulogne-sur-Mer. Entitled &amp;lsquo;Beyond the Horizon: Societies of the Channel &amp;amp; North Sea 3,500 years ago&amp;rsquo;, it brought together for the first time a remarkable collection of Bronze-Age objects, including many new discoveries for commercial archaeology and some of the great treasure of the past. The reconstructed boat, as a symbol of the maritime connections that bound together the communities either side of the Channel, was the centrepiece.&lt;/p></description></item><item><title>Docs: R.OG1.2 The changing role of airports</title><link>https://note.codiy.net/ielts/r.og1.2</link><pubDate>Tue, 09 Aug 2022 13:11:52 +0800</pubDate><guid>https://note.codiy.net/ielts/r.og1.2</guid><description>
&lt;blockquote>
&lt;p>Airports continue to diversify their role in an effort to generate income. Are business meeting facilities the next step? Nigel Halpern, Anne Graham and Rob Davidson investigate.&lt;/p>
&lt;/blockquote>
&lt;h4 id="a">A&lt;/h4>
&lt;p>In recent times developing commercial revenues has become more challenging for airports due to a combination of factors, such as increased competition from Internet shopping, restrictions on certain sales, such as tobacco, and new security procedures that have had an impact on the dwell time of passengers. Moreover, the global economic downturn has caused a reduction in passenger numbers while those that are travelling generally have less money to spend. This has meant that the share of revenue from non-aeronautical revenues actually peaked at 54% at the turn of the century and has subsequently declined slightly. Meanwhile, the pressures to control the level of aeronautical revenues are as strong as ever due to the poor financial health of many airlines and the rapid rise of the low-cost carrier sector.&lt;/p>
&lt;h4 id="b">B&lt;/h4>
&lt;p>Some of the more obvious solutions to growing commercial revenues, such as extending the merchandising space or expanding the variety of shopping opportunities, have already been tried to their limit at many airports. A more radical solution is to find new sources of commercial revenue within the terminal, and this has been explored by many airports over the last decade or so. As a result, many terminals are now much more than just shopping malls and offer an array of entertainment, leisure, and beauty and wellness facilities. At this stage of facilities provision, the airport also has the possibility of taking on the role of the final destination rather than merely a facilitator of access.&lt;/p>
&lt;h4 id="c">C&lt;/h4>
&lt;p>At the same time, airports have been developing and expanding the range of services that they provide specifically for the business traveller in the terminal. This includes offering business centres that supply support services, meeting or conference rooms and other space for special events. Within this context, Jarach (2001 ] discusses how dedicated meetings facilities located within the terminal and managed directly by the airport operator may be regarded as an expansion of the concept of airline lounges or as a way to reconvert abandoned or underused areas of terminal buildings. Previously it was primarily airport hotels and other facilities offered in the surrounding area of the airport that had the potential to take on this role and become active as a business space (McNeill, 2009).&lt;/p>
&lt;h4 id="d">D&lt;/h4>
&lt;p>When an airport location can be promoted as a business venue, this may increase the overall appeal of the airport and help it become more competitive in both attracting and retaining airlines and their passengers. In particular, the presence of meeting facilities could become one of the determining factors taken into consideration when business people are choosing airlines and where they change their planes. This enhanced attractiveness itself may help to improve the airport operator^ financial position and future prospects, but clearly this will be dependent on the competitive advantage that the airport is able to achieve in comparison with other venues.&lt;/p>
&lt;h4 id="e">E&lt;/h4>
&lt;p>In 2011, an online airport survey was conducted and some of the areas investigated included the provision and use of meeting facilities at airports and the perceived role and importance of these facilities in generating income and raising passenger numbers. In total, there were responses from staff at 154 airports and 68% of these answered yes* to the question: Does your airport own and have meetings facilities available for hire? The existence of meeting facilities therefore seems high at airports. In addition, 28% of respondents that did not have meeting facilities stated that they were likely to invest in them during the next five years. The survey also asked to what extent respondents agreed or disagreed with a number of statements about the meeting facilities at their airport. 49% of respondents agreed that they have put more investment into them during recent years; 41% agreed that they would invest more in the immediate future. These are fairly high proportions considering the recent economic climate.&lt;/p>
&lt;h4 id="f">F&lt;/h4>
&lt;p>The survey also asked airports with meeting facilities to estimate what proportion of users are from the local area, i.e. within a 90-minute drive from the airport, or from abroad. Their findings show that meeting facilities provided by the majority of respondents tend to serve local versus non-local or foreign needs. 63% of respondents estimated that over 60% of users are from the local area. Only 3% estimated that over 80% of users are from abroad. It is therefore not surprising that the facilities are of limited importance when it comes to increasing use of flights at the airport: 16% of respondents estimated that none of the users of their meeting facilities use flights when travelling to or from them, while 56% estimated that 20% or fewer of the users of their facilities use flights.&lt;/p>
&lt;h4 id="g">G&lt;/h4>
&lt;p>The survey asked respondents with meeting facilities to estimate how much revenue their airport earned from its meeting facilities during the last financial year. Average revenue per airport was just $12,959. Meeting facilities are effectively a non-aeronautical source of airport revenue. Only 1% of respondents generated more than 20% non-aeronautical revenue from their meetings facilities; none generated more than 40%. Given the focus on local demand, it is not surprising that less than a third of respondents agreed that their meeting facilities support business and tourism development in their home region or country.&lt;/p>
&lt;h4 id="h">H&lt;/h4>
&lt;p>The findings of this study suggest that few airports provide meetings facilities as a serious commercial venture. It may be that, as owners of large property, space is available for meeting facilities at airports and could play an important role in serving the needs of the airport, its partners, and stakeholders such as government and the local community. Thus, while the local orientation means that competition with other airports is likely to be minimal, competition with local providers of meetings facilities is likely to be much greater.&lt;/p></description></item><item><title>Docs: R.OG1.3 IS PHOTOGRAPHY ART?</title><link>https://note.codiy.net/ielts/r.og1.3</link><pubDate>Tue, 09 Aug 2022 13:11:52 +0800</pubDate><guid>https://note.codiy.net/ielts/r.og1.3</guid><description>
&lt;p>This may seem a pointless question today. Surrounded as we are by thousands of photographs, most of us take for granted that, in addition to supplying information and seducing customers, camera images also serve as decoration, afford spiritual enrichment, and provide significant insights into the passing scene. But in the decades following the discovery of photography, this question reflected the search for ways to fit the mechanical medium into the traditional schemes of artistic expression.&lt;/p>
&lt;p>The much-publicized pronouncement by painter Paul Delaroche that the daguerreotype* signalled the end of painting is perplexing because this clever artist also forecast the usefulness of the medium for graphic artists in a letter written in 1839. Nevertheless, it is symptomatic of the swing between the outright rejection and qualified acceptance of the medium that was fairly typical of the artistic establishment. Discussion of the role of photography in art was especially spirited in France, where the internal policies of the time had created a large pool of artists, but it was also taken up by important voices in England. In both countries, public interest in this topic was a reflection of the belief that national stature and achievement in the arts were related.&lt;/p>
&lt;p>From the maze of conflicting statements and heated articles on the subject, three main positions about the potential of camera art emerged. The simplest, entertained by many painters and a section of the public, was that photographs should not be considered &amp;lsquo;art&amp;rsquo; because they were made with a mechanical device and by physical and chemical phenomena instead of by human hand and spirit； to some, camera images seemed to have more in common with fabric produced by machinery in a mill than with handmade creations fired by inspiration. The second widely held view, shared by painters, some photographers, and some critics, was that photographs would be useful to art but should not be considered equal in creativeness to drawing and painting. Lastly, by assuming that the process was comparable to other techniques such as etching and lithography, a fair number of individuals realized that camera images were or could be as significant as handmade works of art and that they might have a positive influence on the arts and on culture in general.&lt;/p>
&lt;p>Artists reacted to photography in various ways. Many portrait painters 一 miniaturists in particular - who realized that photography represented the ‘handwriting on the wall’ became involved with daguerreotyping or 60 paper photography in an effort to save their careers； some incorporated it with painting, while others renounced painting altogether. Still other painters, the most prominent among them the French painter, Jean- Auguste-Dominique Ingres, began almost immediately to use photography to make a record of their own output and also to provide themselves with source material for poses and backgrounds, vigorously denying at the same time its influence on their vision or its claims as art.&lt;/p>
&lt;p>The view that photographs might be worthwhile to artists was enunciated in considerable detail by Lacan and Francis Wey. The latter, an art and literary critic, who eventually recognised that camera images could be inspired as well as informative, suggested that they would lead to greater naturalness in the graphic depiction of anatomy, clothing, likeness, expression, and landscape. By studying photographs, true artists, he claimed, would be relieved of menial tasks and become free to devote themselves to the more important spiritual aspects of their work.&lt;/p>
&lt;p>Wey left unstated what the incompetent artist might do as an alternative, but according to the influential French critic and poet Charles Baudelaire, writing in response to an exhibition of photography in 1859, lazy and untalented painters would become photographers. Fired by a belief in art as an imaginative embodiment of cultivated ideas and dreams, Baudelaire regarded photography as &amp;lsquo;a very humble servant of art and science,； a medium largely unable to transcend &amp;rsquo;external reality&amp;rsquo;. For this critic, photography was linked with &amp;rsquo;the great industrial madness&amp;rsquo; of the time, which in his eyes exercised disastrous consequences on the spiritual qualities of life and art.&lt;/p>
&lt;p>Eugene Delacroix was the most prominent of the French artists who welcomed photography as help-mate but recognized its limitations. Regretting that &amp;lsquo;such a wonderful invention&amp;rsquo; had arrived so late in his lifetime, he still took lessons in daguerreotyping, and both commissioned and collected photographs. Delacroix&amp;rsquo;s enthusiasm for the medium can be sensed in a journal entry noting that if photographs were used as they should be, an artist might &amp;lsquo;raise himself to heights that we do not yet know&amp;rsquo;.&lt;/p>
&lt;p>The question of whether the photograph was document or art aroused interest in England also. The most important statement on this matter was an unsigned article that concluded that while photography had a role to play, it should not be &amp;lsquo;constrained&amp;rsquo; into &amp;lsquo;competition&amp;rsquo; with art； a more stringent viewpoint led critic Philip Gilbert Hamerton to dismiss camera images as &amp;rsquo;narrow in range, emphatic in assertion, telling one truth for ten falsehoods5.&lt;/p>
&lt;p>These writers reflected the opposition of a section of the cultural elite in England and France to the &amp;lsquo;cheapening of art&amp;rsquo; which the growing acceptance and purchase of camera pictures by the middle class represented. Technology made photographic images a common sight in the shop windows of Regent Street and Piccadilly in London and the commercial boulevards of Paris. In London, for example, there were at the time some commercial establishments where portraits, landscapes, and photographic reproductions of works of art could be bought. This appeal to the middle class convinced the elite that photographs would foster a desire for realism instead of idealism, even though some critics recognized that the work of individual photographers might display an uplifting style and substance that was consistent with the defining characteristics of art.&lt;/p>
&lt;ul>
&lt;li>the name given to the first commercially successful photographic images&lt;/li>
&lt;/ul></description></item><item><title>Docs: R.OG2.1 The Flavor of Pleasure</title><link>https://note.codiy.net/ielts/r.og2.1</link><pubDate>Tue, 09 Aug 2022 13:11:52 +0800</pubDate><guid>https://note.codiy.net/ielts/r.og2.1</guid><description>
&lt;blockquote>
&lt;p>When it comes to celebrating the flavor of food, our mouth gets all the credit But in truth, it is the nose that knows.&lt;/p>
&lt;/blockquote>
&lt;p>No matter how much we talk about tasting our favorite flavors, relishing them really depends on a combined input from our senses that we experience through mouth, tongue and nose. The taste, texture, and feel of food are what we tend to focus on, but most important are the slight puffs of air as we chew our food - what scientists call &amp;lsquo;retronasal smell&amp;rsquo;.&lt;/p>
&lt;p>Certainly, our mouths and tongues have taste buds, which are receptors for the five basic flavors: sweet, salty, sour, bitter, and umami, or what is more commonly referred to as savory. But our tongues are inaccurate instruments as far as flavor is concerned. They evolved to recognise only a few basic tastes in order to quickly identify toxins, which in nature are often quite bitter or acidly sour.&lt;/p>
&lt;p>All the complexity, nuance, and pleasure of flavor come from the sense of smell operating in the back of the nose. It is there that a kind of alchemy occurs when we breathe up and out the passing whiffs of our chewed food. Unlike a hound&amp;rsquo;s skull with its extra long nose, which evolved specifically to detect external smells, our noses have evolved to detect internal scents. Primates specialise in savoring the many millions of flavor combinations that they can create for their mouths.&lt;/p>
&lt;p>Taste without retronasal smell is not much help in recognising flavor. Smell has been the most poorly understood of our senses, and only recently has neuroscience, led by Yale University&amp;rsquo;s Gordon Shepherd, begun to shed light on its workings. Shepherd has come up with the term &amp;rsquo;neurogastronomy&amp;rsquo; to link the disciplines of food science, neurology, psychology, and anthropology with the savory elements of eating, one of the most enjoyed of human experiences.&lt;/p>
&lt;p>In many ways, he is discovering that smell is rather like face recognition. The visual system detects patterns of light and dark and, building on experience, the brain creates a spatial map. It uses this to interpret the interrelationship of the patterns and draw conclusions that allow us to identify people and places. In the same way, we use patterns and ratios to detect both new and familiar flavors. As we eat, specialised receptors in the back of the nose detect the air molecules in our meals. From signals sent by the receptors, the brain understands smells as complex spatial patterns. Using these, as well as input from the other senses, it constructs the idea of specific flavors.&lt;/p>
&lt;p>This ability to appreciate specific aromas turns out to be central to the pleasure we get from food, much as our ability to recognise individuals is central to the pleasures of social life. The process is so embedded in our brains that our sense of smell is critical to our enjoyment of life at large. Recent studies show that people who lose the ability to smell become socially insecure, and their overall level of happiness plummets.&lt;/p>
&lt;p>Working out the role of smell in flavor interests food scientists, psychologists, and cooks alike. The relatively new discipline of molecular gastronomy, especially, relies on understanding the mechanics of aroma to manipulate flavor for maximum impact. In this discipline, chefs use their knowledge of the chemical changes that take place during cooking to produce eating pleasures that go beyond the &amp;lsquo;ordinary&amp;rsquo;.&lt;/p>
&lt;p>However, whereas molecular gastronomy is concerned primarily with the food or &amp;lsquo;smell, molecules, neurogastronomy is more focused on the receptor molecules and the brain&amp;rsquo;s spatial images for smell. Smell stimuli form what Shepherd terms *odor objects&amp;rsquo;, stored as memories, and these have a direct link with our emotions. The brain creates images of unfamiliar smells by relating them to other more familiar smells. Go back in history and this was part of our survival repertoire; like most animals, we drew on our sense of smell, when visual information was scarce, to single out prey.&lt;/p>
&lt;p>Thus the brain&amp;rsquo;s flavor-recognition system is a highly complex perceptual mechanism that puts all five senses to work in various combinations. Visual and sound cues contribute, such as crunching, as does touch, including the texture and feel of food on our lips and in our mouths. Then there are the taste receptors, and finally, the smell, activated when we inhale. The engagement of our emotions can be readily illustrated when we picture some of the wide- ranging facial expressions that are elicited by various foods - many of them hard-wired into our brains at birth. Consider the response to the sharpness of a lemon and compare that with the face that is welcoming the smooth wonder of chocolate.&lt;/p>
&lt;p>The flavor-sensing system, ever receptive to new combinations, helps to keep our brains active and flexible. It also has the power to shape our desires and ultimately our bodies. On the horizon we have the positive application of neurogastronomy: manipulating flavor to curb our appetites.&lt;/p></description></item><item><title>Docs: R.OG2.2 Dawn of the robots</title><link>https://note.codiy.net/ielts/r.og2.2</link><pubDate>Tue, 09 Aug 2022 13:11:52 +0800</pubDate><guid>https://note.codiy.net/ielts/r.og2.2</guid><description>
&lt;blockquote>
&lt;p>They’re already here 一 driving cars, vacuuming carpets and feeding hospital patients. They may not be walking, talking, human-like sentient beings, but they are clever&amp;hellip; and a little creepy.&lt;/p>
&lt;/blockquote>
&lt;h4 id="a">A&lt;/h4>
&lt;p>At first sight it looked like a typical suburban road accident. A Land Rover approached a Chevy Tahoe estate car that had stopped at a kerb; the Land Rover pulled out and tried to pass the Tahoe just as it started off again. There was a crack of fenders and the sound of paintwork being scraped, the kind of minor mishap that occurs on roads thousands of times every day. Normally drivers get out, gesticulate, exchange insurance details and then drive off. But not on this occasion. No one got out of the cars for the simple reason that they had no humans inside them; the Tahoe and Land Rover were being controlled by computers competing in November&amp;rsquo;s DARPA (the U.S. Defence Advanced Research Projects Agency) Urban Challenge.&lt;/p>
&lt;h4 id="b">B&lt;/h4>
&lt;p>The idea that machines could perform to such standards is startling. Driving is a complex task that takes humans a long time to perfect. Yet here, each car had its on-board computer loaded with a digital map and route plans, and was instructed to negotiate busy roads; differentiate between pedestrians and stationary objects; determine whether other vehicles were parked or moving off; and handle various parking manoeuvres, which robots turn out to be unexpectedly adept at. Even more striking was the fact that the collision between the robot Land Rover, built by researchers at the Massachusetts Institute of Technology, and the Tahoe, fitted out by Cornell University Artificial Intelligence (Al) experts, was the only scrape in the entire competition. Yet only three years earlier, at DARPA&amp;rsquo;s previous driverless car race, every robot competitor 一 directed to navigate across a stretch of open desert 一 either crashed or seized up before getting near the finishing line.&lt;/p>
&lt;h4 id="c">C&lt;/h4>
&lt;p>It is a remarkable transition that has clear implications for the car of the future. More importantly, it demonstrates how robotics sciences and Artificial Intelligence have progressed in the past few years 一 a point stressed by Bill Gates, the Microsoft boss who is a convert to these causes. ‘The robotics industry is developing in much the same way the computer business did 30 years ago,&amp;rsquo; he argues. As he points out, electronics companies make toys that mimic pets and children with increasing sophistication. ‘I can envision a future in which robotic devices will become a nearly ubiquitous part of our day-to-day lives/ says Gates. ‘We may be on the verge of a new era, when the PC will get up off the desktop and allow us to see, hear, touch and manipulate objects in places where we are not physically present.'&lt;/p>
&lt;h4 id="d">D&lt;/h4>
&lt;p>What is the potential for robots and computers in the near future? ‘The fact is we still have a way to go before real robots catch up with their science fiction counterparts,&amp;rsquo; Gates says. So what are the stumbling blocks? One key difficulty is getting robots to know their place. This has nothing to do with class or etiquette, but concerns the simple issue of positioning. Humans orient themselves with other objects in a room very easily. Robots find the task almost impossible. &amp;ldquo;Even something as simple as telling the difference between an open door and a window can be tricky for a robot,&amp;rsquo; says Gates. This has, until recently, reduced robots to fairly static and cumbersome roles.&lt;/p>
&lt;h4 id="e">E&lt;/h4>
&lt;p>For a long time, researchers tried to get round the problem by attempting to re-create the visual processing that goes on in the human cortex. However, that challenge has proved to be singularly exacting and complex. So scientists have turned to simpler alternatives: ‘We have become far more pragmatic in our work,’ says Nello Cristianini, Professor of Artificial Intelligence at the University of Bristol in England and associate editor of the Journal of Artificial Intelligence Research. ‘We are no longer trying to re-create human functions. Instead, we are looking for simpler solutions with basic electronic sensors, for example. &amp;lsquo;This approach is exemplified by vacuuming robots such as the Electrolux Trilobite. The Trilobite scuttles around homes emitting ultrasound signals to create maps of rooms, which are remembered for future cleaning. Technology like this is now changing the face of robotics, says philosopher Ron Chrisley, director of the Centre for Research in Cognitive Science at the University of Sussex in England.&lt;/p>
&lt;h4 id="f">F&lt;/h4>
&lt;p>Last year, a new Hong Kong restaurant, Robot Kitchen, opened with a couple of sensor-laden humanoid machines directing customers to their seats. Each possesses a touch-screen on which orders can be keyed in. The robot then returns with the correct dishes. In Japan, University of Tokyo researchers recently unveiled a kitchen zandroid7 that could wash dishes, pour tea and make a few limited meals. The ultimate aim is to provide robot home helpers for the sick and the elderly, a key concern in a country like Japan where 22 percent of the population is 65 or older. Over US$1 billion a year is spent on research into robots that will be able to care for the elderly. &amp;lsquo;Robots first learn basic competence 一 how to move around a house without bumping into things. Then we can think about teaching them how to interact with humans,’ Chrisley said. Machines such as these take researchers into the field of socialised robotics: how to make robots act in a way that does not scare or offend individuals. ‘We need to study how robots should approach people, how they should appear. That is going to be a key area for future research,&amp;rsquo; adds Chrisley.&lt;/p></description></item><item><title>Docs: R.OG2.3 It's your choice! 一 Or is it really?</title><link>https://note.codiy.net/ielts/r.og2.3</link><pubDate>Tue, 09 Aug 2022 13:11:52 +0800</pubDate><guid>https://note.codiy.net/ielts/r.og2.3</guid><description>
&lt;blockquote>
&lt;p>As we move from the industrial age to the information age, societal demands on our mental capabilities are no less taxing ···&lt;/p>
&lt;/blockquote>
&lt;p>We are constantly required to process a wide range of information to make decisions. Sometimes, these decisions are trivial, such as what marmalade to buy. At other times, the stakes are higher, such as deciding which symptoms to report to the doctor. However, the fact that we are accustomed to processing large amounts of information does not mean that we are better at it (Chabris &amp;amp; Simons, 2009). Our sensory and cognitive systems have systematic ways of failing of which we are often, perhaps blissfully, unaware.&lt;/p>
&lt;p>Imagine that you are taking a walk in your local city park when a tourist approaches you asking for directions. During the conversation, two men carrying a door pass between the two of you. If the person asking for directions had changed places with one of the people carrying the door, would you notice? Research suggests that you might not. Harvard psychologists Simons and Levi (1998) conducted a field study using this exact set-up and found that the change in identity went unnoticed by 7 (46.6%) of the 15 participants. This phenomenon has been termed ‘change blindness&amp;rsquo; and refers to the difficulty that observers have in noticing changes to visual scenes (e.g. the person swap), when the changes are accompanied by some other visual disturbance (e.g. the passing of the door).&lt;/p>
&lt;p>Over the past decade, the change blindness phenomenon has been replicated many times. Especially noteworthy is an experiment by Davies and Hine (2007) who studied whether change blindness affects eyewitness identification. Specifically, participants were presented with a video enactment of a burglary. In the video, a man entered a house, walking through the different rooms and putting valuables into a knapsack. However, the identity of the burglar changed after the first half of the film while the initial burglar was out of sight. Out of the 80 participants, 49 (61%) did not notice the change of the burglar&amp;rsquo;s identity, suggesting that change blindness may have serious implications for criminal proceedings.&lt;/p>
&lt;p>To most of us, it seems bizarre that people could miss such obvious changes while they are paying active attention. However, to catch those changes, attention must be targeted to the changing feature. In the study described above, participants were likely not to have been expecting the change to happen, and so their attention may have been focused on the valuables the burglar was stealing, rather than the burglar.&lt;/p>
&lt;p>Drawing from change blindness research, scientists have come to the conclusion that we perceive the world in much less detail than previously thought (Johansson, Hall, &amp;amp; Sikstrom, 2008). Rather than monitoring all of the visual details that surround us, we seem to focus our attention only on those features that are currently meaningful or important, ignoring those that are irrelevant to our current needs and goals. Thus at any given time, our representation of the world surrounding us is crude and incomplete, making it possible for changes or manipulations to go undetected (Chabris &amp;amp; Simons, 2010).&lt;/p>
&lt;p>Given the difficulty people have in noticing changes to visual stimuli, one may wonder what would happen if these changes concerned the decisions people make. To examine choice blindness, Hall and colleagues (2010) invited supermarket customers to sample two different kinds of jams and teas. After participants had tasted or smelled both samples, they indicated which one they preferred. Subsequently, they were purportedly given another sample of their preferred choice. On half of the trials, however, these were samples of the non-chosen jam or tea. As expected, only about one-third of the participants detected this manipulation. Based on these findings, Hall and colleagues proposed that choice blindness is a phenomenon that occurs not only for choices involving visual material, but also for choices involving gustatory and olfactory information.&lt;/p>
&lt;p>Recently, the phenomenon has also been replicated for choices involving auditory stimuli (Sauerland, Sagana, &amp;amp; Otgaar, 2012). Specifically, participants had to listen to three pairs of voices and decide for each pair which voice they found more sympathetic or more criminal. The voice was then presented again; however, the outcome was manipulated for the second voice pair and participants were presented with the non-chosen voice. Replicating the findings by Hall and colleagues, only 29% of the participants detected this change.&lt;/p>
&lt;p>Merckelbach, Jelicic, and Pieters (2011) investigated choice blindness for intensity ratings of one&amp;rsquo;s own psychological symptoms. Their participants had to rate the frequency with which they experienced 90 common symptoms (e.g. anxiety, lack of concentration, stress, headaches etc.) on a 5-point scale. Prior to a follow-up interview, the researchers inflated ratings for two symptoms by two points. For example, when participants had rated their feelings of shyness, as 2 (i.e. occasionally), it was changed to 4 (i.e. all the time). This time, more than half (57%) of the 28 participants were blind to the symptom rating escalation and accepted it as their own symptom intensity rating. This demonstrates that blindness is not limited to recent preference selections, but can also occur for intensity and frequency.&lt;/p>
&lt;p>Together, these studies suggest that choice blindness can occur in a wide variety of situations and can have serious implications for medical and judicial outcomes. Future research is needed to determine how, in those situations, choice blindness can be avoided.&lt;/p></description></item><item><title>Docs: R4.1.1 These Misconceptions of Tropical Rainforests</title><link>https://note.codiy.net/ielts/r4.1.1.html</link><pubDate>Tue, 09 Aug 2022 13:11:52 +0800</pubDate><guid>https://note.codiy.net/ielts/r4.1.1.html</guid><description>
&lt;p>Adults and children are frequently confronted with statements about the alarming rate of loss of tropical rainforests. For example, one graphic illustration to which children might readily relate is the estimate that rainforests are being destroyed at a rate equivalent to one thousand football fields every forty minutes – about the duration of a normal classroom period. In the face of the frequent and often vivid media coverage,it is likely that children will have formed ideas about rainforests – what and where they are, why they are important, what endangers them – independent of any formal tuition. It is also possible that some of these ideas will be mistaken.&lt;/p>
&lt;p>Many studies have shown that children harbor misconceptions about ‘pure’,curriculum science. These misconceptions do not remain isolated but become incorporated into a multifaceted, but organized, conceptual framework, making it and the component ideas, some of which are erroneous, more robust but also accessible to modification. These ideas may be developed by children absorbing ideas through the popular media. Sometimes this information may be erroneous. It seems schools may not be providing an opportunity for children to re-express their ideas and so have them tested and refined by teachers and their peers.&lt;/p>
&lt;p>Despite the extensive coverage in the popular media of the destruction of rainforests, little formal information is available about children’s ideas in this area,the aim of the present study is to start to provide such information, to help teachers design their educational strategies to build upon correct ideas and to displace misconceptions and to plan programs in environmental studies in their schools.&lt;/p>
&lt;p>The study surveys children’s scientific knowledge and attitudes to rainforests. Secondary school children were asked to complete a questionnaire containing five open-form questions. The most frequent responses to the first question were descriptions which are self-evident from the term ‘rainforest’. Some children described them as damp, wet or hot. The second question concerned the geographical location of rainforests. The commonest responses were continents or countries: Africa(given by 43% of children), South America (30%), Brazil (25%). Some children also gave more general locations, such as being near the Equator.&lt;/p>
&lt;p>Responses to question three concerned the importance of rainforests. The dominant idea, raised by 64% of the pupils, was that rainforests provide animals with habitats.Fewer students responded that rainforests provide plant habitats, and even fewer mentioned the indigenous populations of rainforests. More girls (70%) than boys (60%) raised the idea of rainforest as animal habitats.&lt;/p>
&lt;p>Similarly, but at a lower level, more girls (13%) than boys (5%) said that rainforests provided human habitats. These observations are generally consistent with our previous studied of pupils’ views about the use and conservation of rainforests, in which girls were shown to be more sympathetic to animals and expressed views which seem to place an intrinsic value on non-human animal life.&lt;/p>
&lt;p>The fourth question concerned the causes of the destruction of rainforests. Perhaps encouragingly, more than half of the pupil (59%) identified that it is human activities which are destroying rainforests, some personalizing the responsibility by the use of terms such as ‘we are’. About 18% of the pupils referred specifically to logging activity.&lt;/p>
&lt;p>One misconception, expressed by some 10% of the pupils, was that acid rain is responsible for rainforest destruction; a similar proportion said that pollution is destroying rainforests. Here, children are confusing rainforest destruction with damage to the forests of Western Europe by these factors. While two fifths of the students provided the information that the rainforests provide oxygen, in some cases this response also embraced the misconception that rainforest destruction would reduce atmospheric oxygen, making the atmosphere incompatible with human life on Earth.&lt;/p>
&lt;p>In answer to the final question about the importance of rainforest conservation, the majority of children simply said that we need rainforests to survive. Only a few of the pupils (6%) mentioned that rainforest destruction may contribute to global warming. This is surprising considering the high level of media coverage on this issue. Some children expressed the idea that the conservation of rainforests is not important.&lt;/p>
&lt;p>The results of this study suggest that certain ideas predominate in the thinking of children about rainforests. Pupils’ responses indicate some misconceptions in basic scientific knowledge of rainforests’ ecosystems such as their ideas about rainforests as habitats for animals, plants and humans and the relationship between climatic change and destruction of rainforests.&lt;/p>
&lt;p>Pupils did not volunteer ideas that suggested that they appreciated the complexity of causes of rainforest destruction. In other words, they gave no indication of an appreciation of either the rage of ways in which rainforests are important or the complex social, economic and political factors which drive the activities which are destroying the rainforests. One encouragement is that the results of similar studies about other environmental issues suggest that older children seem to acquire the ability to appreciate value and evaluate conflicting views. Environmental education offers an arena in which these sills can be developed, which is essential for these children as future decision –makers.&lt;/p></description></item><item><title>Docs: R4.1.2 What Do Whales Feel?</title><link>https://note.codiy.net/ielts/r4.1.2</link><pubDate>Tue, 09 Aug 2022 13:11:52 +0800</pubDate><guid>https://note.codiy.net/ielts/r4.1.2</guid><description>
&lt;p>An examination of the functioning of the senses in cetaceans, the group of mammals comprising whales, dolphins and porpoises
Some of the senses that we and other terrestrial mammals take for granted are either reduced or absent in cetaceans or fail to function well in water. For example, it appears from their brain structure that toothed species are unable to smell. Baleen species, on the other hand, appear to have some related brain structures but it is not known whether these are functional. It has been speculated that, as the blowholes evolved and migrated to the top of the head, the neural pathways serving sense of smell may have been nearly all sacrificed. Similarly, although at least some cetaceans have taste buds, the nerves serving these have degenerated or are rudimentary.&lt;/p>
&lt;p>The sense of touch has sometimes been described as weak too, but this view is probably mistaken. Trainers of captive dolphins and small whales often remark on their animals’ responsiveness to being touched or rubbed, and both captive and freeranging cetacean individuals of all species (particularly adults and calves, or members of the same subgroup) appear to make frequent contact. This contact may help to maintain order within a group, and stroking or touching are part of the courtship ritual in most species. The area around the blowhole is also particularly sensitive and captive animals often object strongly to being touched there.&lt;/p>
&lt;p>The sense of vision is developed to different degrees in different species. Baleen species studied at close quarters underwater – specifically a grey whale calf in captivity for a year, and free-ranging right whales and humpback whales studied and filmed off Argentina and Hawaii – have obviously tracked objects with vision underwater, and they can apparently see moderately well both in water and in air. However, the position of the eyes so restricts the field of vision in baleen whales that they probably do not have stereoscopic vision.&lt;/p>
&lt;p>On the other hand, the position of the eyes in most dolphins and porpoises suggests that they have stereoscopic vision forward and downward. Eye position in freshwater dolphins, which often swim on their side or upside down while feeding, suggests that what vision they have is stereoscopic forward and upward. By compare-son, the bottlenose dolphin has extremely keen vision in water. Judging from the way it watches and tracks airborne flying fish, it can apparently see fairly well through the air–water interface as well. And although preliminary experimental evidence suggests that their in-air vision is poor, the accuracy with which dolphins leap high to take small fish out of a trainer’s hand provides anecdotal evidence to the contrary.&lt;/p>
&lt;p>Such variation can no doubt be explained with reference to the habitats in which individual species have developed. For example, vision is obviously more useful to species inhabiting clear open waters than to those living in turbid rivers and flooded plains. The South American boutu and Chinese beiji, for instance, appear to have very limited vision, and the Indian susus are blind, their eyes reduced to slits that probably allow them to sense only the direction and intensity of light.&lt;/p>
&lt;p>Although the senses of taste and smell appear to have deteriorated, and vision in water appears to be uncertain, such weaknesses are more than compensated for by cetaceans’ well-developed acoustic sense. Most species are highly vocal, although they vary in the range of sounds they produce, and many forage for food using echo-location*. Large baleen whales primarily use the lower frequencies and are often limited in their repertoire. Notable exceptions are the nearly song-like choruses of bowhead whales in summer and the complex, haunting utterances of the humpback whales. Toothed species in general employ more of the frequency spectrum, and produce a wider variety of sounds, than baleen species (though the sperm whale apparently produces a monotonous series of high-energy clicks and little else). Some of the more complicated sounds are clearly communicative, although what role they may play in the social life and ‘culture’ of cetaceans has been more the subject of wild speculation than of solid science.&lt;/p>
&lt;ol>
&lt;li>Echolocation: the perception of objects by means of sound wave echoes.&lt;/li>
&lt;/ol></description></item><item><title>Docs: R4.1.3 Visual Symbols and the Blind</title><link>https://note.codiy.net/ielts/r4.1.3</link><pubDate>Tue, 09 Aug 2022 13:11:52 +0800</pubDate><guid>https://note.codiy.net/ielts/r4.1.3</guid><description>
&lt;h4 id="part-1">Part 1&lt;/h4>
&lt;p>From a number of recent studies, it has become clear that blind people can appreciate the use of outlines and perspectives to describe the arrangement of objects and other surfaces in space. But pictures are more than literal representations. This fact was drawn to my attention dramatically when a blind woman in one of my investigations decided on her own initiative to draw a wheel as it was spinning. To show this motion, she traced a curve inside the circle (Fig.1). I was taken aback. Lines of motion, such as the one she used, are a very recent invention in the history of illustration. Indeed, as art scholar David Kunzle notes, Wilhelm Busch, a trend-setting nineteenth-century cartoonist, used virtually no motion lines in his popular figures until about 1877.&lt;/p>
&lt;p>When I asked several other blind study subjects to draw a spinning wheel, one particularly clever rendition appeared repeatedly: several subjects showed the wheel’s spokes as curved lines. When asked about these curves, they all described them as metaphorical ways of suggesting motion. Majority rule would argue that this device somehow indicated motion very well. But was it a better indicator than, say, broken or wavy lines – or any other kind of line, for that matter? The answer was not clear. So I decided to test whether various lines of motion were apt ways of showing movement or if they were merely idiosyncratic marks. Moreover, I wanted to discover whether there were differences in how the blind and the sighted interpreted lines of motion.&lt;/p>
&lt;p>To search out these answers, I created raised-line drawings of five different wheels, depicting spokes with lines that curved, bent, waved, dashed and extended beyond the perimeter of the wheel. I then asked eighteen blind volunteers to feel the wheels and assign one of the following motions to each wheel: wobbling, spinning fast, spinning steadily, jerking or braking. My control group consisted of eighteen sighted undergraduates from the University of Toronto.&lt;/p>
&lt;p>All but one of the blind subjects assigned distinctive motions to each wheel. Most guessed that the curved spokes indicated that the wheel was spinning steadily; the wavy spokes, they thought, suggested that the wheel was wobbling; and the bent spokes were taken as a sign that the wheel was jerking. Subjects assumed that spokes extending beyond the wheel’s perimeter signified that the wheel had its brakes on and that dashed spokes indicated the wheel was spinning quickly.&lt;/p>
&lt;p>In addition, the favored description for the sighted was the favored description for the blind in every instance. What is more, the consensus among the sighted was barely higher than that among the blind. Because motion devices are unfamiliar to the blind, the task I gave them involved some problem solving. Evidently, however, the blind not only figured out meanings for each line of motion, but as a group they generally came up with the same meaning at least as frequently as did sighted subjects.&lt;/p>
&lt;h4 id="part-2">Part 2&lt;/h4>
&lt;p>We have found that the blind understand other kinds of visual metaphors as well. One blind woman drew a picture of a child inside a heart – choosing that symbol, she said, to show that love surrounded the child. With Chang Hong Liu, a doctoral student from China, I have begun exploring how well blind people understand the symbolism behind shapes such as hearts that do not directly represent their meaning. We gave a list of twenty pairs of words to sighted subjects and asked them to pick from each pair the term that best related to a circle and the term that best related to a square. For example, we asked: What goes with soft? A circle or a square? Which shape goes with hard?&lt;/p>
&lt;p>All our subjects deemed the circle soft and the square hard. A full 94% ascribed happy to the circle, instead of sad. But other pairs revealed less agreement: 79% matched fast to slow and weak to strong, respectively. And only 51% linked deep to circle and shallow to square. (See Fig.2) When we tested four totally blind volunteers using the same list, we found that their choices closely resembled those made by the sighted subjects. One man, who had been blind since birth, scored extremely well. He made only one match differing from the consensus, assigning ‘far’ to square and ‘near’ to circle. In fact, only a small majority of sighted subjects – 53% – had paired far and near to the opposite partners. Thus, we concluded that the blind interpret abstract shapes as sighted people do.&lt;/p></description></item><item><title>Docs: R4.2.1 Lost for Words</title><link>https://note.codiy.net/ielts/r4.2.1</link><pubDate>Tue, 09 Aug 2022 13:11:52 +0800</pubDate><guid>https://note.codiy.net/ielts/r4.2.1</guid><description>
&lt;h3 id="many-minority-languages-are-on-the-danger-lists">Many Minority Languages Are on the Danger Lists&lt;/h3>
&lt;p>In the Native American Navajo nation, which sprawls across four states in the American south-west, the native language is dying. Most of its speakers are middle-aged or elderly. Although many students take classes in Navajo, the schools are run in English. Street signs, supermarket goods and even their own newspaper are all in English. Not surprisingly, linguists doubt that any native speakers of Navajo will remain in a hundred years’ time.&lt;/p>
&lt;p>Navajo is far from alone. Half the world’s 6,800 languages are likely to vanish within two generations—that’s one language lost every ten days. Never before has the planet’s linguistic diversity shrunk at such a pace. ‘At the moment, we are heading for about three or four languages dominating the world,’ says Mark Pagel, an evolutionary biologist at the University of Reading. ‘It’s a mass extinction, and whether we will ever rebound from the loss is difficult to know.’&lt;/p>
&lt;p>Isolation breeds linguistic diversity: as a result, the world is peppered with languages spoken by only a few people. Only 250 languages have more than a million speakers, and at least 3,000 have fewer than 2,500. It is not necessarily these small languages that are about to disappear. Navajo is considered endangered despite having 150,000 speakers. What makes a language endangered is not just the number of speakers, but how old they are. If it is spoken by children it is relatively safe. The critically endangered languages are those that are only spoken by the elderly, according to Michael Krauss, director of the Alassk Native Language Center, in Fairbanks.&lt;/p>
&lt;p>Why do people reject the language of their parents? It begins with a crisis of confidence, when a small community finds itself alongside a larger, wealthier society, says Nicholas Ostler, of Britain’s Foundation for Endangered Languages, in Bath. ‘People lose faith in their culture,’ he says. ‘When the next generation reaches their teens, they might not want to be induced into the old traditions.’&lt;/p>
&lt;p>The change is not always voluntary. Quite often, governments try to kill off a minority language by banning its use in public or discouraging its use in schools, all to promote national unity. The former US policy of running Indian reservation schools in English, for example, effectively put languages such as Navajo on the danger list. But Salikoko Mufwene, who chairs the Linguistics department at the University of Chicago, argues that the deadliest weapon is not government policy but economic globalization. ‘Native Americans have not lost pride in their language, but they have had to adapt to socio-economic pressures,’ he says. ‘They cannot refuse to speak English if most commercial activity is in English.’ But are languages worth saving? At the very least, there is a loss of data for the study of languages and their evolution, which relies on comparisons between languages, both living and dead. When an unwritten and unrecorded language disappears, it is lost to science.&lt;/p>
&lt;p>Language is also intimately bound up with culture, so it may be difficult to preserve one without the other. ‘If a person shifts from Navajo to English, they lose something,’ Mufwene says. ‘Moreover, the loss of diversity may also deprive us of different ways of looking at the world.’ says Pagel. There is mounting evidence that learning a language produces physiological changes in the brain. ‘Your brain and mine are different from the brain of someone who speaks French, for instance,’ Pagel says, and this could affect our thoughts and perceptions. ‘The patterns and connections we make among various concepts may be structured by the linguistic habits of our community.’&lt;/p>
&lt;p>So despite linguists’ best efforts, many languages will disappear over the next century. But a growing interest in cultural identity may prevent the direst predictions from coming true. ‘The key to fostering diversity is for people to learn their ancestral tongue, as well as the dominant language,’ says Doug Whalen, founder and president of the Endangered Language Fund in New Haven, Connecticut . ‘Most of these languages will not survive without a large degree of bilingualism,’ he says. In New Zealand, classes for children have slowed the erosion of Maori and rekindled interest in the language. A similar approach in Hawaii has produced about 8,000 new speakers of Polynesian languages in the past few years. In California,‘apprentice’ programs have provided life support to several indigenous languages. Volunteer ‘apprentices’ pair up with one of the last living speakers of a Native American tongue to learn a traditional skill such as basket weaving, with instruction exclusively in the endangered language. After about 300 hours of training they are generally sufficiently fluent to transmit the language to the next generation. But Mufwene says that preventing a language dying out is not the same as giving it new life by using it every day. ‘Preserving a language is more like preserving fruits in a jar,’ he says.&lt;/p>
&lt;p>However, preservation can bring a language back from the dead. There are examples of languages that have survived in written form and then been revived by later generations. But a written form is essential for this, so the mere possibility of revival has led many speakers of endangered languages to develop systems of writing where none existed before.&lt;/p></description></item><item><title>Docs: R4.2.2 Alternative Medicine in Australia</title><link>https://note.codiy.net/ielts/r4.2.2</link><pubDate>Tue, 09 Aug 2022 13:11:52 +0800</pubDate><guid>https://note.codiy.net/ielts/r4.2.2</guid><description>
&lt;p>The first students to study alternative medicine at university level in Australia began their four-year, full-time course at the University of Technology Sydney, in early 1994. Their course covered, among other therapies, acupuncture. The theory they learnt is based on the traditional Chinese explanation of this ancient healing art that it can regulate the flow of‘Qi’ or energy through pathways in the body. This course reflects how far some alternative therapies have come in their struggle for acceptance by the medical establishment.&lt;/p>
&lt;p>Australia has been unusual in the Western world in having a very conservative attitude to natural or alternative therapies, according to Dr Paul Laver, a lecturer in Public Health at the University of Sydney.‘We’ve had a tradition of doctors being fairly powerful and I guess they are pretty loath to allow any pretenders to their position to come into it.’ In many other industrialized countries, orthodox and alternative medicine have worked‘hand in glove’ for years. In Europe, only orthodox doctors can prescribe herbal medicine. In Germany, plant remedies account for 10% of the national turnover of pharmaceuticals. Americans made more visits to alternative therapists than to orthodox doctors in 1990, and each year they spend about $US12 billion on therapies that have not been scientifically tested.&lt;/p>
&lt;p>Disenchantment with orthodox medicine has seen the popularity of alternative therapies in Australia climb steadily during the past 20 years. In a 1983 national health survey, 1.9% of people said they had contacted a chiropractor, naturopath, osteopath, acupuncturist or herbalist in the two weeks prior to the survey. By 1990, this figure had risen to 2.6% of the population. The 550,000 consultations with alternative therapists reported in the 1990 survey represented about an eighth of the total number of consultations with medically qualified personnel covered by the survey, according to Dr Laver and colleagues writing in the Australian Journal of Public Health in 1993. ‘A better educated and less accepting public has become disillusioned with the experts in general, and increasingly skeptical about science and empirically based knowledge,’ they said.‘The high standing of professionals, including doctors, has been eroded as a consequence.’&lt;/p>
&lt;p>Rather than resisting or criticizing this trend, increasing numbers of Australian doctors, particularly younger ones, are forming group practices with alternative therapists or taking courses themselves, particularly in acupuncture and herbalism. Part of the incentive was financial, Dr Laver said. ‘ The bottom line is that most general practitioners are business people. If they see potential clientele going elsewhere, they might want to be able to offer a similar service.'&lt;/p>
&lt;p>In 1993, Dr Laver and his colleagues published a survey of 289 Sydney people who attended eight alternative therapists’ practices in Sydney. These practices offered a wide range of alternative therapies from 25 therapists. Those surveyed had experienced chronicillnesses, for which orthodox medicine had been able to provide little relief. They commented that they liked the holistic approach of their alternative therapists and the friendly, concerned and detailed attention they had received. The cold, impersonal manner of orthodox doctors featured in the survey. An increasing exodus from their clinics, coupled with this and a number of other relevant surveys carried out in Australia, all pointing to orthodox doctors’ inadequacies, have led mainstream doctors themselves to begin to admit they could learn from the personal style of alternative therapists. Dr Patrick Store, President of the Royal College of General Practitioners, concurs that orthodox doctors could learn a lot about bedside manner and advising patients on preventative health from alternative therapists.&lt;/p>
&lt;p>According to the Australian Journal of Public Health, 18% of patients visiting alternative therapists do so because they suffer from musculo-skeletal complaints, 12% suffer from digestive problems, which is only 1% more than those suffering from emotional problems. Those suffering from respiratory complaints represent 7% of their patients, and candida sufferers represent an equal percentage. Headache sufferers and those complaining of general ill health represent 6% and 5% of patients respectively and a further 4% see therapists for general health maintenance.&lt;/p>
&lt;p>The survey suggested that complementary medicine is probably a better term than alternative medicine. Alternative medicine appears to be an adjunct sought in times of disenchantment when conventional medicine seems not to offer the answer.&lt;/p></description></item><item><title>Docs: R4.2.3 Play is a Serious Business</title><link>https://note.codiy.net/ielts/r4.2.3</link><pubDate>Tue, 09 Aug 2022 13:11:52 +0800</pubDate><guid>https://note.codiy.net/ielts/r4.2.3</guid><description>
&lt;blockquote>
&lt;p>Does Play Help Develop Bigger, Better Brains? Bryant Furlow Investigates.&lt;/p>
&lt;/blockquote>
&lt;h4 id="a">A&lt;/h4>
&lt;p>Playing is a serious business. Children engrossed in a make-believe world, fox cubs play-fighting or kittens teasing a ball of string aren’t just having fun. Play may look like a carefree and exuberant way to pass the time before the hard work of adulthood comes along, but there’s much more to it than that. For a start, play can even cost animals their lives. Eighty percent of deaths among juvenile fur seals occur because playing pups fail to spot predators approaching. It is also extremely expensive in terms of energy. Playful young animals use around two or three percent of their energy cavorting, and in children that figure can be closer to fifteen percent.‘Even two or three percent is huge,’ says John Byers of Idaho University. ‘You just don’t find animals wasting energy like that,’ he adds. There must be a reason.&lt;/p>
&lt;h4 id="b">B&lt;/h4>
&lt;p>But if play is not simply a developmental hiccup, as biologists once thought, why did it evolve? The latest idea suggests that play has evolved to build big brains. In other words, playing makes you intelligent. Playfulness, it seems, is common only among mammals, although a few of the larger-brained birds also indulge. Animals at play often use unique signs-tail-wagging in dogs, for example—to indicate that activity superficially resembling adult behavior is not really in earnest. A popular explanation of play has been that it helps juveniles develop the skills they will need to hunt, mate and socialize as adults. Another has been that it allows young animals to get in shape for adult life by improving their respiratory endurance. Both these ideas have been questioned in recent years.&lt;/p>
&lt;h4 id="c">C&lt;/h4>
&lt;p>Take the exercise theory. If play evolved to build muscle or as a kind of endurance training, then you would expect to see permanent benefits. But Byers points out that the benefits of increased exercise disappear rapidly after training stops, so any improvement in endurance resulting from juvenile play would be lost by adulthood. ‘If the function of play was to get into shape,’ says Byers, ‘the optimum time for playing would depend on when it was most advantageous for the young of a particular species to do so. But it doesn’t work like that.’ Across species, play tends to peak about halfway through the suckling stage and then decline.&lt;/p>
&lt;h4 id="d">D&lt;/h4>
&lt;p>Then there’s the skills-training hypothesis. At first glance, playing animals do appear to be practicing the complex manoeuvres they will need in adulthood. But a closer inspection reveals this interpretation as too simplistic. In one study, behavioral ecologist Tim Caro, from the University of California, looked at the predatory play of kittens and their predatory behavior when they reached adulthood. He found that the way the cats played had no significant effect on their hunting prowess in later life.&lt;/p>
&lt;h4 id="e">E&lt;/h4>
&lt;p>Earlier this year, Sergio Pellis of Lethbridge University, Canada, reported that there is a strong positive link between brain size and playfulness among mammals in general. Comparing measurements for fifteen orders of mammal, he and his team found larger brains (for a given body size) are linked to greater playfulness. The converse was also found to be true. Robert Barton of Durham University believes that, because large brains are more sensitive to developmental stimuli than smaller brains, they require more play to help mould them for adulthood. ‘I concluded it’s to do with learning, and with the importance of environmental data to the brain during development,’ he says.&lt;/p>
&lt;h4 id="f">F&lt;/h4>
&lt;p>According to Byers, the timing of the playful stage in young animals provides an important clue to what’s going on. If you plot the amount of time a juvenile devotes to play each day over the course of its development, you discover a pattern typically associated with a ‘sensitive period’-a brief development window during which the brain can actually be modified in ways that are not possible earlier or later in life. Think of the relative ease with which young children-but not infants or adults-absorb language. Other researchers have found that play in cats, rats and mice is at its most intense just as this ‘window of opportunity’ reaches its peak.&lt;/p>
&lt;h4 id="g">G&lt;/h4>
&lt;p>‘People have not paid enough attention to the amount of the brain activated by play,’says Marc Bekoff from Colorado University. Bekoff studied coyote pups at play and found that the kind of behavior involved was markedly more variable and unpredictable than that of adults. Such behavior activates many different parts of the brain, he reasons. Bekoff likens it to a behavioral kaleidoscope, with animals at play jumping rapidly between activities. ‘They use behavior from a lot of different context&amp;ndash; predation, aggression, reproduction,’ he says. ‘Their developing brain is getting all sorts of stimulation.’&lt;/p>
&lt;h4 id="h">H&lt;/h4>
&lt;p>Not only is more of the brain involved in play than was suspected, but it also seems to activate higher cognitive processes. ‘There’s enormous cognitive, involvement in play,’ says Bekoof. He points out that play often involves complex assessments of playmates, ideas of reciprocity and the use of specialized signals and rules. He believes that play creates a brain that has greater behavioral flexibility and improved potential for learning later in life. The idea is backed up by the work of Stephen Siviy of Gettysburg College. Siviy studied how bouts of play affected the brain’s levels of a particular chemical associated with the stimulation and growth of nerve cells. He was surprised by the extent of the activation. ‘Play just lights everything up,’ he says. By allowing link-ups between brain areas that might not normally communicate with each other, play may enhance creativity.&lt;/p>
&lt;h4 id="i">I&lt;/h4>
&lt;p>What might further experimentation suggest about the way children are raised in many societies today? We already know that rat pups denied the chance to play grow smaller brain components and fail to develop the ability to apply social rules when they interact with their peers. With schooling beginning earlier and becoming increasingly exam-orientated, play is likely to get even less of a look-in. Who knows what the result of that will be?&lt;/p></description></item><item><title>Docs: R4.3.1 Micro-Enterprise Credit for Street Youth</title><link>https://note.codiy.net/ielts/r4.3.1</link><pubDate>Tue, 09 Aug 2022 13:11:52 +0800</pubDate><guid>https://note.codiy.net/ielts/r4.3.1</guid><description>
&lt;blockquote>
&lt;p>&amp;lsquo;I am from a large, poor family and for many years we have done without breakfast. Ever since I joined the Street Kids International program I have been able to buy my family sugar and buns for breakfast. I have also bought myself decent second-hand clothes and shoes.’&lt;br>
Doreen Soko&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>&amp;lsquo;We’ve had business experience. Now I’m confident to expand what we’ve been doing.I’ve learnt cash management, and the way of keeping money so we save for reinvestment.Now business is a part of our lives. As well, we didn’t know each other before—now we’ve made new friends.’&lt;br>
Fan Kaoma.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>Participants in the Youth skills Enterprise Initiative Program, Zambia&lt;/p>
&lt;/blockquote>
&lt;p>Introduction
Although small-scale business training and credit programs have become more common throughout the world, relatively little attention has been paid to the need to direct such opportunities to young people. Even less attention has been paid to children living on the street or in difficult circumstance. Over the past nine years, Street Kids International (S. K. I.) has been working with partnerorganizations in Africa, Latin America and India to support the economic lives of streetchildren. The purpose of this paper is to share some of the lessons S.K.I and our partnershave learned.&lt;/p>
&lt;p>Background
Typically, children do not end up on the streets due to a single cause, but to a combination of factors: a dearth of adequately funded schools, the demand for income at home, family breakdown and violence. The street may be attractive to children as a place to find adventurous play and money. However, it is also a place where some children are exposed, with little or no protection, to exploitative employment, urban crime, and abuse. Children who work on the streets are generally involved in unskilled, labor-intensive tasks which require long hours, such as shining shoes, carrying goods, guarding or washing cars, and informal trading. Some may also earn income through begging, or through theft and other illegal activities. At the same time, there are street children who take pride in supporting themselves and their families and who often enjoy their work.Many children may choose entrepreneurship because it allows them a degree of independence, is less exploitative than many forms of paid-employment, and is flexibleenough to allow them to participate in other activities such as education and domestic task.&lt;/p>
&lt;p>Street Business Partnerships
S. K. I. has worked with partner organizations in Latin America, Africa and India to develop innovative opportunities for street children to earn income. ·The S. K. I. Bicycle Courier Service first started in the Sudan.Participants in this enterprise were supplied with bicycles, which they used to deliver parcels and messages, and which they were required to pay for gradually from their wages. A similar program was taken up in Bangalore, India. ·Another successful project, The Shoe Shine Collective, was a partnership program with the Y.W.C.A. in the Dominican Republic. In this project, participants were lent money to purchase shoe shine boxes. They were also given a safe place to store their equipment, and facilities for individual savings plans. ·The Youth Skills Enterprise Initiative in Zambia is a joint program with the Red Cross Society and the Y.W.C.A. Street youths are supported to start their own small business through business training, life skills training and access to credit.&lt;/p>
&lt;p>Lessons learned
The following lessons have emerged from the programs that S. K. I. and partner organizations have created.·Being an entrepreneur is not for everyone, nor for every street child. Ideally, potential participants will have been involved in the organization’s programs for at least six months, and trust and relationship building will have already been established.·The involvement of the participants has been essential to the development of relevant programs. When children have had a major role in determining procedures, they are more likely to abide by and enforce them.·It is critical for all loans to be linked to training programs that include the development of basic business and life skills.·There are tremendous advantages to involving parents or guardians in the program,where such relationships exist. Home visits allow staff the opportunity to know where the participants live, and to understand more about each individual’s situation.·Small loans are provided initially for purchasing fixed assets such as bicycles, shoe shine kits and basic building materials for a market stall. As the entrepreneurs gain experience, the enterprises can be gradually expanded and consideration can be given to increasing loan amounts. The loan amount in S.K. I. programs have generally ranged from US$30-$100.·All S. K. I. programs have charged interest on the loans, primarily to get the entrepreneurs used to the concept of paying interest on borrowed money. Generally the rates have been modest (lower than bank rates.)&lt;/p>
&lt;p>Conclusion
There is a need to recognize the importance of access to credit for impoverished young people seeking to fulfill economic needs. The provision of small loans to support the entrepreneurial dreams and ambitions of youth can be an effective means to help them change their lives. However, we believe hat credit must be extended in association with other types of support that help participants develop critical life skills as well as productive businesses.&lt;/p></description></item><item><title>Docs: R4.3.2 Volcanoes — Earth-Shattering News</title><link>https://note.codiy.net/ielts/r4.3.2</link><pubDate>Tue, 09 Aug 2022 13:11:52 +0800</pubDate><guid>https://note.codiy.net/ielts/r4.3.2</guid><description>
&lt;blockquote>
&lt;p>When Mount Pinatubo suddenly erupted on 9 June 1991, the power of volcanoes past and present again hit the headlines.&lt;/p>
&lt;/blockquote>
&lt;h4 id="a">A&lt;/h4>
&lt;p>Volcanoes are the ultimate earth-moving machinery. A violent eruption can blow the top few kilometers off a mountain, scatter fine ash practically all over the globe and hurl rock fragments into the stratosphere to darken the skies a continent away.&lt;/p>
&lt;p>But the classic eruption — cone-shaped mountain, big bang, mushroom cloud and surges of molten lava — is only a tiny part of a global story. Volcanism, the name givento volcanic processes, really has shaped the world. Eruptions have rifted continents,raised mountain chains, constructed islands and shaped the topography of the earth. The entire ocean floor has a basement of volcanic basalt.&lt;/p>
&lt;p>Volcanoes have not only made the continents, they are also thought to have made the world’s first stable atmosphere and provided all the water for the oceans, rivers and ice-caps. There are now about 600 active volcanoes. Every year they add two or three cubic kilometers of rock to the continents. Imagine a similar number of volcanoes smoking away for the last 3,500 million years. That is enough rock to explain the continental crust.&lt;/p>
&lt;p>What comes out of volcanic craters is mostly gas. More than 90% of this gas is water vapor from the deep earth: enough to explain, over 3,500 million years, the water in the oceans. The rest of the gas is nitrogen, carbon dioxide, sulphur dioxide, methane,ammonia and hydrogen. The quantity of these gases, again multiplied over 3,500 million years, is enough to explain the mass of the world’s atmosphere. We are alive because volcanoes provided the soil, air and water we need.&lt;/p>
&lt;h4 id="b">B&lt;/h4>
&lt;p>Geologists consider the earth as having a molten core, surrounded by a semi-molten mantle and a brittle, outer skin. It helps to think of a soft-boiled egg with a runny yolk, a firm but squishy white and a hard shell. If the shell is even slightly cracked during boiling,the white material bubbles out and sets like a tiny mountain chain over the crack — like an archipelago of volcanic islands such as the Hawaiian Islands. But the earth is so much bigger and the mantle below is so much hotter.&lt;/p>
&lt;p>Even though the mantle rocks are kept solid by overlying pressure, they can still slowly ‘flow’ like thick treacle. The flow, thought to be in the form of convection currents,is powerful enough to fracture the ‘eggshell’ of the crust into plates, and keep them bumping and grinding against each other, or even overlapping, at the rate of a few centimeters a year. These fracture zones, where the collisions occur, are where earthquakes happen. And, very often, volcanoes.&lt;/p>
&lt;h4 id="c">C&lt;/h4>
&lt;p>These zones are lines of weakness, or hot spots. Every eruption is different, but put at its simplest, where there are weaknesses, rocks deep in the mantle, heated to 1,350℃，will start to expand and rise. As they do so, the pressure drops, and they expand and become liquid and rise more swiftly.&lt;/p>
&lt;p>Sometimes it is slow: vast bubbles of magma — molten rock from the mantle —inch lowards the surface, cooling slowly, to show through as granite extrusions(as on Skye, or the Great Whin Sill, the lava dyke squeezed out like toothpaste that carries part of Hadrian’s Wall in northern England). Sometimes — as in Northern Ireland, Wales and the Karoo in South Africa — the magma rose faster, and then flowed out horizontally on to the surface in vast thick sheets. In the Deccan plateau in western India, there are more than two million cubic kilometers of lava, some of it 2,400 meters thick, formed over 500,000 years of slurping eruption.&lt;/p>
&lt;p>Sometimes the magma moves very swiftly indeed. It does not have time to cool as it surges upwards. The gases trapped inside the boiling rock expand suddenly, the lava glows with heat, it begins to froth, and it explodes with tremendous force. Then the slightly cooler lava following it begins to flow over the lip of the crater. It happens on Mars, it happened on the moon, it even happens on some of the moons of Jupiter and Uranus. By studying the evidence, vulcanologists can read the force of the great blasts of the past. Is the pumice light and full of holes? The explosion was tremendous. Are the rocks heavy, with huge crystalline basalt shapes, like the Giant’s Causeway in Northern Ireland? It was a slow, gentle eruption.&lt;/p>
&lt;p>The biggest eruptions are deep on the mid-ocean floor, where new lava is forcing the continents apart and widening the Atlantic by perhaps five centimeters a year. Look at maps of volcanoes, earthquakes and island chains like the Philippines and Japan, and you can see the rough outlines of what are called tectonic plates — the plates which make up the earth’s crust and mantel. The most dramatic of these is the Pacific ‘ring of fire’ where there have been the most violent explosions — Mount Pinatubo near Manila, Mount St Helen’s in the Rockies and El Chichón in Mexico about a decade ago, not to mention world-shaking blasts like Krakatoa in the Sunda Straits in 1883.&lt;/p>
&lt;h4 id="d">D&lt;/h4>
&lt;p>But volcanoes are not very predictable. That is because geological time is not like human time. During quiet periods, volcanoes cap themselves with their own lava by forming a powerful cone from the molten rocks slopping over the rim of the crater; later the lava cools slowly into a huge, hard, stable plug which blocks one further eruption until the pressure below becomes irresistible. In the case of Mount Pinatubo, this took 600 years.&lt;/p>
&lt;p>Then, sometimes, with only a small warning, the mountain blows its top. It did this at Mont Pelée in Martinique at 7.49 a.m. on 8 May, 1902. Of a town of 28,000, only two people survived. In 1815, a sudden blast removed the top 1,280 meters of Mount Tambora in Indonesia. The eruption was so fierce that dust thrown into the stratosphere darkened the skies, cancelling the following summer in Europe and North America.Thousands starved as the harvests failed, after snow in June and frosts in August.Volcanoes are potentially world news, especially the quiet ones.&lt;/p></description></item><item><title>Docs: R4.3.3 Obtaining Linguistic Data</title><link>https://note.codiy.net/ielts/r4.3.3</link><pubDate>Tue, 09 Aug 2022 13:11:52 +0800</pubDate><guid>https://note.codiy.net/ielts/r4.3.3</guid><description>
&lt;h4 id="a">A&lt;/h4>
&lt;p>Many procedures are available for obtaining data about a language. They range from a carefully planned, intensive field investigation in a foreign country to a casual intro-spection about one&amp;rsquo;s mother tongue carried out in an armchair at home.&lt;/p>
&lt;h4 id="b">B&lt;/h4>
&lt;p>In all cases, someone has to act as a source of language data — an informant. Infor-mants are (ideally) native speakers of a language, who provide utterances for analysis and other kinds of information about the language (e.g. translations, comments about correct-ness, or judgments on usage). Often, when studying their mother tongue, linguists act as their own informants, judging the ambiguity, acceptability, or other properties of utte-rances against their own intuitions. The convenience of this approach makes it widely used, and it is considered the norm in the generative approach to linguistics. But a linguist’s personal judgments are often uncertain, or disagree with the judgments of other linguists, at which point recourse is needed to more objective methods of enquiry, using non-linguists as informants. The latter procedure is unavoidable when working on foreign languages, or child speech.&lt;/p>
&lt;h4 id="c">C&lt;/h4>
&lt;p>Many factors must be considered when selecting informants — whether one is working with single speakers (a common situation when languages have not been described before), two people interacting, small groups or large-scale samples. Age, sex,social background and other aspects of identity are important, as these factors are known to influence the kind of language used. The topic of conversation and the characteristics of the social setting (e.g. the level of formality) are also highly relevant, as are the per-sonal qualities of the informants (e.g. their fluency and consistency). For larger studies,scrupulous attention has been paid to the sampling theory employed, and in all cases,decisions have to be made about the best investigative techniques to use.&lt;/p>
&lt;h4 id="d">D&lt;/h4>
&lt;p>Today, researchers often tape-record informants. This enables the linguist’s claims about the language to be checked, and provides a way of making those claims more accurate (‘difficult’pieces of speech can be listened to repeatedly). But obtaining natural-istic, good-quality data is never easy. People talk abnormally when they know they are being recorded, and sound quality can be poor. A variety of tape-recording procedures have thus been devised to minimize the ‘observer’s paradox’ (how to observe the way people behave when they are not being observed). Some recordings are made without the speakers being aware of the fact — a procedure that obtains very natural data, though ethical objections must be anticipated. Alternatively, attempts can be made to make the speaker forget about the recording, such as keeping the tape recorder out of sight, or us-ing radio microphones. A useful technique is to introduce a topic that quickly involves the speaker, and stimulates a natural language style (e.g. asking older informants about how times have changed in their locality).&lt;/p>
&lt;h4 id="e">E&lt;/h4>
&lt;p>An audio tape recording does not solve all the linguist’s problems, however. Speech is often unclear and ambiguous. Where possible,therefore, the recording has to be supplemented by the observer’s written comments on the non-verbal behavior of the participants, and about the context in general. A facial expression, for example, candramatically alter the meaning of what is-said. Video recordings avoid these problems to a large extent, but even they have limitations (the camera cannot be everywhere), and transcriptions always benefit from any additional commentary provided by an observer.&lt;/p>
&lt;h4 id="f">F&lt;/h4>
&lt;p>Linguists also make great use of structured sessions, in which they systematically ask their informants for utterances that describe certain actions, objects or behavior. With a bilingual informant, or through use of an interpreter, it is possible to use translation tech-niques (‘How do you say table in your language?’). A large number of points can be covered in a short time, using interview worksheets and questionnaires. Often, the re-searcher wishes to obtain information about just a single variable, in which case a re-stricted set of questions may be used: a particular feature of pronunciation, for example,can be elicited by asking the informant to say a restricted set of words. There are also several direct methods of elicitation, such as asking informants to fill in the blanks in a substitution frame (e.g. I ___ see a car), or feeding them the wrong stimulus for correc-tion (‘Is it possible to say I no can see?’).&lt;/p>
&lt;h4 id="g">G&lt;/h4>
&lt;p>A representative sample of language, compiled for the purpose of linguistic analysis,is known as a corpus. A corpus enables the linguist to make unbiased statements about frequency of usage, and it provides accessible data for the use of different researchers. Its range and size are variable. Some corpora attempt to cover the language as a whole, tak-ing extracts from many kinds of text; others are extremely selective, providing a collec-tion of material that deals only with a particular linguistic feature. The size of the corpus depends on practical factors, such as the time available to collect, process and store thedata: it can take up to several hours to provide an accurate transcription of a few minutes of speech. Sometimes a small sample of data will be enough to decide a linguistic hypo-thesis; by contrast, corpora in major research projects can total millions of words. An important principle is that all corpora, whatever their size, are inevitably limited in their coverage, and always need to be supplemented by data derived from the intuitions of native speakers of the language, through either introspection or experimentation.&lt;/p></description></item><item><title>Docs: R4.4.1 How Much Higher? How Much Faster?</title><link>https://note.codiy.net/ielts/r4.4.1</link><pubDate>Tue, 09 Aug 2022 13:11:52 +0800</pubDate><guid>https://note.codiy.net/ielts/r4.4.1</guid><description>
&lt;h3 id="-limits-to-human-sporting-performance-are-not-yet-in-sight-">— Limits to Human Sporting Performance Are Not Yet in Sight —&lt;/h3>
&lt;p>Since the early years of the twentieth century, when the International Athletic Federa-tion began keeping records, there has been a steady improvement in how fast athletes run,how high they jump and how far they are able to hurl massive objects, themselves included, through space. For the so-called power events — that require a relatively brief,explosive release of energy, like the 100-metre sprint and the long jump — times and distances have improved ten to twenty percent. In the endurance events the results have been more dramatic. At the 1908 Olympics, John Hayes of the U.S.team ran a marathon in a time of 2:55:18. In 1999, Morocco&amp;rsquo;s Khalid Khannouchi set a new world record of 2:05:42, almost thirty percent faster.&lt;/p>
&lt;p>No one theory can explain improvements in performance, but the most important fac-tor has been genetics. ‘The athlete must choose his parents carefully,’ says Jesus Dapena,a sports scientist at Indiana University, invoking an oft-cited adage. Over the past century,the composition of the human gene pool has not changed appreciably, but with increasing global participation in athletics — and greater rewards to tempt athletes — it is more likely that individuals possessing the unique complement of genes for athletic perfor-mance can be identified early. ‘Was there someone like [sprinter] Michael Johnson in the 1920s?’ Dapena asks. ‘I’m sure that there was, but his talent was probably never realiz-ed.’&lt;/p>
&lt;p>Identifying genetically talented individuals is only the first step. Michael Yessis, an emeritus professor of Sports Science at California State University at Fullerton, maintains that ‘genetics only determines about one third of what an athlete can do. But with the right training we can go much further with that one third than we’ve been going.’ Yessis believes that U.S. runners, despite their impressive achievements, are ‘running on their genetics’. By applying more scientific methods, ‘they’re going to go much faster’. These methods include strength training that duplicates what they are doing in their running events as well as plyometrics, a technique pioneered in the former Soviet Union.&lt;/p>
&lt;p>Whereas most exercises are designed to build up strength or endurance, plyometrics focuses on increasing power — the rate at which an athlete can expend energy. When a sprinter runs, Yessis explains, her foot stays in contact with the ground for just under a tenth of a second, half of which is devoted to landing and the other half to pushing off.Plyometric exercises help athletes make the best use of this brief interval.&lt;/p>
&lt;p>Nutrition is another area that sports trainers have failed to address adequately. ‘Many athletes are not getting the best nutrition, even through supplements,’ Yessis insists. Each activity has its own nutritional needs. Few coaches, for instance, understand how deficiencies in trace mineral can lead to injuries.&lt;/p>
&lt;p>Focused training will also play a role in enabling records to be broken. ‘If we applied the Russian training model to some of the outstanding runners we have in this country,’Yessis asserts, ‘they would be breaking records left and right.’ He will not predict by how much, however: ‘Exactly what the limits are it’s hard to say, but there will be increase even if only by hundredths of a second, as long as our training continues to improve.’&lt;/p>
&lt;p>One of the most important new methodologies is biomechanics, the study of the body in motion. A biomechanics films an athlete in action and then digitizes her performance,recording the motion of every joint and limb in three dimensions. By applying Newton’s laws to these motions, ‘we can say that this athlete’s run is not fast enough; that this one is not using his arms strongly enough during take-off,’ says Dapena, who uses these methods to help high jumpers. To date, however, biomechan- ics has made only a small difference to athletic performance.&lt;/p>
&lt;p>Revolutionary ideas still come from the athletes themselves. For example, during the 1968 Olympics in Mexico City, a relatively unknown high jumper named Dick Fosbury won the gold by going over the bar backwards, in complete contradiction of all the received high-jumping wisdom, a move instantly dubbed the Fosbury flop. Fosbury him-self did not know what he was doing. That understanding took the later analysis of bio-mechanics, who put their minds to comprehending something that was too complex and unorthodox ever to have been invented through their own mathe- maticcal simula- tions.Fosbury also required another element that lies behind many improvements in athletic performance: an innovation in athletic equipment. In Fosbury’s case, it was the cushions that jumpers land on.Traditionally, high jumpers would land in pits filled with sawdust.But by Fosbury’s time, sawdust pits had been replaced by soft foam cushions, ideal for flopping.&lt;/p>
&lt;p>In the end, most people who examine human performance are humbled by the resourcefulness of athletes and the powers of the human body. ‘Once you study athletics,you learn that it’s a vexingly complex issue,’ says John S. Raglin, a sports psychologist at Indiana University. ‘Core performance is not a simple of mundane thing of higher, faster,longer. So many variables enter into the equation, and our understanding in many cases is fundamental. We’ve got a long way to go.’ For the foreseeable future, records will be made to be broken.&lt;/p></description></item><item><title>Docs: R4.4.2 The Nature and Aims of Archaeology</title><link>https://note.codiy.net/ielts/r4.4.2</link><pubDate>Tue, 09 Aug 2022 13:11:52 +0800</pubDate><guid>https://note.codiy.net/ielts/r4.4.2</guid><description>
&lt;p>Archaeology is partly the discovery of the treasures of the past, partly the careful work of the scientific analyst, partly the exercise of the creative imagination. It is toiling in the sun on an excavation in the Middle-East, it is working with living Inuit in the snows of Alaska, and it is investigating the sewers of Roman Britain. But it is also the painstaking task of interpretation, so that we come to understand what these things mean for the human story. And it is the conservation of the world&amp;rsquo;s cultural heritage against looting and careless harm.&lt;/p>
&lt;p>Archaeology, then, is both a physical activity out in the field, and an intellectual pur-suit in the study or laboratory. That is part of its great attraction. The rich mixture of dan-ger and detective work has also made it the perfect vehicle for fiction writers and film-makers, from Agatha Christie with Murder in Mesopotamia to Stephen Spielberg with Indiana Jones. However far from reality such portrayals are, they capture the essential truth that archaeology is an exciting quest — the quest for knowledge about ourselves and our past.&lt;/p>
&lt;p>But how does archaeology relate to disciplines such as anthropology and history, that are also concerned with the human story? Is archaeology itself a science? And what are the responsibilities of the archaeologist in today&amp;rsquo;s world?&lt;/p>
&lt;p>Anthropology, at its broadest, is the study of humanity — our physical characteristic as animals and our unique non-biological characteristics that we call culture. Culture in this sense includes what the anthropologist, Edward Tylor, summarized in 1871 as‘knowledge, belief, art, morals, custom and any other capabilities and habits acquired by man as a member of society’. Anthropologists also use the term ‘culture’ in a more re-stricted sense when they refer to the ‘culture’ of a particular society, meaning the non-biological characteristics unique to that society, which distinguish it from other societies.Anthropology is thus a broad discipline — so broad that it is generally broken down into three smaller disciplines: physical anthropology, cultural anthropology and archaeology.&lt;/p>
&lt;p>Physical anthropology, or biological anthropology as it is also called, concerns the study of human biological or physical characteristics and how they evolved. Cultural anthropology — or social anthropology — analyses human culture and society. Two of its branches are ethnography (the study at first hand of individual living cultures) and ethnology (which sets out to compare cultures using ethnographic evidence to derive general principles about human society).&lt;/p>
&lt;p>Archaeology is the ‘past tense of culture anthropology’. Whereas cultural anthropolo-gists will often base their conclusions on the experience of living within contemporary communities, archaeologists study past societies primarily through their material remains— the buildings, tools and other artefacts that constitutes what is known as the material culture left over from former societies.&lt;/p>
&lt;p>Nevertheless, one of the most important tasks for the archaeologist today is to know how to interpret material culture in human terms. How were those pots used? Why are some dwellings round and others square? Here the methods of archaeology and ethno-graphy overlap. Archaeologists in recent decades have developed ‘ethnography’, where,like ethnographers, they live among contemporary communities, but with the specific purpose of learning how such societies use material culture — how they make their tools and weapons, why they build their settlements where they do, and so on. Moreover,archaeology has an active role to play in the field of conservation. Heritage studies con-stitutes a developing field, where it is realized that the world’s cultural heritage is a diminishing resource which holds different meanings for different people.&lt;/p>
&lt;p>If, then archaeology deals with the past, in what way does it differ from history? In the broadest sense, just as archaeology is an aspect of anthropology, so too is it a part of history — where we mean the whole history of humankind from its beginnings over three million years ago. Indeed, for more than ninety-nine percent of that huge span of time, archaeology — the study of past material culture — is the only significant source of information. Conventional historical sources begin only with the introduction of writt-en records around 3000 BC in western Asia, and much later in most other parts of the world.&lt;/p>
&lt;p>A commonly drawn distinction is between pre-history, i.e. the period before written records — and history in the narrow sense, meaning the study of the past using written evidence. To archaeology, which studies all cultures and periods, whether with or without writing, the distinction between history and pre-history is a convenient dividing line that recognizes the importance of the written word, but in no way lessens the importance of the useful information contained in oral histories.&lt;/p>
&lt;p>Since the aim of archaeology is the understanding of humankind, it is a humanistic study, and since it deals with the human past, it is a historical discipline. But it differs from the study of written history in a fundamental way. The material the archaeologist finds does not tell us directly what to think. Historical records make statements, offer opinions and pass judgments. The objects the archaeologists discover, on the other hand,tell us nothing directly in themselves. In this respect, the practice of the archaeologist is rather like that of the scientist, who collects data, conducts experiments, formulates a hypothesis, tests the hypothesis against more data, and then, in conclusion, devises a model that seems best to summarize the pattern observed in the data. The archaeologist has to develop a picture of the past, just as the scientist has to develop a coherent view of the natural world.&lt;/p></description></item><item><title>Docs: R4.4.3 The Problem of Scarce Resources</title><link>https://note.codiy.net/ielts/r4.4.3</link><pubDate>Tue, 09 Aug 2022 13:11:52 +0800</pubDate><guid>https://note.codiy.net/ielts/r4.4.3</guid><description>
&lt;h4 id="section-a">Section A&lt;/h4>
&lt;p>The problem of how health-care resources should be allocated or apportioned, so that they are distributed in both the most just and most efficient way, is not a new one. Every health system in an economically developed society is faced with the need to decide(either formally or informally) what proportion of the community’s total resources should be spent on health-care; how resources are to be apportioned; what diseases and disabilities and which forms of treatment are to be given priority; which members of the community are to be given special consideration in respect of their health needs; and which forms of treatment are the most cost-effective.&lt;/p>
&lt;h4 id="section-b">Section B&lt;/h4>
&lt;p>What is new is that, from the 1950s onwards, there have been certain general changes in outlook about the finitude of resources as a whole and of health-care resources in particular, as well as more specific changes regarding the clientele of health-care resources and the cost to the community of those resources. Thus, in the 1950s and 1960s,there emerged an awareness in Western societies that resources for the provision of fossil fuel energy were finite and exhaustible and that the capacity of nature or the environment to sustain economic development and population was also finite. In other words, we became aware of the obvious fact that there were ‘limits to growth’. The new consciousness that there were also severe limits to health-care resources was part of this general revelation of the obvious. Looking back, it now seems quite incredible that in the national health systems that emerged in many counties in the years immediately after the 1939—45 World War, it was assumed without question that all the bachealth needs of any community could be satisfied, at least in principle; the ‘invisible hand’ of economic progress would provide.&lt;/p>
&lt;h4 id="section-c">Section C&lt;/h4>
&lt;p>However, at exactly the same time as this new realization of the finite character of health-care resources was sinking in, an awareness of a contrary kind was developing in Western societies: that people have a basic right to health-care as a necessary condition of a proper human life. Like education, political and legal processes and institutions, public order, communication, transport and money supply, health-care came to be seen as one of the fundamental social facilities necessary for people to exercise their other rights as autonomous human beings. People are not in a position to exercise personal liberty and to be self-determining if they are poverty-stricken, or deprived of basic education, or do not live within a context of law and order. In the same way, basic health-care is a condition of the exercise of autonomy.&lt;/p>
&lt;h4 id="section-d">Section D&lt;/h4>
&lt;p>Although the language of ‘rights’ sometimes leads to confusion, by the late 1970s it was recognized in most societies that people have a right to health-care (though there has been considerable resistance in the United States to the idea that there is a formal right to health-care). It is also accepted that this right generates an obligation or duty for the state to ensure that adequate health-care resources are provided out of the public purse. The state has no obligation to provide a health-care system itself, but to ensure that such a system is provided. Put another way, basic health-care is now recognized as a ‘public good’, rather than a ‘private good’ that one is expected to buy for oneself. As the 1976 declaration of the World Health Organization put it: ‘The enjoyment of the highest attainable standard of health is one of the fundamental rights of every human being without distinction of race, religion, political belief, economic or social condition.’ As has just been remarked, in a liberal society basic health is seen as one of the indispensable conditions for the exercise of personal autonomy.&lt;/p>
&lt;h4 id="section-e">Section E&lt;/h4>
&lt;p>Just at the time when it became obvious that health-care resources could not possibly meet the demands being made upon them, people were demanding that their fundamental right to health-care be satisfied by the state. The second set of more specific changes that have led to the present concern about the distribution of health-care resources stems from the dramatic rise in health costs in most OECD① countries, accompanied by large-scale demographic and social changes which have meant, to take one example, that elderly people are now major (and relatively very expensive) consumers of health-care resources.Thus in OECD countries as a whole, health costs increased from 3.8% of GDP② in 1960 to 7% of GDP in 1980, and it has been predicted that the proportion of health costs to GDP will continue to increase. (In the US the current figure is about 12% of GDP, and in Australia about 7.8% of GDP.)&lt;/p>
&lt;p>As a consequence, during the 1980s a kind of doomsday scenario (analogous to similar doomsday extrapolations about energy needs and fossil fuels or about population increases) was projected by health administrators, economists and politicians. In this scenario, ever-rising health costs were matched against static or declining resources.&lt;/p>
&lt;blockquote>
&lt;p>①. Organization for Economic Cooperation and Development&lt;br>
②. Gross Domestic Product&lt;/p>
&lt;/blockquote></description></item></channel></rss>