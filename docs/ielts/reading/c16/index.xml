<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Notebook – C16</title><link>https://note.codiy.net/docs/ielts/reading/c16/</link><description>Recent content in C16 on Notebook</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://note.codiy.net/docs/ielts/reading/c16/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: R16.1.1 Why we need to protect polar bears</title><link>https://note.codiy.net/ielts/r16.1.1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.codiy.net/ielts/r16.1.1/</guid><description>
&lt;p>Polar bears are being increasingly threatened by the effects of climate change, but their disappearance could have far-reaching consequences. They are uniquely adapted to the extreme conditions of the Arctic Circle, where temperatures can reach -40℃. One reason for this is that they have up to 11 centimetres of fat underneath their skin. Humans with comparative levels of adipose tissue would be considered obese and would be likely to suffer from diabetes and heart disease. Yet the polar bear experiences no such consequences.&lt;/p>
&lt;p>A 2014 study by Shi Ping Liu and colleagues sheds light on this mystery. They compared the genetic structure of polar bears with that of their closest relatives from a warmer climate, the brown bears. This allowed them to determine the genes that have allowed polar bears to survive in one of the toughest environments on Earth. Liu and his colleagues found the polar bears had a gene known as APoB, which reduces levels of low-density lipoproteins(LDLs) - a form of &amp;rsquo; bad&amp;rsquo; cholesterol. In humans, mutations of this gene are associated with increased risk of heart disease. Polar bears may therefore be an important study model to understand heart disease in humans.&lt;/p>
&lt;p>The genome of the polar bear may also provide the solution for another condition, one that particularly affects our older generation: osteoporosis. This is a disease where bones show reduced density, usually caused by insufficient exercise, reduced calcium intake or food starvation. Bone tissue is constantly being remodelled, meaning that bone is added or removed, depending on nutrient availability and the stress that the bone is under. Female polar bears, however, undergo extreme conditions during every pregnancy. Once autumn comes around, these females will dig maternity dens in the snow and will remain there throughout the winter, both before and after the birth of their cubs. This process results in about six months of fasting, where the female bears have to keep themselves and their cubs alive, depleting their own calcium and calorie reserves. Despite this, their bones remain strong and dense.&lt;/p>
&lt;p>Physiologists Alanda Lennox and Allen Goodship found an explanation for this paradox in 2008. They discovered that pregnant bears were able to increase the density of their bones before they started to build their dens. In addition, six months later, when they finally emerged from the den with their cubs, there was no evidence of significant loss of bone density. Hibernating brown bears do not have this capacity and must therefore resort to major bone reformation in the following spring. If the mechanism of bone remodelling in polar bears can be understood, many bedridden humans, and even astronauts, could potentially benefit.&lt;/p>
&lt;p>The medical benefits of the polar bear for humanity certainly have their importance in our conservation efforts, but these should not be the only factors taken into consideration. We tend to want to protect animals we think are intelligent and possess emotions, such as elephants and primates. Bears, on the other hand, seem to be perceived as stupid and in many cases violent. And yet anecdotal evidence from the field challenges those assumptions, suggesting for example that polar bears have good problem-solving abilities. A male bear called GoGo in Tennoji Zoo, Osaka, has even been observed making use of a tool to manipulate his environment. The bear used a tree branch on multiple occasions to dislodge a piece of meat hung out of his reach. Problem-solving ability has also been witnessed in wild polar bears, although not as obviously as with GoGo. A calculated move by a male bear involved running and jumping onto barrels in an attempt to get to a photographer standing on a platform four metres high.&lt;/p>
&lt;p>In other studies, such as one by Alison Ames in 2008, polar bears showed deliberate and focussed manipulation. For example, Ames observed bears putting objects in piles and then knocking them over in what appeared to be a game. The study demonstrates that bears are capable of agile and thought-out behaviours. These examples suggest bears have greater creativity and problem-solving abilities than previously thought.&lt;/p>
&lt;p>As for emotions, while the evidence is once again anecdotal, many bears have been seen to hit out at ice and snow-seemingly out of frustration-when they have just missed out on a kill. Moreover, polar bears can form unusual relationships with other species, including playing with the dogs used to pull sleds in the Arctic. Remarkably, one hand-raised polar bear called Agee has formed a close relationship with her owner Mark Dumas to the point where they even swim together. This is even more astonishing since polar bears are known to actively hunt humans in the wild.&lt;/p>
&lt;p>If climate change were to lead to their extinction, this would mean not only the loss of potential breakthroughs in human medicine, but more importantly, the disappearance of an intelligent, majestic animal.&lt;/p></description></item><item><title>Docs: R16.1.2 The Step Pyramid of Djoser</title><link>https://note.codiy.net/ielts/r16.1.2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.codiy.net/ielts/r16.1.2/</guid><description>
&lt;h4 id="a">A&lt;/h4>
&lt;p>The pyramids are the most famous monuments of ancient Egypt and still hold enormous interest for people in the present day. These grand, impressive tributes to the memory of the Egyptian kings have become linked with the country even though other cultures, such as the Chinese and Mayan, also built pyramids. The evolution of the pyramid form has been written and argued about for centuries. However, there is no question that, as far as Egypt is concerned, it began with one monument to one king designed by one brilliant architect: the Step Pyramid of Djoser at Saqqara.&lt;/p>
&lt;h4 id="b">B&lt;/h4>
&lt;p>Djoser was the first king of the Third Dynasty of Egypt and the first to build in stone. Prior to Djoser&amp;rsquo;s reign, tombs were rectangular monuments made of dried clay brick, which covered underground passages where the deceased person was buried. For reasons which remain unclear, Djoser&amp;rsquo;s main offcial, whose name was Imhotep, conceived of building a taller, more impressive tomb for his king by stacking stone slabs on top of one another, progressively making them smaller, to form the shape now known as the Step Pyramid. Djoser is thought to have reigned for 19 years, but some historians and scholars attribute a much longer time for his rule, owing to the number and size of the monuments he built.&lt;/p>
&lt;h4 id="c">C&lt;/h4>
&lt;p>The Step Pyramid has been thoroughly examined and investigated over the last century, and it is now known that the building process went through many different stages. Historian Marc Van de Mieroop comments on this, writing &amp;rsquo; Much experimentation was involved, which is especially clear in the construction of the pyramid in the center of the complex. It had several plans&amp;hellip; before it became the first Step Pyramid in history, piling six levels on top of one another&amp;hellip; The weight of the enormous mass was a challenge for the builders, who placed the stones at an inward incline in order to prevent the monument breaking up.'&lt;/p>
&lt;h4 id="d">D&lt;/h4>
&lt;p>When finally completed, the Step Pyramid rose 62 meters high and was the tallest structure of its time. The complex in which it was built was the size of a city in ancient Egypt and included a temple, courtyards, shrines, and living quarters for the priests. It covered a region of 16 hectares and was surrounded by a wall 10.5 meters high. The wall had 13 false doors cut into it with only one true entrance cut into the south-east corner; the entire wall was then ringed by a trench 750 meters long and 40 meters wide. The false doors and the trench were incorporated into the complex to discourage unwanted visitors. If someone wished to enter. he or she would have needed to know in advance how to find the location of the true opening in the wall. Djoser was so proud of his accomplishment that he broke the tradition of having only his own name on the monument and had Imhotep&amp;rsquo;s name carved on it as well.&lt;/p>
&lt;h4 id="e">E&lt;/h4>
&lt;p>The burial chamber of the tomb, where the king&amp;rsquo;s body was laid to rest, was dug beneath the base of the pyramid, surrounded by a vast maze of long tunnels that had rooms off them to discourage robbers. One of the most mysterious discoveries found inside the pyramid was a large number of stone vessels. Over 40,000 of these vessels, of various forms and shapes, were discovered in storerooms off the pyramid&amp;rsquo;s underground passages. They are inscribed with the names of rulers from the First and Second Dynasties of Egypt and made from different kinds of stone. There is no agreement among scholars and archaeologists on why the vessels were placed in the tomb of Djoser or what they were supposed to represent. The archaeologist Jean-Philippe Lauer, who excavated most of the pyramid and complex, believes they were originally stored and then given a&amp;rsquo; proper burial&amp;rsquo; by Djoser in his pyramid to honor his predecessors. There are other historians, however, who claim the vessels were dumped into the shafts as yet another attempt to prevent grave robbers from getting to the king&amp;rsquo;s burial chamber.&lt;/p>
&lt;h4 id="f">F&lt;/h4>
&lt;p>Unfortunately, all of the precautions and intricate design of the underground network did not prevent ancient robbers from finding a way in. Djoser&amp;rsquo;s grave goods, and even his body, were stolen at some point in the past and all archaeologists found were a small number of his valuables overlooked by the thieves. There was enough left throughout the pyramid and its complex, however, to astonish and amaze the archaeologists who excavated it.&lt;/p>
&lt;h4 id="g">G&lt;/h4>
&lt;p>Egyptologist Miroslav Verner writes,&amp;rsquo; Few monuments hold a place in human history as significant as that of the Step Pyramid in Saqqara&amp;hellip; It can be said without exaggeration that this pyramid complex constitutes a milestone in the evolution of monumental stone architecture in Egypt and in the world as a whole.&amp;rsquo; The Step Pyramid was a revolutionary advance in architecture and became the archetype which all the other great pyramid builders of Egypt would follow.&lt;/p></description></item><item><title>Docs: R16.1.3 The future of work</title><link>https://note.codiy.net/ielts/r16.1.3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.codiy.net/ielts/r16.1.3/</guid><description>
&lt;p>According to a leading business consultancy, 3-14% of the global workforce will need to switch to a different occupation within the next 10-15 years, and all workers will need to adapt as their occupations evolve alongside increasingly capable machines. Automation-or ‘embodied artificial intelligence’ (AI) - is one aspect of the disruptive effects of technology on the labour market. ‘Disembodied AI’, like the algorithms running in our smartphones, is another.&lt;/p>
&lt;p>Dr Stella Pachidi from Cambridge Judge Business School believes that some of the most fundamental changes are happening as a result of the &amp;lsquo;algorithmication&amp;rsquo; of jobs that are dependent on data rather than on production-the so-called knowledge economy. Algorithms are capable of learning from data to undertake tasks that previously needed human judgement, such as reading legal contracts, analysing medical scans and gathering market intelligence.&lt;/p>
&lt;p>&amp;lsquo;In many cases, they can outperform humans,&amp;rsquo; says Pachidi. ‘Organisations are attracted to using algorithms because they want to make choices based on what they consider is &amp;ldquo;perfect information&amp;rdquo;, as well as to reduce costs and enhance productivity.’&lt;/p>
&lt;p>&amp;lsquo;But these enhancements are not without consequences,&amp;rsquo; says Pachidi. &amp;lsquo;If routine cognitive tasks are taken over by AI, how do professions develop their future experts?’ she asks.&amp;rsquo; One way of learning about a job is &amp;ldquo;legitimate peripheral participation&amp;rdquo; - a novice stands next to experts and learns by observation. If this isn&amp;rsquo;t happening, then you need to find new ways to learn.&amp;rsquo;&lt;/p>
&lt;p>Another issue is the extent to which the technology influences or even controls the workforce. For over two years, Pachidi monitored a telecommunications company.&amp;rsquo; The way telecoms salespeople work is through personal and frequent contact with clients, using the benefit of experience to assess a situation and reach a decision. However, the company had started using a[n]&amp;hellip; algorithm that defined when account managers should contact certain customers about which kinds of campaigns and what to offer them.&amp;rsquo;&lt;/p>
&lt;p>The algorithm - usually built by external designers - often becomes the keeper of knowledge, she explains. In cases like this, Pachidi believes, a short-sighted view begins to creep into working practices whereby workers learn through the &amp;lsquo;algorithm&amp;rsquo;s eyes&amp;rsquo; and become dependent on its instructions. Alternative explorations - where experimentation and human instinct lead to progress and new ideas - are effectively discouraged.&lt;/p>
&lt;p>Pachidi and colleagues even observed people developing strategies to make the algorithm work to their own advantage.&amp;rsquo; We are seeing cases where workers feed the algorithm with false data to reach their targets,&amp;rsquo; she reports.&lt;/p>
&lt;p>It&amp;rsquo;s scenarios like these that many researchers are working to avoid. Their objective is to make AI technologies more trustworthy and transparent, so that organisations and individuals understand how AI decisions are made. In the meantime, says Pachidi, ‘We need to make sure we fully understand the dilemmas that this new world raises regarding expertise, occupational boundaries and control.’&lt;/p>
&lt;p>Economist Professor Hamish Low believes that the future of work will involve major transitions across the whole life course for everyone: ‘The traditional trajectory of full-time education followed by full-time work followed by a pensioned retirement is a thing of the past,’ says Low. Instead, he envisages a multistage employment life: one where retraining happens across the life course, and where multiple jobs and no job happen by choice at different stages.&lt;/p>
&lt;p>On the subject of job losses, Low believes the predictions are founded on a fallacy: ‘It assumes that the number of jobs is fixed. If in 30 years, half of 100 jobs are being carried out by robots, that doesn&amp;rsquo;t mean we are left with just 50 jobs for humans. The number of jobs will increase: we would expect there to be 150 jobs.’&lt;/p>
&lt;p>Dr Ewan McGaughey, at Cambridge&amp;rsquo;s Centre for Business Research and King&amp;rsquo;s College London, agrees that&amp;rsquo; apocalyptic&amp;rsquo; views about the future of work are misguided. ‘It&amp;rsquo;s the laws that restrict the supply of capital to the job market, not the advent of new technologies that causes unemployment.’&lt;/p>
&lt;p>His recently published research answers the question of whether automation, AI and robotics will mean a &amp;lsquo;jobless future&amp;rsquo; by looking at the causes of unemployment. &amp;lsquo;History is clear that change can mean redundancies. But social policies can tackle this through retraining and redeployment.&amp;rsquo;&lt;/p>
&lt;p>He adds: &amp;lsquo;If there is going to be change to jobs as a result of AI and robotics then I&amp;rsquo;d like to see governments seizing the opportunity to improve policy to enforce good job security. We can &amp;ldquo;reprogramme&amp;rdquo; the law to prepare for a fairer future of work and leisure.&amp;rsquo; McGaughey&amp;rsquo;s findings are a call to arms to leaders of organisations, governments and banks to pre-empt the coming changes with bold new policies that guarantee full employment, fair incomes and a thriving economic democracy.&lt;/p>
&lt;p>&amp;lsquo;The promises of these new technologies are astounding. They deliver humankind the capacity to live in a way that nobody could have once imagined,&amp;rsquo; he adds. &amp;lsquo;Just as the industrial revolution brought people past subsistence agriculture, and the corporate revolution enabled mass production, a third revolution has been pronounced. But it will not only be one of technology. The next revolution will be social.&amp;rsquo;&lt;/p></description></item><item><title>Docs: R16.2.1 The White Horse of Uffington</title><link>https://note.codiy.net/ielts/r16.2.1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.codiy.net/ielts/r16.2.1/</guid><description>
&lt;p>The cutting of huge figures or &amp;lsquo;geoglyphs&amp;rsquo; into the earth of English hillsides has taken place for more than 3,000 years. There are 56 hill figures scattered around England, with the vast majority on the chalk downlands of the country&amp;rsquo;s southern counties. The figures include giants, horses, crosses and regimental badges. Although the majority of these geoglyphs date within the last 300 years or so, there are one or two that are much older.&lt;/p>
&lt;p>The most famous of these figures is perhaps also the most mysterious - the Uffington White Horse in Oxfordshire. The White Horse has recently been re-dated and shown to be even older than its previously assigned ancient pre-Roman lron Age* date. More controversial is the date of the enigmatic Long Man of Wilmington in Sussex. While many historians are convinced the figure is prehistoric, others believe that it was the work of an artistic monk from a nearby priory and was created between the 11th and 15th centuries.&lt;/p>
&lt;p>The method of cutting these huge figures was simply to remove the overlying grass to reveal the gleaming white chalk below. However, the grass would soon grow over the geoglyph again unless it was regularly cleaned or scoured by a fairly large team of people. One reason that the vast majority of hill figures have disappeared is that when the traditions associated with the figures faded, people no longer bothered or remembered to clear away the grass to expose the chalk outline. Furthermore, over hundreds of years the outlines would sometimes change due to people not always cutting in exactly the same place, thus creating a different shape to the original geoglyph. The fact that any ancient hill figures survive at all in England today is testament to the strength and continuity of local customs and beliefs which, in one case at least, must stretch back over millennia.&lt;/p>
&lt;p>The Uffington White Horse is a unique, stylised representation of a horse consisting of a long, sleek back, thin disjointed legs, a streaming tail, and a bird-like beaked head. The elegant creature almost melts into the landscape. The horse is situated 2.5 km from Uffington village on a steep slope close to the Late Bronze Age* (c.7th century BCE) hillfort of Uffington Castle and below the Ridgeway, a long-distance Neolithic** track.&lt;/p>
&lt;p>The Uffington Horse is also surrounded by Bronze Age burial mounds. It is not far from the Bronze Age cemetery of Lambourn Seven Barrows, which consists of more than 30 well-preserved burial mounds. The carving has been placed in such a way as to make it extremely difficult to see from close quarters, and like many geoglyphs is best appreciated from the air. Nevertheless, there are certain areas of the Vale of the White Horse, the valley containing and named after the enigmatic creature, from which an adequate impression may be gained. Indeed on a clear day the carving can be seen from up to 30 km away.&lt;/p>
&lt;p>The earliest evidence of a horse at Uffington is from the 1070s CE when &amp;ldquo;White Horse Hill&amp;rsquo; is mentioned in documents from the nearby Abbey of Abingdon, and the first reference to the horse itself is soon after, in 1190 CE. However, the carving is believed to date back much further than that. Due to the similarity of the Uffington White Horse to the stylised depictions of horses on 1st century BCE coins, it had been thought that the creature must also date to that period.&lt;/p>
&lt;p>However, in 1995 Optically Stimulated Luminescence(OSL) testing was carried out by the Oxford Archaeological Unit on soil from two of the lower layers of the horse&amp;rsquo;s body, and from another cut near the base. The result was a date for the horse&amp;rsquo;s construction somewhere between 1400 and 600 BCE - in other words, it had a Late Bronze Age or Early lron Age origin.&lt;/p>
&lt;p>The latter end of this date range would tie the carving of the horse in with occupation of the nearby Uffington hillfort, indicating that it may represent a tribal emblem marking the land of the inhabitants of the hillfort. Alternatively, the carving may have been carried out during a Bronze or lron Age ritual. Some researchers see the horse as representing the Celtic*** horse goddess Epona, who was worshipped as a protector of horses, and for her associations with fertility. However, the cult of Epona was not imported from Gaul(France) until around the first century CE. This date is at least six centuries after the Uffington Horse was probably carved. Nevertheless, the horse had great ritual and economic significance during the Bronze and Iron Ages, as attested by its depictions on jewellery and other metal objects. It is possible that the carving represents a goddess in native mythology, such as Rhiannon, described in later Welsh mythology as a beautiful woman dressed in gold and riding a white horse.&lt;/p>
&lt;p>The fact that geoglyphs can disappear easily, along with their associated rituals and meaning, indicates that they were never intended to be anything more than temporary gestures. But this does not lessen their importance. These giant carvings are a fascinating glimpse into the minds of their creators and how they viewed the landscape in which they lived.&lt;/p>
&lt;p>&lt;code>*Iron Age: a period (in Britain 800 BCE - 43 CE) that is characterised by the use of iron tools&lt;/code>&lt;/p>
&lt;p>&lt;code>*Bronze Age: a period (in Britain c. 2500 BCE - 800 BCE) that is characterised by the development of bronze tools&lt;/code>&lt;/p>
&lt;p>&lt;code>**Neolithic: a period (in Britain c. 4000BCE - c. 2500BCE) that is significant for the spread of agricultural practices, and the use of stone tools&lt;/code>&lt;/p>
&lt;p>&lt;code>***Celtic: an ancient people who migrated from Europe to Britain before the Romans&lt;/code>&lt;/p></description></item><item><title>Docs: R16.2.2 I contain multitudes</title><link>https://note.codiy.net/ielts/r16.2.2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.codiy.net/ielts/r16.2.2/</guid><description>
&lt;blockquote>
&lt;p>&lt;strong>Wendy Moore reviews Ed Yong&amp;rsquo;s book about microbes&lt;/strong>&lt;/p>
&lt;/blockquote>
&lt;p>Microbes, most of them bacteria, have populated this planet since long before animal life developed and they will outlive us. Invisible to the naked eye, they are ubiquitous. They inhabit the soil, air, rocks and water and are present within every form of life, from seaweed and coral to dogs and humans. And, as Yong explains in his utterly absorbing and hugely important book, we mess with them at our peril.&lt;/p>
&lt;p>Every species has its own colony of microbes, called a &amp;lsquo;microbiome&amp;rsquo;, and these microbes vary not only between species but also between individuals and within different parts of each individual. What is amazing is that while the number of human cells in the average person is about 30 trillion, the number of microbial ones is higher - about 39 trillion. At best, Yong informs us, we are only 50 per cent human. Indeed, some scientists even suggest we should think of each species and its microbes as a single unit, dubbed a &amp;rsquo; holobiont&amp;rsquo;.&lt;/p>
&lt;p>In each human there are microbes that live only in the stomach, the mouth or the armpit and by and large they do so peacefully. So &amp;lsquo;bad&amp;rsquo; microbes are just microbes out of context. Microbes that sit contentedly in the human gut(where there are more microbes than there are stars in the galaxy) can become deadly if they find their way into the bloodstream. These communities are constantly changing too. The right hand shares just one sixth of its microbes with the left hand. And, of course, we are surrounded by microbes. Every time we eat, we swallow a million microbes in each gram of food; we are continually swapping microbes with other humans, pets and the world at large.&lt;/p>
&lt;p>It&amp;rsquo;s a fascinating topic and Yong, a young British science journalist, is an extraordinarily adept guide. Writing with lightness and panache, he has a knack of explaining complex science in terms that are both easy to understand and totally enthralling. Yong is on a mission. Leading us gently by the hand, he takes us into the world of microbes - a bizarre, alien planet - in a bid to persuade us to love them as much as he does. By the end, we do.&lt;/p>
&lt;p>For most of human history we had no idea that microbes existed. The first man to see these extraordinarily potent creatures was a Dutch lens-maker called Antony van Leeuwenhoek in the 1670s. Using microscopes of his own design that could magnify up to 270 times, he examined a drop of water from a nearby lake and found it teeming with tiny creatures he called &amp;lsquo;animalcules&amp;rsquo;. It wasn&amp;rsquo;t until nearly two hundred years later that the research of French biologist Louis Pasteur indicated that some microbes caused disease. It was Pasteur&amp;rsquo;s ‘germ theory’ that gave bacteria the poor image that endures today.&lt;/p>
&lt;p>Yong&amp;rsquo;s book is in many ways a plea for microbial tolerance, pointing out that while fewer than one hundred species of bacteria bring disease, many thousands more play a vital role in maintaining our health. The book also acknowledges that our attitude towards bacteria is not a simple one. We tend to see the dangers posed by bacteria, yet at the same time we are sold yoghurts and drinks that supposedly nurture &amp;lsquo;friendly&amp;rsquo; bacteria. In reality, says Yong, bacteria should not be viewed as either friends or foes, villains or heroes. Instead we should realise we have a symbiotic relationship, that can be mutually beneficial or mutually destructive.&lt;/p>
&lt;p>What then do these millions of organisms do? The answer is pretty much everything. New research is now unravelling the ways in which bacteria aid digestion, regulate our immune systems, eliminate toxins, produce vitamins, affect our behaviour and even combat obesity. &amp;lsquo;They actually help us become who we are,&amp;rsquo; says Yong. But we are facing a growing problem. Our obsession with hygiene, our overuse of antibiotics and our unhealthy, low-fibre diets are disrupting the bacterial balance and may be responsible for soaring rates of allergies and immune problems, such as inflammatory bowel disease(IBD).&lt;/p>
&lt;p>The most recent research actually turns accepted norms upside down. For example, there are studies indicating that the excessive use of household detergents and antibacterial products actually destroys the microbes that normally keep the more dangerous germs at bay. Other studies show that keeping a dog as a pet gives children early exposure to a diverse range of bacteria, which may help protect them against allergies later.&lt;/p>
&lt;p>The readers of Yong&amp;rsquo;s book must be prepared for a decidedly unglamorous world. Among the less appealing case studies is one about a fungus that is wiping out entire populations of frogs and that can be halted by a rare microbial bacterium. Another is about squid that carry luminescent bacteria that protect them against predators. However, if you can overcome your distaste for some of the investigations, the reasons for Yong&amp;rsquo;s enthusiasm become clear. The microbial world is a place of wonder. Already, in an attempt to stop mosquitoes spreading dengue fever-a disease that infects 400 million people a year-mosquitoes are being loaded with a bacterium to block the disease. In the future, our ability to manipulate microbes means we could construct buildings with useful microbes built into their walls to fight off infections. Just imagine a neonatal hospital ward coated in a specially mixed cocktail of microbes so that babies get the best start in life.&lt;/p></description></item><item><title>Docs: R16.2.3 How to make wise decisions</title><link>https://note.codiy.net/ielts/r16.2.3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.codiy.net/ielts/r16.2.3/</guid><description>
&lt;p>Across cultures, wisdom has been considered one of the most revered human qualities. Although the truly wise may seem few and far between, empirical research examining wisdom suggests that it isn&amp;rsquo;t an exceptional trait possessed by a small handful of bearded philosophers after all - in fact, the latest studies suggest that most of us have the ability to make wise decisions, given the right context.&lt;/p>
&lt;p>&amp;lsquo;It appears that experiential, situational, and cultural factors are even more powerful in shaping wisdom than previously imagined,&amp;rsquo; says Associate Professor Igor Grossmann of the University of Waterloo in Ontario, Canada. &amp;lsquo;Recent empirical findings from cognitive, developmental, social, and personality psychology cumulatively suggest that people&amp;rsquo;s ability to reason wisely varies dramatically across experiential and situational contexts. Understanding the role of such contextual factors offers unique insights into understanding wisdom in daily life, as well as how it can be enhanced and taught.&amp;rsquo;&lt;/p>
&lt;p>It seems that it&amp;rsquo;s not so much that some people simply possess wisdom and others lack it, but that our ability to reason wisely depends on a variety of external factors. &amp;lsquo;It is impossible to characterize thought processes attributed to wisdom without considering the role of contextual factors,&amp;rsquo; explains Grossmann. &amp;lsquo;In other words, wisdom is not solely an &amp;ldquo;inner quality&amp;rdquo; but rather unfolds as a function of situations people happen to be in. Some situations are more likely to promote wisdom than others.&amp;rsquo;&lt;/p>
&lt;p>Coming up with a definition of wisdom is challenging, but Grossmann and his colleagues have identified four key characteristics as part of a framework of wise reasoning. One is intellectual humility or recognition of the limits of our own knowledge, and another is appreciation of perspectives wider than the issue at hand. Sensitivity to the possibility of change in social relations is also key, along with compromise or integration of different attitudes and beliefs.&lt;/p>
&lt;p>Grossmann and his colleagues have also found that one of the most reliable ways to support wisdom in our own day-to-day decisions is to look at scenarios from a third-party perspective, as though giving advice to a friend. Research suggests that when adopting a first-person viewpoint we focus on &amp;rsquo;the focal features of the environment&amp;rsquo; and when we adopt a third-person,&amp;rsquo; observer viewpoint we reason more broadly and focus more on interpersonal and moral ideals such as justice and impartiality. Looking at problems from this more expansive viewpoint appears to foster cognitive processes related to wise decisions.&lt;/p>
&lt;p>What are we to do, then, when confronted with situations like a disagreement with a spouse or negotiating a contract at work, that require us to take a personal stake? Grossmann argues that even when we aren&amp;rsquo;t able to change the situation, we can still evaluate these experiences from different perspectives.&lt;/p>
&lt;p>For example, in one experiment that took place during the peak of a recent economic recession, graduating college seniors were asked to reflect on their job prospects. The students were instructed to imagine their career either &amp;lsquo;as if you were a distant observer&amp;rsquo; or &amp;lsquo;before your own eyes as if you were right there&amp;rsquo;. Participants in the group assigned to the &amp;lsquo;distant observer&amp;rsquo; role displayed more wisdom-related reasoning(intellectual humility and recognition of change) than did participants in the control group.&lt;/p>
&lt;p>In another study, couples in long-term romantic relationships were instructed to visualize an unresolved relationship conflict either through the eyes of an outsider or from their own perspective. Participants then discussed the incident with their partner for 10 minutes, after which they wrote down their thoughts about it. Couples in the &amp;lsquo;other&amp;rsquo;s eyes&amp;rsquo; condition were significantly more likely to rely on wise reasoning-recognizing others&amp;rsquo; perspectives and searching for a compromise-compared to the couples in the egocentric condition.&lt;/p>
&lt;p>&amp;lsquo;Ego-decentering promotes greater focus on others and enables a bigger picture, conceptual view of the experience, affording recognition of intellectual humility and change,&amp;rsquo; says Grossmann.&lt;/p>
&lt;p>We might associate wisdom with intelligence or particular personality traits, but research shows only a small positive relationship between wise thinking and crystallized intelligence and the personality traits of openness and agreeableness. &amp;lsquo;It is remarkable how much people can vary in their wisdom from one situation to the next, and how much stronger such contextual effects are for understanding the relationship between wise judgment and its social and affective outcomes as compared to the generalized &amp;ldquo;traits&amp;rdquo;,&amp;rsquo; Grossmann explains.&amp;rsquo; That is, knowing how wisely a person behaves in a given situation is more informative for understanding their emotions or likelihood to forgive [or] retaliate as compared to knowing whether the person may be wise &amp;ldquo;in general&amp;rdquo;.&amp;rsquo;&lt;/p></description></item><item><title>Docs: R16.3.1 Roman shipbuilding and navigation</title><link>https://note.codiy.net/ielts/r16.3.1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.codiy.net/ielts/r16.3.1/</guid><description>
&lt;p>Shipbuilding today is based on science and ships are built using computers and sophisticated tools. Shipbuilding in ancient Rome, however, was more of an art relying on estimation, inherited techniques and personal experience. The Romans were not traditionally sailors but mostly land-based people, who learned to build ships from the people that they conquered, namely the Greeks and the Egyptians.&lt;/p>
&lt;p>There are a few surviving written documents that give descriptions and representations of ancient Roman ships, including the sails and rigging. Excavated vessels also provide some clues about ancient shipbuilding techniques. Studies of these have taught us that ancient Roman shipbuilders built the outer hull first, then proceeded with the frame and the rest of the ship. Planks used to build the outer hull were initially sewn together. Starting from the 6th century BCE, they were fixed using a method called mortise and tenon, whereby one plank locked into another without the need for stitching. Then in the first centuries of the current era, Mediterranean shipbuilders shifted to another shipbuilding method, still in use today, which consisted of building the frame first and then proceeding with the hull and the other components of the ship. This method was more systematic and dramatically shortened ship construction times. The ancient Romans built large merchant ships and warships whose size and technology were unequalled until the 16th century CE.&lt;/p>
&lt;p>Warships were built to be lightweight and very speedy. They had to be able to sail near the coast, which is why they had no ballast or excess load and were built with a long, narrow hull. They did not sink when damaged and often would lie crippled on the sea&amp;rsquo;s surface following naval battles. They had a bronze battering ram, which was used to pierce the timber hulls or break the oars of enemy vessels. Warships used both wind(sails) and human power(oarsmen) and were therefore very fast. Eventually, Rome&amp;rsquo;s navy became the largest and most powerful in the Mediterranean, and the Romans had control over what they therefore called Mare Nostrum meaning &amp;lsquo;our sea&amp;rsquo;.&lt;/p>
&lt;p>There were many kinds of warship. The &amp;rsquo;trireme&amp;rsquo; was the dominant warship from the 7th to 4th century BCE. It had rowers in the top, middle and lower levels, and approximately 50 rowers in each bank. The rowers at the bottom had the most uncomfortable position as they were under the other rowers and were exposed to the water entering through the oar-holes. It is worth noting that contrary to popular perception, rowers were not slaves but mostly Roman citizens enrolled in the military. The trireme was superseded by larger ships with even more rowers.&lt;/p>
&lt;p>Merchant ships were built to transport lots of cargo over long distances and at a reasonable cost. They had a wider hull, double planking and a solid interior for added stability. Unlike warships, their V-shaped hull was deep underwater, meaning that they could not sail too close to the coast. They usually had two huge side rudders located off the stern and controlled by a small tiller bar connected to a system of cables. They had from one to three masts with large square sails and a small triangular sail at the bow. Just like warships, merchant ships used oarsmen, but coordinating the hundreds of rowers in both types of ship was not an easy task. In order to assist them, music would be played on an instrument, and oars would then keep time with this.&lt;/p>
&lt;p>The cargo on merchant ships included raw materials(e.g. iron bars, copper, marble and granite), and agricultural products(e.g. grain from Egypt&amp;rsquo;s Nile valley). During the Empire, Rome was a huge city by ancient standards of about one million inhabitants. Goods from all over the world would come to the city through the port of Pozzuoli situated west of the bay of Naples in Italy and through the gigantic port of Ostia situated at the mouth of the Tiber River. Large merchant ships would approach the destination port and, just like today, be intercepted by a number of towboats that would drag them to the quay.&lt;/p>
&lt;p>The time of travel along the many sailing routes could vary widely. Navigation in ancient Rome did not rely on sophisticated instruments such as compasses but on experience, local knowledge and observation of natural phenomena. In conditions of good visibility, seamen in the Mediterranean often had the mainland or islands in sight, which greatly facilitated navigation. They sailed by noting their position relative to a succession of recognisable landmarks. When weather conditions were not good or where land was no longer visible, Roman mariners estimated directions from the pole star or, with less accuracy, from the Sun at noon. They also estimated directions relative to the wind and swell. Overall, shipping in ancient Roman times resembled shipping today with large vessels regularly crossing the seas and bringing supplies from their Empire.&lt;/p></description></item><item><title>Docs: R16.3.2 Climate change reveals ancient artefacts in Norway's glaciers</title><link>https://note.codiy.net/ielts/r16.3.2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.codiy.net/ielts/r16.3.2/</guid><description>
&lt;h4 id="a">A&lt;/h4>
&lt;p>Well above the treeline in Norway&amp;rsquo;s highest mountains, ancient fields of ice are shrinking as Earth&amp;rsquo;s climate warms. As the ice has vanished, it has been giving up the treasures it has preserved in cold storage for the last 6,000 years - items such as ancient arrows and skis from Viking Age* traders. And those artefacts have provided archaeologists with some surprising insights into how ancient Norwegians made their livings.&lt;/p>
&lt;h4 id="b">B&lt;/h4>
&lt;p>Organic materials like textiles and hides are relatively rare finds at archaeological sites. This is because unless they&amp;rsquo; re protected from the microorganisms that cause decay, they tend not to last long. Extreme cold is one reliable way to keep artefacts relatively fresh for a few thousand years, but once thawed out, these materials experience degradation relatively swiftly.&lt;/p>
&lt;p>With climate change shrinking ice cover around the world, glacial archaeologists need to race the clock to find newly revealed artefacts, preserve them, and study them. If something fragile dries and is windblown it might very soon be lost to science, or an arrow might be exposed and then covered again by the next snow and remain well-preserved. The unpredictability means that glacial archaeologists have to be systematic in their approach to fieldwork.&lt;/p>
&lt;h4 id="c">C&lt;/h4>
&lt;p>Over a nine-year period, a team of archaeologists, which included Lars Pile of Oppland County Council, Norway, and James Barrett of the McDonald Institute for Archaeological Research, surveyed patches of ice in Oppland, an area of south-central Norway that is home to some of the country&amp;rsquo;s highest mountains. Reindeer once congregated on these icy patches in the later summer months to escape biting insects, and from the late Stone Age**, hunters followed. In addition, trade routes threaded through the mountain passes of Oppland, linking settlements in Norway to the rest of Europe.&lt;/p>
&lt;p>The slow but steady movement of glaciers tends to destroy anything at their bases, so the team focused on stationary patches of ice, mostly above 1,400 metres. That ice is found amid fields of frost-weathered boulders, fallen rocks, and exposed bedrock that for nine months of the year is buried beneath snow.&lt;/p>
&lt;p>&amp;lsquo;Fieldwork is hard work - hiking with all our equipment, often camping on permafrost-but very rewarding. You&amp;rsquo; re rescuing the archaeology, bringing the melting ice to wider attention, discovering a unique environmental history and really connecting with the natural environment,&amp;rsquo; says Barrett.&lt;/p>
&lt;h4 id="d">D&lt;/h4>
&lt;p>At the edges of the contracting ice patches, archaeologists found more than 2,000 artefacts, which formed a material record that ran from 4,000 BCE to the beginnings of the Renaissance in the 14th century. Many of the artefacts are associated with hunting. Hunters would have easily misplaced arrows and they often discarded broken bows rather than take them all the way home. Other items could have been used by hunters traversing the high mountain passes of Oppland: all-purpose items like tools, skis, and horse tack.&lt;/p>
&lt;h4 id="e">E&lt;/h4>
&lt;p>Barrett&amp;rsquo;s team radiocarbon-dated 153 of the artefacts and compared those dates to the timing of major environmental changes in the region-such as periods of cooling or warming-and major social and economic shifts-such as the growth of farming settlements and the spread of international trade networks leading up to the Viking Age. They found that some periods had produced lots of artefacts, which indicates that people had been pretty active in the mountains during those times. But there were few or no signs of activity during other periods.&lt;/p>
&lt;h4 id="f">F&lt;/h4>
&lt;p>What was surprising, according to Barrett, was the timing of these periods. Oppland&amp;rsquo;s mountains present daunting terrain and in periods of extreme cold, glaciers could block the higher mountain passes and make travel in the upper reaches of the mountains extremely difficult. Archaeologists assumed people would stick to lower elevations during a time like the Late Antique Little Ice Age,a short period of deeper-than-usual cold from about 536-600 CE. But it turned out that hunters kept regularly venturing into the mountains even when the climate turned cold, based on the amount of stuff they had apparently dropped there.&lt;/p>
&lt;p>&amp;lsquo;Remarkably, though, the finds from the ice may have continued through this period, perhaps suggesting that the importance of mountain hunting increased to supplement failing agricultural harvests in times of low temperatures,&amp;rsquo; says Barrett. A colder turn in the Scandinavian climate would likely have meant widespread crop failures, so more people would have depended on hunting to make up for those losses.&lt;/p>
&lt;h4 id="g">G&lt;/h4>
&lt;p>Many of the artefacts Barrett&amp;rsquo;s team recovered date from the beginning of the Viking Age, the 700s through to the 900s CE. Trade networks connecting Scandinavia with Europe and the Middle East were expanding around this time. Although we usually think of ships when we think of Scandinavian expansion, these recent discoveries show that plenty of goods travelled on overland routes, like the mountain passes of Oppland. And growing Norwegian towns, along with export markets, would have created a booming demand for hides to fight off the cold, as well as antlers to make useful things like combs. Business must have been good for hunters.&lt;/p>
&lt;h4 id="h">H&lt;/h4>
&lt;p>Norway&amp;rsquo;s mountains are probably still hiding a lot of history -and prehistory-in remote ice patches. When Barrett&amp;rsquo;s team looked at the dates for their sample of 153 artefacts, they noticed a gap with almost no artefacts from about 3,800 to 2,200 BCE. In fact, archaeological finds from that period are rare all over Norway. The researchers say that could be because many of those artefacts have already disintegrated or are still frozen in the ice. That means archaeologists could be extracting some of those artefacts from retreating ice in years to come.&lt;/p>
&lt;p>&lt;code>*Viking Age: a period of European history from around 700 CE to around 1050 CE when Scandinavian Vikings migrated throughout Europe by means of trade and warfare&lt;/code>&lt;/p>
&lt;p>&lt;code>**The Stone Age: a period in early history that began about 3.4 million years ago&lt;/code>&lt;/p></description></item><item><title>Docs: R16.3.3 Plant' thermometer' triggers springtime growth by measuring night-time heat</title><link>https://note.codiy.net/ielts/r16.3.3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.codiy.net/ielts/r16.3.3/</guid><description>
&lt;blockquote>
&lt;p>&lt;strong>A photoreceptor molecule in plant cells has been found to have a second job as a thermometer after dark - allowing plants to read seasonal temperature changes. Scientists say the discovery could help breed crops that are more resilient to the temperatures expected to result from climate change&lt;/strong>&lt;/p>
&lt;/blockquote>
&lt;h4 id="a">A&lt;/h4>
&lt;p>An international team of scientists led by the University of Cambridge has discovered that the &amp;rsquo;thermometer&amp;rsquo; molecule in plants enables them to develop according to seasonal temperature changes. Researchers have revealed that molecules called phytochromes - used by plants to detect light during the day-actually change their function in darkness to become cellular temperature gauges that measure the heat of the night.&lt;/p>
&lt;p>The new findings, published in the journal Science, show that phytochromes control genetic switches in response to temperature as well as light to dictate plant development.&lt;/p>
&lt;h4 id="b">B&lt;/h4>
&lt;p>At night, these molecules change states, and the pace at which they change is &amp;lsquo;directly proportional to temperature&amp;rsquo;, say scientists, who compare phytochromes to mercury in a thermometer. The warmer it is, the faster the molecular change-stimulating plant growth.&lt;/p>
&lt;h4 id="c">C&lt;/h4>
&lt;p>Farmers and gardeners have known for hundreds of years how responsive plants are to temperature: warm winters cause many trees and flowers to bud early, something humans have long used to predict weather and harvest times for the coming year. The latest research pinpoints for the first time a molecular mechanism in plants that reacts to temperature-often triggering the buds of spring we long to see at the end of winter.&lt;/p>
&lt;h4 id="d">D&lt;/h4>
&lt;p>With weather and temperatures set to become ever more unpredictable due to climate change, researchers say the discovery that this light-sensing molecule also functions as the internal thermometer in plant cells could help us breed tougher crops.&amp;rsquo; It is estimated that agricultural yields will need to double by 2050, but climate change is a major threat to achieving this. Key crops such as wheat and rice are sensitive to high temperatures. Thermal stress reduces crop yields by around 10% for every one degree increase in temperature,&amp;rsquo; says lead researcher Dr Philip Wigge from Cambridge&amp;rsquo;s Sainsbury Laboratory. &amp;lsquo;Discovering the molecules that allow plants to sense temperature has the potential to accelerate the breeding of crops resilient to thermal stress and climate change.&amp;rsquo;&lt;/p>
&lt;h4 id="e">E&lt;/h4>
&lt;p>In their active state, phytochrome molecules bind themselves to DNA to restrict plant growth. During the day, sunlight activates the molecules, slowing down growth. If a plant finds itself in shade, phytochromes are quickly inactivated-enabling it to grow faster to find sunlight again. This is how plants compete to escape each other&amp;rsquo;s shade.&amp;rsquo; Light-driven changes to phytochrome activity occur very fast, in less than a second,&amp;rsquo; says Wigge.&lt;/p>
&lt;p>At night, however, it&amp;rsquo;s a different story. Instead of a rapid deactivation following sundown, the molecules gradually change from their active to inactive state. This is called&amp;rsquo; dark reversion&amp;rsquo;. &amp;lsquo;Just as mercury rises in a thermometer, the rate at which phytochromes revert to their inactive state during the night is a direct measure of temperature,&amp;rsquo; says Wigge.&lt;/p>
&lt;h4 id="f">F&lt;/h4>
&lt;p>‘The lower the temperature, the slower the rate at which phytochromes revert to inactivity, so the molecules spend more time in their active, growth-suppressing state. This is why plants are slower to grow in winter. Warm temperatures accelerate dark reversion, so that phytochromes rapidly reach an inactive state and detach themselves from the plant&amp;rsquo;s DNA - allowing genes to be expressed and plant growth to resume.’ Wigge believes phytochrome thermo-sensing evolved at a later stage, and co-opted the biological network already used for light-based growth during the downtime of night.&lt;/p>
&lt;h4 id="g">G&lt;/h4>
&lt;p>Some plants mainly use day length as an indicator of the season. Other species, such as daffodils, have considerable temperature sensitivity, and can flower months in advance during a warm winter. In fact, the discovery of the dual role of phytochromes provides the science behind a well-known rhyme long used to predict the coming season: oak before ash we&amp;rsquo; ll have a splash, ash before oak we&amp;rsquo; re in for a soak.&lt;/p>
&lt;p>Wigge explains: &amp;lsquo;Oak trees rely much more on temperature, likely using phytochromes as thermometers to dictate development, whereas ash trees rely on measuring day length to determine their seasonal timing.A warmer spring, and consequently a higher likeliness of a hot summer, will result in oak leafing before ash. A cold spring will see the opposite. As the British know only too well,a colder summer is likely to be a rain-soaked one.&amp;rsquo;&lt;/p>
&lt;h4 id="h">H&lt;/h4>
&lt;p>The new findings are the culmination of twelve years of research involving scientists from Germany, Argentina and the US, as well as the Cambridge team. The work was done in a model system, using a mustard plant called Arabidopsis, but Wigge says the phytochrome genes necessary for temperature sensing are found in crop plants as well. ‘Recent advances in plant genetics now mean that scientists are able to rapidly identify the genes controlling these processes in crop plants, and even alter their activity using precise molecular&amp;quot;scalpels&amp;quot;,&amp;rsquo; adds Wigge. &amp;lsquo;Cambridge is uniquely well-positioned to do this kind of research as we have outstanding collaborators nearby who work on more applied aspects of plant biology, and can help us transfer this new knowledge into the field.&amp;rsquo;&lt;/p></description></item><item><title>Docs: R16.4.1 Roman tunnels</title><link>https://note.codiy.net/ielts/r16.4.1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.codiy.net/ielts/r16.4.1/</guid><description>
&lt;blockquote>
&lt;p>&lt;strong>The Romans, who once controlled areas of Europe, North Africa and Asia Minor, adopted the construction techniques of other civilizations to build tunnels in their territories&lt;/strong>&lt;/p>
&lt;/blockquote>
&lt;p>The Persians, who lived in present-day Iran, were one of the first civilizations to build tunnels that provided a reliable supply of water to human settlements in dry areas. In the early first millennium BCE, they introduced the qanat method of tunnel construction, which consisted of placing posts over a hill in a straight line, to ensure that the tunnel kept to its route, and then digging vertical shafts down into the ground at regular intervals. Underground, workers removed the earth from between the ends of the shafts, creating a tunnel. The excavated soil was taken up to the surface using the shafts, which also provided ventilation during the work. Once the tunnel was completed, it allowed water to flow from the top of a hillside down towards a canal, which supplied water for human use. Remarkably, some qanats built by the Persians 2,700 years ago are still in use today.&lt;/p>
&lt;p>They later passed on their knowledge to the Romans, who also used the qanat method to construct water-supply tunnels for agriculture. Roman qanat tunnels were constructed with vertical shafts dug at intervals of between 30 and 60 meters. The shafts were equipped with handholds and footholds to help those climbing in and out of them and were covered with a wooden or stone lid. To ensure that the shafts were vertical, Romans hung a plumb line from a rod placed across the top of each shaft and made sure that the weight at the end of it hung in the center of the shaft. Plumb lines were also used to measure the depth of the shaft and to determine the slope of the tunnel. The 5.6-kilometer-long Claudius tunnel, built in 41 CE to drain the Fucine Lake in central Italy, had shafts that were up to 122 meters deep, took 11 years to build and involved approximately 30,000 workers.&lt;/p>
&lt;p>By the 6th century BCE,a second method of tunnel construction appeared called the counter-excavation method, in which the tunnel was constructed from both ends. It was used to cut through high mountains when the qanat method was not a practical alternative. This method required greater planning and advanced knowledge of surveying, mathematics and geometry as both ends of a tunnel had to meet correctly at the center of the mountain. Adjustments to the direction of the tunnel also had to be made whenever builders encountered geological problems or when it deviated from its set path. They constantly checked the tunnel&amp;rsquo;s advancing direction, for example, by looking back at the light that penetrated through the tunnel mouth, and made corrections whenever necessary. Large deviations could happen, and they could result in one end of the tunnel not being usable. An inscription written on the side of a 428-meter tunnel, built by the Romans as part of the Saldae aqueduct system in modern-day Algeria, describes how the two teams of builders missed each other in the mountain and how the later construction of a lateral link between both corridors corrected the initial error.&lt;/p>
&lt;p>The Romans dug tunnels for their roads using the counter-excavation method, whenever they encountered obstacles such as hills or mountains that were too high for roads to pass over. An example is the 37-meter-long,6-meter-high, Furlo Pass Tunnel built in Italy in 69-79 CE. Remarkably,a modern road still uses this tunnel today. Tunnels were also built for mineral extraction. Miners would locate a mineral vein and then pursue it with shafts and tunnels underground. Traces of such tunnels used to mine gold can still be found at the Dolaucothi mines in Wales. When the sole purpose of a tunnel was mineral extraction, construction required less planning, as the tunnel route was determined by the mineral vein.&lt;/p>
&lt;p>Roman tunnel projects were carefully planned and carried out. The length of time it took to construct a tunnel depended on the method being used and the type of rock being excavated. The qanat construction method was usually faster than the counter-excavation method as it was more straightforward. This was because the mountain could be excavated not only from the tunnel mouths but also from shafts. The type of rock could also influence construction times. When the rock was hard, the Romans employed a technique called fire quenching which consisted of heating the rock with fire, and then suddenly cooling it with cold water so that it would crack. Progress through hard rock could be very slow, and it was not uncommon for tunnels to take years, if not decades, to be built. Construction marks left on a Roman tunnel in Bologna show that the rate of advance through solid rock was 30 centimeters per day. In contrast, the rate of advance of the Claudius tunnel can be calculated at 1.4 meters per day. Most tunnels had inscriptions showing the names of patrons who ordered construction and sometimes the name of the architect. For example, the 1.4-kilometer Cevlik tunnel in Turkey, built to divert the floodwater threatening the harbor of the ancient city of Seleuceia Pieria, had inscriptions on the entrance, still visible today, that also indicate that the tunnel was started in 69 CE and was completed in 81 CE.&lt;/p></description></item><item><title>Docs: R16.4.2 Changes in reading habits</title><link>https://note.codiy.net/ielts/r16.4.2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.codiy.net/ielts/r16.4.2/</guid><description>
&lt;blockquote>
&lt;p>&lt;strong>What are the implications of the way we read today?&lt;/strong>&lt;/p>
&lt;/blockquote>
&lt;p>Look around on your next plane trip. The iPad is the new pacifier for babies and toddlers. Younger school-aged children read stories on smartphones; older kids don&amp;rsquo;t read at all, but hunch over video games. Parents and other passengers read on tablets or skim a flotilla of email and news feeds. Unbeknown to most of us, an invisible, game-changing transformation links everyone in this picture: the neuronal circuit that underlies the brain&amp;rsquo;s ability to read is subtly, rapidly changing and this has implications for everyone from the pre-reading toddler to the expert adult.&lt;/p>
&lt;p>As work in neurosciences indicates, the acquisition of literacy necessitated a new circuit in our species&amp;rsquo; brain more than 6,000 years ago. That circuit evolved from a very simple mechanism for decoding basic information, like the number of goats in one&amp;rsquo;s herd, to the present, highly elaborated reading brain. My research depicts how the present reading brain enables the development of some of our most important intellectual and affective processes: internalized knowledge, analogical reasoning, and inference; perspective-taking and empathy; critical analysis and the generation of insight. Research surfacing in many parts of the world now cautions that each of these essential &amp;lsquo;deep reading&amp;rsquo; processes may be under threat as we move into digital-based modes of reading.&lt;/p>
&lt;p>This is not a simple, binary issue of print versus digital reading and technological innovation. As MIT scholar Sherry Turkle has written, we do not err as a society when we innovate but when we ignore what we disrupt or diminish while innovating. In this hinge moment between print and digital cultures, society needs to confront what is diminishing in the expert reading circuit, what our children and older students are not developing, and what we can do about it.&lt;/p>
&lt;p>We know from research that the reading circuit is not given to human beings through a genetic blueprint like vision or language; it needs an environment to develop. Further, it will adapt to that environment&amp;rsquo;s requirements-from different writing systems to the characteristics of whatever medium is used. If the dominant medium advantages processes that are fast, multi-task oriented and well-suited for large volumes of information, like the current digital medium, so will the reading circuit. As UCLA psychologist Patricia Greenfield writes, the result is that less attention and time will be allocated to slower, time-demanding deep reading processes.&lt;/p>
&lt;p>Increasing reports from educators and from researchers in psychology and the humanities bear this out. English literature scholar and teacher Mark Edmundson describes how many college students actively avoid the classic literature of the 19th and 20th centuries in favour of something simpler as they no longer have the patience to read longer, denser, more difficult texts. We should be less concerned with students’ ‘cognitive impatience’, however, than by what may underlie it: the potential inability of large numbers of students to read with a level of critical analysis sufficient to comprehend the complexity of thought and argument found in more demanding texts.&lt;/p>
&lt;p>Multiple studies show that digital screen use may be causing a variety of troubling downstream effects on reading comprehension in older high school and college students. In Stavanger, Norway, psychologist Anne Mangen and her colleagues studied how high school students comprehend the same material in different mediums. Mangen&amp;rsquo;s group asked subjects questions about a short story whose plot had universal student appeal; half of the students read the story on a tablet, the other half in paperback. Results indicated that students who read on print were superior in their comprehension to screen-reading peers, particularly in their ability to sequence detail and reconstruct the plot in chronological order.&lt;/p>
&lt;p>Ziming Liu from San Jose State University has conducted a series of studies which indicate that the new norm&amp;rsquo; in reading is skimming, involving word-spotting and browsing through the text. Many readers now use a pattern when reading in which they sample the first line and then word-spot through the rest of the text. When the reading brain skims like this, it reduces time allocated to deep reading processes. In other words, we don&amp;rsquo;t have time to grasp complexity, to understand another&amp;rsquo;s feelings, to perceive beauty, and to create thoughts of the reader&amp;rsquo;s own.&lt;/p>
&lt;p>The possibility that critical analysis, empathy and other deep reading processes could become the unintended&amp;rsquo; collateral damage&amp;rsquo; of our digital culture is not a straightforward binary issue about print versus digital reading. It is about how we all have begun to read on various mediums and how that changes not only what we read, but also the purposes for which we read. Nor is it only about the young. The subtle atrophy of critical analysis and empathy affects us all equally. It affects our ability to navigate a constant bombardment of information. It incentivizes a retreat to the most familiar stores of unchecked information, which require and receive no analysis, leaving us susceptible to false information and irrational ideas.&lt;/p>
&lt;p>There&amp;rsquo;s an old rule in neuroscience that does not alter with age: use it or lose it. It is a very hopeful principle when applied to critical thought in the reading brain because it implies choice. The story of the changing reading brain is hardly finished. We possess both the science and the technology to identify and redress the changes in how we read before they become entrenched. If we work to understand exactly what we will lose, alongside the extraordinary new capacities that the digital world has brought us, there is as much reason for excitement as caution.&lt;/p></description></item><item><title>Docs: R16.4.3 Attitudes towards Artificial Intelligence</title><link>https://note.codiy.net/ielts/r16.4.3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.codiy.net/ielts/r16.4.3/</guid><description>
&lt;h4 id="a">A&lt;/h4>
&lt;p>Artificial intelligence (AI) can already predict the future. Police forces are using it to map when and where crime is likely to occur. Doctors can use it to predict when a patient is most likely to have a heart attack or stroke. Researchers are even trying to give AI imagination so it can plan for unexpected consequences.&lt;/p>
&lt;p>Many decisions in our lives require a good forecast, and AI is almost always better at forecasting than we are. Yet for all these technological advances, we still seem to deeply lack confidence in AI predictions. Recent cases show that people don&amp;rsquo;t like relying on AI and prefer to trust human experts, even if these experts are wrong.&lt;/p>
&lt;p>If we want AI to really benefit people, we need to find a way to get people to trust it. To do that, we need to understand why people are so reluctant to trust AI in the first place.&lt;/p>
&lt;h4 id="b">B&lt;/h4>
&lt;p>Take the case of Watson for Oncology, one of technology giant IBM&amp;rsquo;s supercomputer programs. Their attempt to promote this program to cancer doctors was a PR disaster. The AI promised to deliver top-quality recommendations on the treatment of 12 cancers that accounted for 80% of the world&amp;rsquo;s cases. But when doctors first interacted with Watson, they found themselves in a rather difficult situation. On the one hand, if Watson provided guidance about a treatment that coincided with their own opinions, physicians did not see much point in Watson&amp;rsquo;s recommendations. The supercomputer was simply telling them what they already knew, and these recommendations did not change the actual treatment.&lt;/p>
&lt;p>On the other hand, if Watson generated a recommendation that contradicted the experts&amp;rsquo; opinion, doctors would typically conclude that Watson wasn&amp;rsquo;t competent. And the machine wouldn&amp;rsquo;t be able to explain why its treatment was plausible because its machine-learning algorithms were simply too complex to be fully understood by humans. Consequently, this has caused even more suspicion and disbelief, leading many doctors to ignore the seemingly outlandish AI recommendations and stick to their own expertise.&lt;/p>
&lt;h4 id="c">C&lt;/h4>
&lt;p>This is just one example of people&amp;rsquo;s lack of confidence in AI and their reluctance to accept what AI has to offer. Trust in other people is often based on our understanding of how others think and having experience of their reliability. This helps create a psychological feeling of safety. AI, on the other hand, is still fairly new and unfamiliar to most people. Even if it can be technically explained (and that&amp;rsquo;s not always the case), AI&amp;rsquo;s decision-making process is usually too difficult for most people to comprehend. And interacting with something we don&amp;rsquo;t understand can cause anxiety and give us a sense that we&amp;rsquo; re losing control.&lt;/p>
&lt;p>Many people are also simply not familiar with many instances of AI actually working, because it often happens in the background. Instead, they are acutely aware of instances where AI goes wrong. Embarrassing AI failures receive a disproportionate amount of media attention, emphasising the message that we cannot rely on technology. Machine learning is not foolproof, in part because the humans who design it aren&amp;rsquo;t.&lt;/p>
&lt;h4 id="d">D&lt;/h4>
&lt;p>Feelings about AI run deep. In a recent experiment, people from a range of backgrounds were given various sci-fi films about AI to watch and then asked questions about automation in everyday life. It was found that, regardless of whether the film they watched depicted AI in a positive or negative light, simply watching a cinematic vision of our technological future polarised the participants&amp;rsquo; attitudes. Optimists became more extreme in their enthusiasm for AI and sceptics became even more guarded.&lt;/p>
&lt;p>This suggests people use relevant evidence about AI in a biased manner to support their existing attitudes,a deep-rooted human tendency known as&amp;quot;confirmation bias&amp;quot;. As AI is represented more and more in media and entertainment, it could lead to a society split between those who benefit from AI and those who reject it. More pertinently, refusing to accept the advantages offered by AI could place a large group of people at a serious disadvantage.&lt;/p>
&lt;h4 id="e">E&lt;/h4>
&lt;p>Fortunately, we already have some ideas about how to improve trust in AI. Simply having previous experience with AI can significantly improve people&amp;rsquo;s opinions about the technology, as was found in the study mentioned above. Evidence also suggests the more you use other technologies such as the internet, the more you trust them.&lt;/p>
&lt;p>Another solution may be to reveal more about the algorithms which AI uses and the purposes they serve. Several high-profile social media companies and online marketplaces already release transparency reports about government requests and surveillance disclosures.A similar practice for AI could help people have a better understanding of the way algorithmic decisions are made.&lt;/p>
&lt;h4 id="f">F&lt;/h4>
&lt;p>Research suggests that allowing people some control over AI decision-making could also improve trust and enable AI to learn from human experience. For example, one study showed that when people were allowed the freedom to slightly modify an algorithm, they felt more satisfied with its decisions, more likely to believe it was superior and more likely to use it in the future.&lt;/p>
&lt;p>We don&amp;rsquo;t need to understand the intricate inner workings of AI systems, but if people are given a degree of responsibility for how they are implemented, they will be more willing to accept AI into their lives.&lt;/p></description></item></channel></rss>